{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraper\n",
    "\n",
    "import glob\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://www.alexa.com/topsites/countries/KR'\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "#requests.get(url)을 통해 url의 html을 '요청'함 - 응답(Response)이 돌아옴\n",
    "#이를 확인하려면 .text 달아주면 html 전체 보여줌\n",
    "soup_website_ranking = BS(html_website_ranking, 'lxml')\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "del website_ranking[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>,\n",
       " <a href=\"/siteinfo/tmall.com\">Tmall.com</a>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking_address = [element.get_text() for element in website_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tistory.com',\n",
       " 'Tmall.com']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_address[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5  Tistory.com\n",
       "6    Tmall.com"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_dict = {'Website': website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict, columns = ['Website'],\n",
    "                 index = range(1, len(website_ranking_address) + 1))\n",
    "df[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#음악 순위 music ranking\n",
    "\n",
    "url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1&page='\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BS(html_music, 'lxml')\n",
    "\n",
    "\"\"\"\n",
    "<a href=\"#40987767\" class=\"_title title NPI=a:track,r:1,i:40987767\" title=\"다시 여기 바닷가\"><span class=\"ellipsis\">다시 여기 바닷가</span></a>\n",
    "    <span class=\"ellipsis\">다시 여기 바닷가</span>\n",
    "\n",
    "=> a 태그 요소 중 class 속성값이 _title 인 것 찾고\n",
    "    그 다음 span 태그의 ellipsis 요소를 추출\n",
    "\"\"\"\n",
    "titles = soup_music.select('a._title span.ellipsis')\n",
    "titles[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles = [title.get_text() for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', '기억의 빈자리', '선물', 'Beautiful', '좋아', '피카부 (Peek-A-Boo)', '좋니']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\tZion.T\\r\\n\\t\\t'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artists 추출\n",
    "\n",
    "artists = soup_music.select('a._artist span.ellipsis')\n",
    "artists[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신',\n",
       " '뉴이스트 W']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists[:7]\n",
    "#5위 좋아의 artist 민서가 출력 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"_artist NPI=a:artist,r:1,i:115967\" href=\"/artist/home.nhn?artistId=115967\" title=\"Zion.T\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tZion.T\n",
       " \t\t</span>\n",
       " </a>,\n",
       " <a class=\"_artist NPI=a:artist,r:2,i:9483\" href=\"/artist/home.nhn?artistId=9483\" title=\"나얼\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t나얼\n",
       " \t\t</span>\n",
       " </a>,\n",
       " <a class=\"_artist NPI=a:artist,r:3,i:377357\" href=\"/artist/home.nhn?artistId=377357\" title=\"멜로망스(Melomance)\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t멜로망스(Melomance)\n",
       " \t\t</span>\n",
       " </a>,\n",
       " <a class=\"_artist NPI=a:artist,r:4,i:1748968\" href=\"/artist/home.nhn?artistId=1748968\" title=\"Wanna One(워너원)\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tWanna One(워너원)\n",
       " \t\t</span>\n",
       " </a>,\n",
       " <a alt=\"\" class=\"NPI=a:layerbtn,r:5\" href=\"javascript:void(0);\" title=\"\">민서</a>,\n",
       " <a class=\"_artist NPI=a:artist,r:6,i:343255\" href=\"/artist/home.nhn?artistId=343255\" title=\"Red Velvet (레드벨벳)\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\tRed Velvet (레드벨벳)\n",
       " \t\t</span>\n",
       " </a>,\n",
       " <a class=\"_artist NPI=a:artist,r:7,i:182\" href=\"/artist/home.nhn?artistId=182\" title=\"윤종신\">\n",
       " <span class=\"ellipsis\">\n",
       " \t\t\t\n",
       " \t\t\t\n",
       " \t\t\t윤종신\n",
       " \t\t</span>\n",
       " </a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "artist 민서 요소 검사 해보면 다른 artist의 html 코드와 다름\n",
    "다른 artist는\n",
    "td class=\"_artist\"\n",
    "    a class=\"_artist\"\n",
    "        span class=\"ellipsis\"\n",
    "형식인 데 반해\n",
    "\n",
    "민서 artist는\n",
    "td class=\"_artist artist no_ell2\"\n",
    "    a class=\"NPI=a:layerbtn,r:5\"\n",
    "형식이므로\n",
    "\n",
    "공통으로 존재하는 td 클래스와 a 클래스를 이용\n",
    "'''\n",
    "artists = soup_music.select('td._artist a')\n",
    "artists[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " '민서',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists[:7]\n",
    "#민서도 정상적으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 눈 (Feat. 이문세) / Zion.T\n",
      "2: 기억의 빈자리 / 나얼\n",
      "3: 선물 / 멜로망스(Melomance)\n",
      "4: Beautiful / Wanna One(워너원)\n",
      "5: 좋아 / 민서\n",
      "6: 피카부 (Peek-A-Boo) / Red Velvet (레드벨벳)\n",
      "7: 좋니 / 윤종신\n"
     ]
    }
   ],
   "source": [
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_titles[k], music_artists[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles_artists = {}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(music_titles, music_artists):\n",
    "    order += 1\n",
    "    music_titles_artists[order] = [music_title, music_artist]\n",
    "    \n",
    "#music_titles_artists = dict(zip(music_titles, music_artists))\n",
    "#위 for 문은 순위를 key로 함 / 밑에는 간단하지만 title이 key가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['눈 (Feat. 이문세)', 'Zion.T'],\n",
       " 2: ['기억의 빈자리', '나얼'],\n",
       " 3: ['선물', '멜로망스(Melomance)'],\n",
       " 4: ['Beautiful', 'Wanna One(워너원)'],\n",
       " 5: ['좋아', '민서'],\n",
       " 6: ['피카부 (Peek-A-Boo)', 'Red Velvet (레드벨벳)'],\n",
       " 7: ['좋니', '윤종신'],\n",
       " 8: ['WHERE YOU AT', '뉴이스트 W'],\n",
       " 9: ['LIKEY', 'TWICE(트와이스)'],\n",
       " 10: ['그때의 나, 그때의 우리', '어반자카파'],\n",
       " 11: ['있다면', '뉴이스트 W'],\n",
       " 12: ['뻔한 이별 (PROD. 13)', '소유 (SOYOU..'],\n",
       " 13: ['하루만', '뉴이스트 W'],\n",
       " 14: ['여보세요', '뉴이스트'],\n",
       " 15: ['DNA', '방탄소년단'],\n",
       " 16: ['갖고 싶어', 'Wanna One(워너원)'],\n",
       " 17: ['에너제틱 (Energetic)', 'Wanna One(워너원)'],\n",
       " 18: ['특별해', '젝스키스'],\n",
       " 19: ['연애소설 (Feat. 아이유)', '에픽하이 (EPIK HIGH)'],\n",
       " 20: ['썸 탈꺼야', '볼빨간사춘기'],\n",
       " 21: ['웃어줘', '젝스키스'],\n",
       " 22: ['지금까지 행복했어요 (BAEKHO SOLO)', '뉴이스트 W'],\n",
       " 23: ['그리워하다', '비투비'],\n",
       " 24: ['GOOD LOVE (ARON SOLO)', '뉴이스트 W'],\n",
       " 25: ['Twilight', 'Wanna One(워너원)'],\n",
       " 26: ['밤편지', '아이유(IU)'],\n",
       " 27: ['PARADISE (REN SOLO)', '뉴이스트 W'],\n",
       " 28: ['WITH (JR SOLO)', '뉴이스트 W'],\n",
       " 29: ['봄날', '방탄소년단'],\n",
       " 30: ['Nothing Without You (Intro.)', 'Wanna One(워너원)'],\n",
       " 31: ['아프지 마요', '젝스키스'],\n",
       " 32: ['시차 (We Are) (Feat. 로꼬 & GRAY)', '우원재'],\n",
       " 33: ['오랜만이에요', '젝스키스'],\n",
       " 34: ['백허그', '젝스키스'],\n",
       " 35: ['MOVE', '태민(TAEMIN)'],\n",
       " 36: ['슬픈 노래', '젝스키스'],\n",
       " 37: ['나의 사춘기에게', '볼빨간사춘기'],\n",
       " 38: ['Havana (Feat. Young Thug)', 'Camila Cabello'],\n",
       " 39: ['네가 필요해', '젝스키스'],\n",
       " 40: ['술끊자', '젝스키스'],\n",
       " 41: ['다신', '젝스키스'],\n",
       " 42: ['REALLY REALLY', 'WINNER'],\n",
       " 43: ['느낌이 와', '젝스키스'],\n",
       " 44: ['첫눈처럼 너에게 가겠다', '에일리'],\n",
       " 45: ['현기증', '젝스키스'],\n",
       " 46: ['나의 밤 나의 너', '성시경'],\n",
       " 47: ['Snowman', 'Sia'],\n",
       " 48: ['세 단어', '젝스키스'],\n",
       " 49: ['비도 오고 그래서 (Feat. 신용재)', '헤이즈(Heize)'],\n",
       " 50: ['빈차 (Feat. 오혁)', '에픽하이 (EPIK HIGH)']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', 'Zion.T']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\users\\\\Yubin\\\\mypy\\\\data\\\\NaverMusicTop100.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#반복되는 것을 함수로 정의\n",
    "naver_music_url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1&page='\n",
    "#TOP 100 차트 히스토리의 2017년 12월 1주차 차트, 마지막 &page= 을 붙여 열 page 선택 (+숫자만 해주면 됨, 두 페이지이므로 1 혹은 2)\n",
    "\n",
    "def naver_music(url):\n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BS(html_music, 'lxml')\n",
    "    \n",
    "    titles = soup_music.select('a._title span.ellipsis')\n",
    "    artists = soup_music.select('td._artist a')\n",
    "    \n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    return music_titles, music_artists\n",
    "\n",
    "file_name = 'C:\\\\users\\\\Yubin\\\\mypy\\\\data\\\\NaverMusicTop100.txt'\n",
    "\n",
    "f = open(file_name, 'w')\n",
    "\n",
    "for page in range(2):\n",
    "    naver_music_url_page = naver_music_url + str(page + 1)\n",
    "    #naver_music_url + 1 혹은 2를 해주어 페이지 열기\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page)\n",
    "    #url을 합친 뒤 함수에 삽입, return 되는 music_titles와 music_artists를 각각 리스트로 줌\n",
    "    \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        f.write('{0:2d}: {1}/{2}\\n'.format(page*50 + k+1, naver_music_titles[k], naver_music_artists[k]))\n",
    "        #한 페이지에 순위 50개 -> 계산해준 값이 {0}\n",
    "f.close()\n",
    "\n",
    "glob.glob(file_name)\n",
    "#생성된 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\users\\\\Yubin\\\\mypy\\\\data\\\\NaverMusicTop100.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인 결과 82위의 All I Want For Christmas Is You 가 다른 항목들과 달라 출력 안 됨\n",
    "\n",
    "#반복되는 것을 함수로 정의\n",
    "naver_music_url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1&page='\n",
    "#TOP 100 차트 히스토리의 2017년 12월 1주차 차트, 마지막 &page= 을 붙여 열 page 선택 (+숫자만 해주면 됨, 두 페이지이므로 1 혹은 2)\n",
    "\n",
    "def naver_music(url):\n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BS(html_music, 'lxml')\n",
    "    \n",
    "    titles = soup_music.select('td.name span.ellipsis')\n",
    "    #요소 검사 통해 확인한 결과 이렇게 하면 82위도 포함됨\n",
    "    artists = soup_music.select('td._artist a')\n",
    "    \n",
    "    music_titles = [title.get_text() for title in titles]\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "    \n",
    "    return music_titles, music_artists\n",
    "\n",
    "file_name = 'C:\\\\users\\\\Yubin\\\\mypy\\\\data\\\\NaverMusicTop100.txt'\n",
    "\n",
    "f = open(file_name, 'w')\n",
    "\n",
    "for page in range(2):\n",
    "    naver_music_url_page = naver_music_url + str(page + 1)\n",
    "    #naver_music_url + 1 혹은 2를 해주어 페이지 열기\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page)\n",
    "    #url을 합친 뒤 함수에 삽입, return 되는 music_titles와 music_artists를 각각 리스트로 줌\n",
    "    \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        f.write('{0:2d}: {1}/{2}\\n'.format(page*50 + k+1, naver_music_titles[k], naver_music_artists[k]))\n",
    "        #한 페이지에 순위 50개 -> 계산해준 값이 {0}\n",
    "f.close()\n",
    "\n",
    "glob.glob(file_name)\n",
    "#생성된 파일 확인\n",
    "\n",
    "#체크해보니 100개 다 제대로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p.439 이미지 가져오기\n",
    "import requests\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image\n",
    "#Response [200]이 반환 = 이미지 파일 주소에 문제 없다는 의미\n",
    "#앞에서는 .text 로 문자열 가져옴 - 이미지는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_file_name = os.path.basename(url)\n",
    "#basename 이용해 url에서 이미지 파일명만 추출\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:/users/Yubin/mypy/download'\n",
    "\n",
    "#if not os.path.exists(folder):\n",
    "#    os.makedirs(folder)\n",
    "\n",
    "#os.path.exists로 folder가 이미 존재하는지 확인, 없으면 새로 만듦(makedirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/users/Yubin/mypy/download\\\\python-logo.png'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "#폴더 + 파일 이름\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, 'wb')\n",
    "#파일을 쓰기 + 바이너리 파일 모드로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000000 #이미지 데이터를 1000000바이트씩 나눠서 내려받음\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giraffe_1.jpg', 'python-logo.png']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)\n",
    "#지정 폴더의 파일 목록 보여주는 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Free Stock Photos\" height=\"34\" src=\"https://picjumbo.com/wp-content/themes/picjumbofree/data/picjumbo_logo.png\" width=\"130\"/>,\n",
       " <img alt=\"Stock Photo Selection from iStock\" src=\"https://picjumbo.com/wp-content/themes/picjumbofree/data/istock-logo.jpg?v=2.1\"/>,\n",
       " <img alt=\"Download The Golden Gate Bridge Sunset FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/free-stock-photos-san-francisco-1080x720.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/free-stock-photos-san-francisco.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download The Golden Gate Bridge Sunset FREE Stock Photo\" width=\"1080\"/>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#여러 이미지 내려받기\n",
    "#pixabay.com - 둘러보기 - Popular Images - 카테고리: 동물\n",
    "#<img srcset=\"https://cdn.pixabay.com/photo/2020/07/21/22/30/heron-5427512__340.jpg 1x, https://cdn.pixabay.com/photo/2020/07/21/22/30/heron-5427512__480.jpg 2x\" src=\"https://cdn.pixabay.com/photo/2020/07/21/22/30/heron-5427512__340.jpg\" alt=\"헤론, 섭 새, 동물, 깃털, 부리, 서있는, 먹이, 물, 연못\">\n",
    "#위는 한 사진의 html - img 태그를 select 하면 사진 파일 가져올 수 있을 듯하다.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "URL = 'https://picjumbo.com/'\n",
    "#URL = 'https://www.google.com/search?q=giraffe&tbm=isch&ved=2ahUKEwitubig8O3qAhWPAaYKHam5DX0Q2-cCegQIABAA&oq=giraffe&gs_lcp=CgNpbWcQAzIFCAAQsQMyAggAMgIIADICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgAEAM6CAgAELEDEIMBUJ8gWPpCYNlEaABwAHgBgAF3iAH_CZIBBDEuMTGYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABAMABAQ&sclient=img&ei=FQQfX-3aEo-DmAWp87boBw&bih=615&biw=957&client=firefox-b-d'\n",
    "\n",
    "html_google_image = requests.get(URL).text\n",
    "soup_google_image = BS(html_google_image, 'lxml')\n",
    "google_image_elements = soup_google_image.select('img')\n",
    "google_image_elements[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport requests\\nfrom bs4 import BeautifulSoup as BS\\n\\nURL = 'https://pixabay.com/ko/images/search/?cat=animals'\\n\\nhtml_pixabay_image = requests.get(URL).text\\nsoup_pixabay_image = BS(html_pixabay_image, 'lxml')\\npixabay_image_elements = soup_pixabay_image.select('img')\\npixabay_image_elements\\n\\nhtml_pixabay_image = requests.get(URL).text\\nsoup_pixabay_image = BS(html_pixabay_image, 'lxml')\\nimg_tags = soup_pixabay_image.find_all('a')\\nprint(img_tags)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "URL = 'https://pixabay.com/ko/images/search/?cat=animals'\n",
    "\n",
    "html_pixabay_image = requests.get(URL).text\n",
    "soup_pixabay_image = BS(html_pixabay_image, 'lxml')\n",
    "pixabay_image_elements = soup_pixabay_image.select('img')\n",
    "pixabay_image_elements\n",
    "\n",
    "html_pixabay_image = requests.get(URL).text\n",
    "soup_pixabay_image = BS(html_pixabay_image, 'lxml')\n",
    "img_tags = soup_pixabay_image.find_all('a')\n",
    "print(img_tags)\n",
    "'''\n",
    "\n",
    "#출력에서 보다시피 cloudflare로 인해 막힘\n",
    "#https://github.com/Anorov/cloudflare-scrape - cloudflare 우회 방법인 듯함\n",
    "#아래에서 직접 사용해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport cfscrape\\n\\nscraper = cfscrape.create_scraper()\\nhtml_pixabay_image = scraper.get(URL).content\\nsoup_pixabay_image = BS(html_pixabay_image, 'lxml')\\npixabay_image_elements = soup_pixabay_image.select('a')\\npixabay_image_elements[:3]\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cfscrape\n",
    "\n",
    "scraper = cfscrape.create_scraper()\n",
    "html_pixabay_image = scraper.get(URL).content\n",
    "soup_pixabay_image = BS(html_pixabay_image, 'lxml')\n",
    "pixabay_image_elements = soup_pixabay_image.select('a')\n",
    "pixabay_image_elements[:3]\n",
    "'''\n",
    "#안 됨 ㅋㅋ\n",
    "\n",
    "#https://studyforus.com/innisfree/647489\n",
    "#https://stackoverflow.com/questions/18408307/how-to-extract-and-download-all-images-from-a-website-using-beautifulsoup\n",
    "#검색해본 다른 해결책 / 자야해서 아직 안 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://picjumbo.com/wp-content/uploads/free-stock-photos-san-francisco-1080x720.jpg'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위위위의 구글 이미지 검색으로 책 내용 계속 진행\n",
    "\n",
    "google_image_url = google_image_elements[3].get('src')\n",
    "google_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(google_image_url)\n",
    "\n",
    "#folder = 'C:/users/Yubin/mypy/download'\n",
    "#/ 쓰면 문제 발생하나봄\n",
    "folder = 'C:\\\\users\\\\Yubin\\\\mypy\\\\download'\n",
    "\n",
    "#os.path.basename(URL)은 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리하는 방법\n",
    "#imageFile = open(os.path.join(folder, os.path.basename(google_image_url)), 'wb')\n",
    "#google_image_url을 파일명으로 하긴 부적합한가봄\n",
    "\n",
    "imageFile = open(os.path.join(folder, os.path.basename(google_image_url[-10:] + '.jpg')), 'wb')\n",
    "#imageFile = open(os.path.join(folder, os.path.basename('giraffe_1.jpg')), 'wb')\n",
    "\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "write to closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-e87c51989dce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_download_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoogle_image_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'================================='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'선택한 모든 이미지 내려받기 완료!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-e87c51989dce>\u001b[0m in \u001b[0;36mdownload_image\u001b[1;34m(img_folder, img_url)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mchunk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhtml_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mimageFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mimageFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'이미지 파일명: {0}. 내려받기 완료!'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: write to closed file"
     ]
    }
   ],
   "source": [
    "#코드 합치기 + 여러 이미지 받기\n",
    "#반복 작업은 함수화\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BS(html_image_url, 'lxml')\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        image_File = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "        chunk_size = 1000000\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "        print('이미지 파일명: {0}. 내려받기 완료!'.format(os.path.basename(img_url)))\n",
    "    else:\n",
    "        print('내려받을 이미지가 없습니다.')\n",
    "\n",
    "google_url = 'https://picjumbo.com/'\n",
    "figure_folder = 'C:\\\\users\\\\Yubin\\\\mypy\\\\download'\n",
    "google_image_urls = get_image_url(google_url) #이미지 파일의 주소 가져오기\n",
    "num_of_download_image = len(google_image_urls)\n",
    "\n",
    "for k in range(3, num_of_download_image):\n",
    "    download_image(figure_folder, google_image_urls[k])\n",
    "print('=================================')\n",
    "print('선택한 모든 이미지 내려받기 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ValueError: write to closed file 뜨고 제대로 결과 저장 안됨\n",
    "#urllib, urlretrieve 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
