{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "#모든 특성을 고르게 반영하기 위해 정규화 필요\n",
    "#k가 너무 작으면 오버피팅 / 반대도 발생 가능\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "standardizer = StandardScaler()\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "#k=2\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
    "new_observation = [1,1,1,1]\n",
    "\n",
    "distances, indices = nearest_neighbors.kneighbors([new_observation])\n",
    "print('최근접 이웃 확인:', features_standardized[indices])\n",
    "\n",
    "nearestneighbors_euclidean = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(features_standardized)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=3\n",
    "nearestneighbors_euclidean = NearestNeighbors(n_neighbors=3, metric=\"euclidean\").fit(features_standardized) \n",
    "\n",
    "nearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(features_standardized).toarray()\n",
    "for i, x in enumerate(nearest_neighbors_with_self):\n",
    "    x[i] = 0\n",
    "    \n",
    "nearest_neighbors_with_self[0]\n",
    "\n",
    "# 첫 번째 샘플에 대한 두 개의 최근접 이웃을 확인합니다.\n",
    "print(nearest_neighbors_with_self[0])\n",
    "\n",
    "# 이 샘플과 가장 가까운 이웃의 다섯개의 인덱스를 찾습니다.\n",
    "indices = nearest_neighbors.kneighbors([new_observation], n_neighbors=5, return_distance=False)\n",
    "features_standardized[indices] # 최근접 이웃을 확인\n",
    "\n",
    "# 반경 0.5 안에 있는 모든 샘플의 인덱스를 찾습니다.\n",
    "indices = nearest_neighbors.radius_neighbors( [new_observation], radius=0.5, return_distance=False)\n",
    "features_standardized[indices[0]] # 반경 내의 이웃을 확인\n",
    "\n",
    "# 반경 내의 이웃을 나타내는 리스트의 리스트\n",
    "nearest_neighbors_with_self = nearest_neighbors.radius_neighbors_graph( [new_observation], radius=0.5).toarray()\n",
    "nearest_neighbors_with_self[0] # 첫 번째 샘플에 대한 반경 내의 이웃을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "standardizer = StandardScaler() # 표준화 객체\n",
    "X_std = standardizer.fit_transform(X) # 특성을 표준화\n",
    "\n",
    "# 5개의 이웃을 사용한 KNN 분류기를 훈련합니다.\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_std, y)\n",
    "new_observations = [[ 0.75, 0.75, 0.75, 0.75],\n",
    "                    [ 1, 1, 1, 1]] # 두 개의 샘플을 만듭니다.\n",
    "\n",
    "print(knn.predict(new_observations)) # 두 샘플의 클래스를 예측\n",
    "print(knn.predict_proba(new_observations)) # 각 샘플이 세 클래스에 속할 확률을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston() # 데이터 로드\n",
    "features = boston.data[:,0:2] #두 개의 특성만 선택\n",
    "target = boston.target\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=10) # 최근접 회귀 모델 객체 생성\n",
    "model = knn_regressor.fit(features, target) # 모델 훈련\n",
    "\n",
    "# 첫 번째 샘플의 타깃 값을 예측하고 1000을 곱합니다.\n",
    "print(model.predict(features[0:1])[0]*1000)\n",
    "\n",
    "import numpy as np\n",
    "indices = model.kneighbors(features[0:1], return_distance=False) #첫 예측값의 neighbors의 index인 indices\n",
    "np.mean(target[indices]) * 1000 #단위가 천 달러이므로 곱하기 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#GridSearchCV로 KNN의 k값 결정\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "standardizer = StandardScaler() # 표준화 객체 생성\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) # KNN 분류기 객체 생성\n",
    "pipe = Pipeline([(\"standardizer\", standardizer),\n",
    "                 (\"knn\", knn)]) # 파이프라인 생성\n",
    "\n",
    "# 탐색 영역의 후보를 만듭니다.\n",
    "search_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "# 그리드 서치 객체 생성\n",
    "classifier = GridSearchCV(pipe, search_space, cv=5, verbose=0).fit(features, target)\n",
    "\n",
    "# 최선의 이웃 개수 (k)\n",
    "classifier.best_estimator_.get_params()[\"knn__n_neighbors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris() #데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "standardizer = StandardScaler() #표준화 객체 생성\n",
    "features_standardized = standardizer.fit_transform(features) #특성을 표준화\n",
    "\n",
    "#반지름 이웃 분류기를 훈련합니다.\n",
    "rnn = RadiusNeighborsClassifier(radius=.5, n_jobs=-1).fit(features_standardized, target)\n",
    "new_observations = [[1, 1, 1, 1]] #두 개의 샘플을 만듭니다.\n",
    "rnn.predict(new_observations) #두 샘플의 클래스를 예측\n",
    "\n",
    "#반지름 이웃 분류기를 훈련합니다.\n",
    "rnn = RadiusNeighborsClassifier(radius=.5, outlier_label=-1, n_jobs=-1).fit(features_standardized, target)\n",
    "rnn.predict([[100, 100, 100, 100]]) #값이 너무 커서 outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data[:100,:2] #두 개의 클래스와 두 개의 특성만 선택\n",
    "target = iris.target[:100]\n",
    "\n",
    "scaler = StandardScaler() # 특성 표준화\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "svc = LinearSVC(C=1.0) # 서포트 벡터 분류기 생성\n",
    "model = svc.fit(features_standardized, target) # 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# 클래스를 색으로 구분한 산점도를 그립니다.\n",
    "color = [\"black\" if c == 0 else \"lightgrey\" for c in target]\n",
    "plt.scatter(features_standardized[:,0], features_standardized[:,1], c=color)\n",
    "w = svc.coef_[0] # 초평면을 만듭니다.\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-2.5, 2.5)\n",
    "yy = a * xx - (svc.intercept_[0]) / w[1]\n",
    "plt.plot(xx, yy) # 초평면을 그립니다.\n",
    "plt.axis(\"off\"), plt.show();\n",
    "new_observation = [[ -2, 3]] # 새로운 샘플을 만듭니다.\n",
    "svc.predict(new_observation) # 새로운 샘플의 클래스를 예측\n",
    "svc.decision_function(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "np.random.seed(0) # 랜덤 시드를 지정\n",
    "features = np.random.randn(200, 2) # 두 개의 특성을 만듭니다.\n",
    "# XOR 연산(이것이 무엇인지 알 필요는 없습니다)을 사용하여\n",
    "# 선형적으로 구분할 수 없는 클래스를 만듭니다.\n",
    "target_xor = np.logical_xor(features[:, 0] > 0, features[:, 1] > 0)\n",
    "target = np.where(target_xor, 0, 1)\n",
    "# 방사 기저 함수 커널을 사용한 서포트 벡터 머신을 만듭니다.\n",
    "svc = SVC(kernel=\"rbf\", random_state=0, gamma=1, C=1)\n",
    "model = svc.fit(features, target) # 분류기 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플과 결정 경계를 그립니다.\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_decision_regions(X, y, classifier):\n",
    "cmap = ListedColormap((\"red\", \"blue\"))\n",
    "xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))\n",
    "Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "alpha=0.8, c=cmap.colors[idx],\n",
    "marker=\"+\", label=cl)\n",
    "# 선형 커널을 사용한 서포트 벡터 분류기를 만듭니다.\n",
    "svc_linear = SVC(kernel=\"linear\", random_state=0, C=1)\n",
    "svc_linear.fit(features, target) # 모델 훈련\n",
    "plot_decision_regions(features, target, classifier=svc_linear) # 샘플과 초평면을 그립니다.\n",
    "plt.axis(\"off\"), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "scaler = StandardScaler() # 특성 표준화\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# 서포트 벡터 분류기 객체 생성\n",
    "svc = SVC(kernel=\"linear\", probability=True, random_state=0)\n",
    "model = svc.fit(features_standardized, target) # 분류기 훈련\n",
    "new_observation = [[.4, .4, .4, .4]] #New Sample Data\n",
    "model.predict_proba(new_observation) # 예측 확률 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "iris = datasets.load_iris() #데이터 로드\n",
    "features = iris.data[:100,:] #두 개의 클래스만 선택\n",
    "target = iris.target[:100]\n",
    "scaler = StandardScaler() # 특성을 표준화\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "svc = SVC(kernel=\"linear\", random_state=0) # 서포트 벡터 분류기 객체 생성\n",
    "model = svc.fit(features_standardized, target) # 분류기 훈련\n",
    "model.support_vectors_ # 서포트 벡터를 확인\n",
    "model.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "iris = datasets.load_iris() #데이터 로드\n",
    "features = iris.data[:100,:] #두 개의 클래스만 선택\n",
    "target = iris.target[:100]\n",
    "features = features[40:,:] # 처음 40개 샘플을 제거\n",
    "target = target[40:] #불균형한 클래스를 만듭니다.\n",
    "# 타깃 벡터에서 0이 아닌 클래스는 모두 1로 만듭니다.\n",
    "target = np.where((target == 0), 0, 1)\n",
    "scaler = StandardScaler() # 특성을 표준화\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "svc = SVC(kernel=\"linear\", class_weight=\"balanced\", C=1.0, random_state=0)\n",
    "model = svc.fit(features_standardized, target) # 분류기 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "classifer = GaussianNB() # 가우시안 나이브 베이지 객체 생성\n",
    "model = classifer.fit(features, target) # 모델 훈련\n",
    "new_observation = [[ 4, 4, 4, 0.4]] #New Sample Data\n",
    "model.predict(new_observation) # 클래스 예측\n",
    "# 각 클래스별 사전 확률을 지정한 가우시안 나이브 베이즈 객체 생성\n",
    "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "model = classifer.fit(features, target) # 모델 훈련\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#이산적인 카운트 특성으로 분류기 훈련\n",
    "#텍스트 분류 예측\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Brazil is best',\n",
    "                      'Germany beats both'])\n",
    "count = CountVectorizer() #bag of words 객체 생성\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "features = bag_of_words.toarray()\n",
    "target = np.array([0,0,1])\n",
    "\n",
    "#MultinomialNB를 사용해 두 클래스(brazil과 germany)에 대한 사전 확률을 지정하여 모델을 훈련\n",
    "#각 클래스별 사전확률을 지정한 다항 나이브 베이즈 객체 생성\n",
    "clf = MultinomialNB(class_prior=[0.25,0.5])\n",
    "model = clf.fit(features, target)\n",
    "new_observation = ([[0,0,0,1,0,1,0],\n",
    "                   [1,1,0,1,0,0,0]])\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이진 특성으로 나이브 베이지 분류기 훈련(베르누이)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#세 개의 이진 특성을 만듭니다\n",
    "features = np.random.randint(2, size=(100,3))\n",
    "\n",
    "#이진 타깃 벡터를 만듭니다.\n",
    "target = np.random.randint(2, size=(100,1)).ravel()\n",
    "\n",
    "#각 클래스별 사전 확률을 지정하여 베르누이 나이브 베이즈 객체 생성\n",
    "clf = BernoulliNB(class_prior=[0.25,0.5])\n",
    "model = clf.fit(features,target)\n",
    "\n",
    "#10개의 observations에 대한 예측\n",
    "model.predict(np.random.randint(2, size=(10,3)))\n",
    "\n",
    "#clf = BernoulliNB(class_prior=None, fit_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나이브 베이즈에서는 타깃 클래스에 대한 예측 확률의 순위는 유효하지만\n",
    "#예측 확률이 0 또는 1에 극단적으로 가까워지는 경향이 있음\n",
    "#이를 CalibratedClassifierCV 를 이용해 보정하여 의미 있는 예측 확률을 얻을 수 있음\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "#가우시안 나이브 객체를 생성\n",
    "clf = GaussianNB()\n",
    "\n",
    "#시그모이드 보정을 사용해 보정 교차 검증을 만듭니다.\n",
    "clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')\n",
    "clf_sigmoid.fit(features, target)\n",
    "\n",
    "new_observation = [[2.6,2.6,2.6,0.4]]\n",
    "\n",
    "print('원본 확률값:', clf.fit(features, target).predict_proba(new_observation)) #원본 분류기의 확률값들\n",
    "print('보정된 값:', clf_sigmoid.predict_proba(new_observation)) #보정한 확률값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit([\"첫번째 문서 테스트\", \"두번째 문서 테스트\"])\n",
    "\n",
    "#['두번째', '문서', '첫번째', '테스트'] - 단어 오름차순?\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "counts = vectorizer.transform(['직접 첫번째 테스트 두번째 테스트'])\n",
    "counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "#clf.fit(counts, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering - 비지도 학습, 샘플의 그룹 식별\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris() # 데이터 로드\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler() # 특성 표준화\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = KMeans(n_clusters=3, random_state=0, n_jobs=-1) # k-평균 객체 생성\n",
    "model = cluster.fit(features_std) # 모델 훈련\n",
    "\n",
    "print('예측', model.labels_) # 예측 클래스 확인\n",
    "print('실제', iris.target) # 진짜 클래스 확인\n",
    "\n",
    "new_observation = [[0.8, 0.8, 0.8, 0.8]] #New Sample Data\n",
    "print('예측:', model.predict(new_observation)) # 샘플의 클러스터를 예측\n",
    "model.cluster_centers_ #중심점에 가장 가까운 샘플들(세 개) 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inertia_\n",
    "model.score(features_std)\n",
    "model.transform(new_observation)\n",
    "\n",
    "inertia = []\n",
    "for n in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0, n_jobs=-1)\n",
    "    inertia.append(kmeans.fit(features_std).inertia_)\n",
    "\n",
    "#시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 10), inertia)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미니배치 k-평균은 랜덤 샘플에 대해서만 수행 - 성능을 조금 희생하고 학습 시간 대폭 줄임\n",
    "#매개변수 batch_size는 각 배치에 선택할 샘플의 수\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = MiniBatchKMeans(n_clusters=3,\n",
    "                          random_state=0,\n",
    "                          batch_size=100)\n",
    "model = cluster.fit(features_std)\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_kmeans = MiniBatchKMeans()\n",
    "for i in range(3):\n",
    "    mb_kmeans.partial_fit(features_std[i*50:(i+1)*50])\n",
    "    #훈련 세트가 너무 클 때 하나의 넘파이 배열로 전달하기 어려움\n",
    "    #데이터를 조금씩 전달하며 훈련하는 partial_fit() 사용\n",
    "\n",
    "mb_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균이동을 사용한 군집 - 거리 중심이 아닌 밀도 중심?\n",
    "#클러스터 수와 모양을 가정하지 않음\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = MeanShift(n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "print(model.labels_)\n",
    "print(model.cluster_centers_) #중심점에 가장 가까운 샘플 둘 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris_df = pd.DataFrame(data=iris.data,\n",
    "                       columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++',\n",
    "               max_iter=300, random_state=0)\n",
    "kmeans.fit(iris_df)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "\n",
    "iris_df['target'] = iris.target\n",
    "iris_df['cluster'] = kmeans.labels_\n",
    "iris_result = iris_df.groupby(['target', 'cluster'])['sepal_width'].count()\n",
    "#target과 cluster로 묶어서 count로 출력(count할 index는 상관 x 세기만 하면 됨)\n",
    "print(iris_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전통적인 K-Means는 아래와 같은 원리로 진행된다.\n",
    "\n",
    "일단 K개의 임의의 중심점(centroid)을 배치하고 각 데이터들을 가장 가까운 중심점으로 할당한다. (일종의 군집을 형성한다.) 군집으로 지정된 데이터들을 기반으로 해당 군집의 중심점을 업데이트한다. 2번, 3번 단계를 그래서 수렴이 될 때까지, 즉 더이상 중심점이 업데이트 되지 않을 때까지 반복한다. 그러나 K-Means++는 좀 다르다. K-Means에서 가장 첫번째 단계, 즉 중심점을 배치하는 걸 그냥 임의로 하는 대신 좀 더 신중하게(?) 하는 거다.\n",
    "\n",
    "(일단 아무 공간에나 중심점을 k개 찍고 시작하는 게 아니라) 가지고 있는 데이터 포인트 중에서 무작위로 1개를 선택하여 그 녀석을 첫번째 중심점으로 지정한다. 나머지 데이터 포인트들에 대해 그 첫번째 중심점까지의 거리를 계산한다. 두번째 중심점은 각 점들로부터 거리비례 확률에 따라 선택한다. (뭔 소리야?) 즉, 이미 지정된 중심점으로부터 최대한 먼 곳에 배치된 데이터포인트를 그 다음 중심점으로 지정한다는 뜻이다. 중심점이 k개가 될 때까지 2, 3번을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클러스터 알고리즘 테스트\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=200, n_features=2,\n",
    "                  centers=3, cluster_std=0.8,\n",
    "                  random_state=0)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts = True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])\n",
    "cluster_df['target'] = y\n",
    "cluster_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans 객체를 이용하여 X 데이터를 K-Means 클러스터링 수행\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++',\n",
    "                max_iter=200, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "cluster_df['kmeans_label'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_centers_는 개별 클러스터의 중심 위치 좌표 시각화를 위해 추출\n",
    "centers = kmeans.cluster_centers_\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "markers = ['o', 's', 'p', 'P', 'D', 'H', 'x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#군집된 label 유형별로 iteration 하면서 marker별로 scatter plot 시각화\n",
    "#먼저 샘플들 시각화\n",
    "for label in unique_labels:\n",
    "    label_cluster = cluster_df[cluster_df['kmeans_label'] == label]\n",
    "    center_x_y = centers[label]\n",
    "    plt.scatter(x=label_cluster['ftr1'],\n",
    "                y=label_cluster['ftr2'],\n",
    "                edgecolor='k',\n",
    "                marker=markers[label]) #마커 모양\n",
    "    \n",
    "    #군집별 중심 위치 좌표 시각화\n",
    "    #하얀 중심원\n",
    "    plt.scatter(x=center_x_y[0], y=center_x_y[1],\n",
    "               s=350, color='white', alpha=0.9,\n",
    "               edgecolor='k', marker=markers[label]) #샘플의 마커 모양 따라서\n",
    "    #그 안에 군집 숫자\n",
    "    plt.scatter(x=center_x_y[0], y=center_x_y[1],\n",
    "               s=70, color='k', edgecolor='k',\n",
    "                marker='$%d$' % label) #중심 좌표에 라벨 숫자 표시    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_df.groupby('target')['kmeans_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "feature_names = ['sepal_length', 'sepal_width',\n",
    "                 'petal_length', 'petal_width']\n",
    "iris_df = pd.DataFrame(data=iris.data,\n",
    "                       columns=feature_names)\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++',\n",
    "               max_iter=300, random_state=0).fit(iris_df)\n",
    "iris_df['cluster'] = kmeans.labels_ #예측 군집을 df에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "#iris의 모든 개별 데이터에 실루엣 계수값을 구함\n",
    "#실루엣 계수: -1~1, 1에 가까울수록 군집화가 잘 된 것\n",
    "score_samples = silhouette_samples(iris.data, iris_df['cluster'])\n",
    "print('silhouette_samples() return 값의 shape', score_samples.shape)\n",
    "\n",
    "#iris_df에 실루엣 계수 컬럼 추가\n",
    "iris_df['silhouette_coeff'] = score_samples\n",
    "\n",
    "#모든 데이터의 평균 실루엣 계수값을 구함\n",
    "average_score = np.mean(score_samples)\n",
    "print('total average silhouette score', average_score)\n",
    "iris_df.groupby('cluster')['silhouette_coeff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density-based Spatial Clustering of Applications with Noise, DBSCAN\n",
    "#각각의 데이터들에 대해 이웃한 데이터와의 밀도를 계산하며 불특정한 모양의 클러스터를 생성\n",
    "#pdf. 14, page. 13\n",
    "\n",
    "#매개변수 eps: 다른 샘플을 이웃으로 고려하기 위한 최대 거리\n",
    "#min_samples: 핵심 샘플이 되기 위한 eps 내 필요 최소 샘플 수\n",
    "#metric: eps에서 사용할 거리 측정 방식\n",
    "#찾은 핵심 샘플의 인덱스는 core_sample_indices_ 속성에 저장됨\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = DBSCAN(n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "model.labels_\n",
    "model.core_sample_indices_\n",
    "cluster.fit_predict(features_std)\n",
    "#-1, 0 1 세 개의 군집으로 나뉘는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#계층적 병합을 사용한 군집\n",
    "#모든 샘플이 각자 하나의 클러스터로 시작 - 조건에 부합하는 클러스터끼리 병합\n",
    "#종료 조건 도달까지 계속 클러스터가 커짐\n",
    "#ex) 매개변수 linkage에 따라 조건 달라짐\n",
    "#- ward 분산, average 샘플 간 평균 거리, complete 샘플 간 최대 거리 최소화하는 병합 전략\n",
    "#single 두 클러스터 샘플 간의 최소 거리를 최소화하는 병합 전략\n",
    "\n",
    "#affinity: linkage에서 사용할 거리 측정 방식 (minkowski, euclidean 등)\n",
    "#n_clusters: 찾을 클러스터 수 (종료 조건?)\n",
    "#labels_ 속성을 사용해 각 샘플이 속한 클러스터 확인 가능\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering as AggClustering\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = AggClustering(n_clusters=3)\n",
    "model = cluster.fit(features_std)\n",
    "model.labels_ #클러스터 소속 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.fit_predict(features_std) #그냥 predict는 없고 fit_predict 써야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이차원 시각화 위한 함수\n",
    "def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):\n",
    "    if iscenter :\n",
    "        centers = clusterobj.cluster_centers_\n",
    "        \n",
    "    unique_labels = np.unique(dataframe[label_name].values)\n",
    "    markers=['o', 's', '^', 'x', '*']\n",
    "    isNoise=False\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_cluster = dataframe[dataframe[label_name]==label]\n",
    "        if label == -1:\n",
    "            cluster_legend = 'Noise'\n",
    "            isNoise=True\n",
    "        else :\n",
    "            cluster_legend = 'Cluster '+str(label)\n",
    "        \n",
    "        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,\\\n",
    "                    edgecolor='k', marker=markers[label], label=cluster_legend)\n",
    "        \n",
    "        if iscenter:\n",
    "            center_x_y = centers[label]\n",
    "            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',\n",
    "                        alpha=0.9, edgecolor='k', marker=markers[label])\n",
    "            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',\\\n",
    "                        edgecolor='k', marker='$%d$' % label)\n",
    "    if isNoise:\n",
    "        legend_loc='upper center'\n",
    "    else: legend_loc='upper right'\n",
    "    \n",
    "    plt.legend(loc=legend_loc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris() #데이터 로드\n",
    "\n",
    "feature_names = ['sepal_length', 'sepal_width',\n",
    "                 'petal_length', 'petal_width']\n",
    "iris_df = pd.DataFrame(data=iris.data,\n",
    "                       columns=feature_names) #데이터 df화\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.6, min_samples=8,\n",
    "               metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(iris.data)\n",
    "\n",
    "iris_df['dbscan_cluster'] = dbscan_labels #dbscan으로 예측한 라벨\n",
    "iris_df['target'] = iris.target #실제 라벨\n",
    "\n",
    "iris_result = iris_df.groupby(['target'])['dbscan_cluster'].value_counts()\n",
    "iris_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차원으로 시각화하기 위해 PCA 이용하여 2차원으로 내림\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca_transformed = pca.fit_transform(iris.data)\n",
    "\n",
    "#visualize_cluster_plot() 함수는 df 안의 ftr1과 ftr2 열을 이용해 시각화하므로\n",
    "#PCA 변환값을 df의 ftr1, ftr2로 삽입\n",
    "iris_df['ftr1'] = pca_transformed[:,0]\n",
    "iris_df['ftr2'] = pca_transformed[:,1]\n",
    "\n",
    "visualize_cluster_plot(dbscan, iris_df,\n",
    "                       'dbscan_cluster',\n",
    "                       iscenter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris() #데이터 로드\n",
    "\n",
    "feature_names = ['sepal_length', 'sepal_width',\n",
    "                 'petal_length', 'petal_width']\n",
    "iris_df = pd.DataFrame(data=iris.data,\n",
    "                       columns=feature_names) #데이터 df화\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#eps=0.8인 경우\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=8,\n",
    "               metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(iris.data)\n",
    "\n",
    "iris_df['dbscan_cluster'] = dbscan_labels #dbscan으로 예측한 라벨\n",
    "iris_df['target'] = iris.target #실제 라벨\n",
    "\n",
    "iris_result = iris_df.groupby(['target'])['dbscan_cluster'].value_counts()\n",
    "iris_result\n",
    "#훨씬 잘 분류함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca_transformed = pca.fit_transform(iris.data)\n",
    "\n",
    "#visualize_cluster_plot() 함수는 df 안의 ftr1과 ftr2 열을 이용해 시각화하므로\n",
    "#PCA 변환값을 df의 ftr1, ftr2로 삽입\n",
    "iris_df['ftr1'] = pca_transformed[:,0]\n",
    "iris_df['ftr2'] = pca_transformed[:,1]\n",
    "\n",
    "visualize_cluster_plot(dbscan, iris_df,\n",
    "                       'dbscan_cluster',\n",
    "                       iscenter=False)\n",
    "#핵심 샘플에 더 많이 포함됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
