{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1598341536679,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "Vg96YzNYHFe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    " \n",
    " \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18491,
     "status": "ok",
     "timestamp": 1598333708469,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "4J-uM_NYZaui",
    "outputId": "bb277836-c91a-4408-c9f7-5a439be7222f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2749,
     "status": "ok",
     "timestamp": 1598346055653,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "9GCffZJIRF-O",
    "outputId": "f7870203-ecd8-41f5-b6ad-b21a15753508"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>...</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "      <th>771</th>\n",
       "      <th>772</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>2044</td>\n",
       "      <td>6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>2046</td>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2047</td>\n",
       "      <td>0</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  digit letter  0  1  2  3  4  ...  776  777  778  779  780  781  782  783\n",
       "0        1      5      L  1  1  1  4  3  ...    0    1    2    4    4    4    3    4\n",
       "1        2      0      B  0  4  0  0  4  ...    0    1    4    1    4    2    1    2\n",
       "2        3      4      L  1  1  2  2  1  ...    3    0    2    0    3    0    2    2\n",
       "3        4      9      D  1  2  0  2  0  ...    2    0    1    4    0    0    1    1\n",
       "4        5      6      A  3  0  2  4  0  ...    3    2    1    3    4    3    1    2\n",
       "...    ...    ...    ... .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "2043  2044      6      V  2  4  3  4  2  ...    2    0    0    1    3    1    4    0\n",
       "2044  2045      1      L  3  2  2  1  1  ...    4    2    1    2    3    4    1    1\n",
       "2045  2046      9      A  4  0  4  0  2  ...    1    1    3    4    2    2    0    0\n",
       "2046  2047      0      Z  2  3  3  0  3  ...    1    1    0    4    1    4    3    1\n",
       "2047  2048      5      Z  4  2  2  1  3  ...    4    0    4    3    2    4    3    4\n",
       "\n",
       "[2048 rows x 787 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('/content/drive/My Drive/cvision_1/train.csv')\n",
    "test = pd.read_csv('/content/drive/My Drive/cvision_1/test.csv')\n",
    "submission = pd.read_csv('/content/drive/My Drive/cvision_1/submission.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpT2mQmsHFfM",
    "outputId": "595271f5-6621-4320-d0b5-6929e0b44407"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>2044</td>\n",
       "      <td>6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>2046</td>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2047</td>\n",
       "      <td>0</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  \\\n",
       "0        1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2   \n",
       "1        2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4   \n",
       "2        3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2   \n",
       "3        4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1   \n",
       "4        5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1   \n",
       "...    ...    ...    ... .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...   \n",
       "2043  2044      6      V  2  4  3  4  2  4  4  ...    0    2    2    0    0   \n",
       "2044  2045      1      L  3  2  2  1  1  4  0  ...    2    3    4    2    1   \n",
       "2045  2046      9      A  4  0  4  0  2  4  4  ...    2    3    1    1    3   \n",
       "2046  2047      0      Z  2  3  3  0  3  0  4  ...    2    3    1    1    0   \n",
       "2047  2048      5      Z  4  2  2  1  3  0  0  ...    4    2    4    0    4   \n",
       "\n",
       "      779  780  781  782  783  \n",
       "0       4    4    4    3    4  \n",
       "1       1    4    2    1    2  \n",
       "2       0    3    0    2    2  \n",
       "3       4    0    0    1    1  \n",
       "4       3    4    3    1    2  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "2043    1    3    1    4    0  \n",
       "2044    2    3    4    1    1  \n",
       "2045    4    2    2    0    0  \n",
       "2046    4    1    4    3    1  \n",
       "2047    3    2    4    3    4  \n",
       "\n",
       "[2048 rows x 787 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local 연결 전용\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('data/cvision/train.csv')\n",
    "test = pd.read_csv('data/cvision/test.csv')\n",
    "submission = pd.read_csv('data/cvision/submission.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1598346059030,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "1zwkSGXiRF-S"
   },
   "outputs": [],
   "source": [
    "#X와 y 분리\n",
    "X = train.drop(['id','digit','letter'], axis=1).values\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "X = X/255.\n",
    "\n",
    "y = train['digit']\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1598346059031,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "XrcmgXVBRF-V"
   },
   "outputs": [],
   "source": [
    "#train과 valid로 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#test\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1598341304472,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "aqxLynpVYoEB",
    "outputId": "578660e7-867c-4bae-d6af-6391d9f485b7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#그림 시각화\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    img = train.iloc[i,3:].values.astype('int64').reshape(28,28)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('letter ' + str(train.iloc[i, 2]))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.where(img >= 135, img, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title('number ' + str(train.iloc[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1598342410301,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "Ec4UH2IrRF-c"
   },
   "outputs": [],
   "source": [
    "#숫자 있는 부분만 추출 (이걸로만 딥러닝해보니 성과 매우 안 좋음,\n",
    "#이 방식 적용할 거면 train, valid, test 모두 이런 형식으로 변형한 후 해야할 듯)\n",
    "\n",
    "train_X_ = np.where(train_X >= 135/255., train_X, 0)\n",
    "valid_X_ = np.where(valid_X >= 135/255., valid_X, 0)\n",
    "test_X_ = np.where(test_X >= 135/255., test_X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1598342411073,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "7gAGejz1kYY7"
   },
   "outputs": [],
   "source": [
    "train_X_2 = np.concatenate([train_X, train_X_])\n",
    "valid_X_2 = np.concatenate([valid_X, valid_X_])\n",
    "\n",
    "train_y_2 = np.concatenate([train_y, train_y])\n",
    "valid_y_2 = np.concatenate([valid_y, valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqeCSgUDRF-f"
   },
   "outputs": [],
   "source": [
    "#별 거 아님 kernel 예시\n",
    "from scipy.signal import correlate2d\n",
    "\n",
    "kernel = np.array(\n",
    "[\n",
    "    [0, -100, 0],\n",
    "    [0, 255, 0],\n",
    "    [0,-100,0]\n",
    "])\n",
    "plt.imshow(correlate2d(train_X[0].reshape(28,28), kernel, mode='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1598346069043,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "fd66eFzY4Mj3"
   },
   "outputs": [],
   "source": [
    "#model_4\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout, MaxPool2D,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10,\n",
    "                            zoom_range=0.10,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=3, activation='relu', input_shape = train_X.shape[1:]))\n",
    "    model.add(Conv2D(32,kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(32,kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(64,kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(64,kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128,kernel_size=4, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRduAyEEAZ9F"
   },
   "outputs": [],
   "source": [
    "#model_5\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10, #10도 돌림\n",
    "                            zoom_range=0.10, #10퍼센트 확대(crop?)\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape = train_X.shape[1:], filters = 32, kernel_size = (3,3), strides = 2, padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1598346094045,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "wDOUiuAiRF-p",
    "outputId": "d302b390-caea-49ce-bb59-a57147f78be3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_98 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,658\n",
      "Trainable params: 243,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1598346095428,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "wPbJftOsRF-s"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.975 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619875,
     "status": "ok",
     "timestamp": 1598346717394,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "vJymm1W6RF-u",
    "outputId": "f4ad5e2d-5d08-48af-cb97-1df4c286547b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 2.2951 - accuracy: 0.1314 - val_loss: 2.2283 - val_accuracy: 0.1854\n",
      "Epoch 2/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 2.1455 - accuracy: 0.2115 - val_loss: 1.7611 - val_accuracy: 0.4878\n",
      "Epoch 3/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.9327 - accuracy: 0.3274 - val_loss: 1.4464 - val_accuracy: 0.5073\n",
      "Epoch 4/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.7993 - accuracy: 0.3876 - val_loss: 1.4248 - val_accuracy: 0.5268\n",
      "Epoch 5/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.7123 - accuracy: 0.3959 - val_loss: 1.2285 - val_accuracy: 0.6098\n",
      "Epoch 6/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.5285 - accuracy: 0.4721 - val_loss: 1.0865 - val_accuracy: 0.6585\n",
      "Epoch 7/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 1.4226 - accuracy: 0.5091 - val_loss: 1.1102 - val_accuracy: 0.6244\n",
      "Epoch 8/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.3286 - accuracy: 0.5439 - val_loss: 0.8998 - val_accuracy: 0.6732\n",
      "Epoch 9/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.2833 - accuracy: 0.5605 - val_loss: 0.8270 - val_accuracy: 0.7512\n",
      "Epoch 10/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.2326 - accuracy: 0.5809 - val_loss: 0.9370 - val_accuracy: 0.6488\n",
      "Epoch 11/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 1.1329 - accuracy: 0.6173 - val_loss: 0.8614 - val_accuracy: 0.6976\n",
      "Epoch 12/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.1034 - accuracy: 0.6256 - val_loss: 0.8238 - val_accuracy: 0.7463\n",
      "Epoch 13/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.0408 - accuracy: 0.6394 - val_loss: 0.8644 - val_accuracy: 0.6732\n",
      "Epoch 14/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.0446 - accuracy: 0.6549 - val_loss: 0.8154 - val_accuracy: 0.7024\n",
      "Epoch 15/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 1.0012 - accuracy: 0.6576 - val_loss: 0.8477 - val_accuracy: 0.7073\n",
      "Epoch 16/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.9574 - accuracy: 0.6742 - val_loss: 0.9636 - val_accuracy: 0.6780\n",
      "Epoch 17/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.9477 - accuracy: 0.6593 - val_loss: 0.7996 - val_accuracy: 0.7073\n",
      "Epoch 18/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.9194 - accuracy: 0.6792 - val_loss: 0.8402 - val_accuracy: 0.6927\n",
      "Epoch 19/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.9335 - accuracy: 0.6875 - val_loss: 0.6183 - val_accuracy: 0.7756\n",
      "Epoch 20/120\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.8771 - accuracy: 0.6869 - val_loss: 0.8499 - val_accuracy: 0.6878\n",
      "Epoch 21/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.8457 - accuracy: 0.7217 - val_loss: 0.8991 - val_accuracy: 0.6634\n",
      "Epoch 22/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.8183 - accuracy: 0.7245 - val_loss: 0.6942 - val_accuracy: 0.7463\n",
      "Epoch 23/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.8414 - accuracy: 0.7145 - val_loss: 0.6907 - val_accuracy: 0.7805\n",
      "Epoch 24/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.8053 - accuracy: 0.7211 - val_loss: 0.8330 - val_accuracy: 0.6780\n",
      "Epoch 25/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.7969 - accuracy: 0.7278 - val_loss: 0.8011 - val_accuracy: 0.7317\n",
      "Epoch 26/120\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.7826 - accuracy: 0.7338 - val_loss: 0.7136 - val_accuracy: 0.7561\n",
      "Epoch 27/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.7687 - accuracy: 0.7405 - val_loss: 0.7736 - val_accuracy: 0.7122\n",
      "Epoch 28/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.7255 - accuracy: 0.7333 - val_loss: 0.6167 - val_accuracy: 0.7805\n",
      "Epoch 29/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.7192 - accuracy: 0.7587 - val_loss: 0.6873 - val_accuracy: 0.7659\n",
      "Epoch 30/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.7275 - accuracy: 0.7620 - val_loss: 0.6293 - val_accuracy: 0.7756\n",
      "Epoch 31/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6925 - accuracy: 0.7565 - val_loss: 0.5580 - val_accuracy: 0.8000\n",
      "Epoch 32/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6851 - accuracy: 0.7626 - val_loss: 0.7464 - val_accuracy: 0.7415\n",
      "Epoch 33/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.7485 - accuracy: 0.7554 - val_loss: 0.5435 - val_accuracy: 0.8195\n",
      "Epoch 34/120\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.6557 - accuracy: 0.7753 - val_loss: 0.6633 - val_accuracy: 0.7707\n",
      "Epoch 35/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6704 - accuracy: 0.7708 - val_loss: 1.0307 - val_accuracy: 0.6390\n",
      "Epoch 36/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6147 - accuracy: 0.7846 - val_loss: 0.5286 - val_accuracy: 0.8488\n",
      "Epoch 37/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6389 - accuracy: 0.7869 - val_loss: 0.7704 - val_accuracy: 0.7268\n",
      "Epoch 38/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6099 - accuracy: 0.7907 - val_loss: 0.7311 - val_accuracy: 0.7610\n",
      "Epoch 39/120\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.6068 - accuracy: 0.7869 - val_loss: 0.8353 - val_accuracy: 0.7415\n",
      "Epoch 40/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6096 - accuracy: 0.8012 - val_loss: 0.6484 - val_accuracy: 0.7659\n",
      "Epoch 41/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6423 - accuracy: 0.7742 - val_loss: 0.5731 - val_accuracy: 0.8049\n",
      "Epoch 42/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6199 - accuracy: 0.7858 - val_loss: 0.6189 - val_accuracy: 0.8098\n",
      "Epoch 43/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.6143 - accuracy: 0.7951 - val_loss: 0.6277 - val_accuracy: 0.8000\n",
      "Epoch 44/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.5820 - accuracy: 0.8056 - val_loss: 0.7377 - val_accuracy: 0.7610\n",
      "Epoch 45/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.5514 - accuracy: 0.8161 - val_loss: 0.5469 - val_accuracy: 0.8098\n",
      "Epoch 46/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.5780 - accuracy: 0.8189 - val_loss: 0.5167 - val_accuracy: 0.8049\n",
      "Epoch 47/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.5748 - accuracy: 0.7946 - val_loss: 0.6258 - val_accuracy: 0.7951\n",
      "Epoch 48/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.5328 - accuracy: 0.8089 - val_loss: 0.6782 - val_accuracy: 0.8098\n",
      "Epoch 49/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.5560 - accuracy: 0.8183 - val_loss: 0.5433 - val_accuracy: 0.8098\n",
      "Epoch 50/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.5415 - accuracy: 0.8134 - val_loss: 0.6208 - val_accuracy: 0.8049\n",
      "Epoch 51/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.5587 - accuracy: 0.8084 - val_loss: 0.4741 - val_accuracy: 0.8341\n",
      "Epoch 52/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.5265 - accuracy: 0.8327 - val_loss: 0.5380 - val_accuracy: 0.8195\n",
      "Epoch 53/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.5089 - accuracy: 0.8349 - val_loss: 0.6767 - val_accuracy: 0.7805\n",
      "Epoch 54/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4914 - accuracy: 0.8277 - val_loss: 0.5387 - val_accuracy: 0.8537\n",
      "Epoch 55/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4862 - accuracy: 0.8388 - val_loss: 0.6321 - val_accuracy: 0.8098\n",
      "Epoch 56/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.5015 - accuracy: 0.8239 - val_loss: 0.5905 - val_accuracy: 0.7951\n",
      "Epoch 57/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.5026 - accuracy: 0.8288 - val_loss: 0.6228 - val_accuracy: 0.8146\n",
      "Epoch 58/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4808 - accuracy: 0.8383 - val_loss: 0.7290 - val_accuracy: 0.7902\n",
      "Epoch 59/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4854 - accuracy: 0.8360 - val_loss: 0.6225 - val_accuracy: 0.7951\n",
      "Epoch 60/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4742 - accuracy: 0.8366 - val_loss: 0.6864 - val_accuracy: 0.8098\n",
      "Epoch 61/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4830 - accuracy: 0.8321 - val_loss: 0.6171 - val_accuracy: 0.8098\n",
      "Epoch 62/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4554 - accuracy: 0.8321 - val_loss: 0.6485 - val_accuracy: 0.7854\n",
      "Epoch 63/120\n",
      "57/57 [==============================] - 8s 139ms/step - loss: 0.4796 - accuracy: 0.8366 - val_loss: 0.6392 - val_accuracy: 0.7805\n",
      "Epoch 64/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4724 - accuracy: 0.8421 - val_loss: 0.5261 - val_accuracy: 0.8341\n",
      "Epoch 65/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4568 - accuracy: 0.8360 - val_loss: 0.7813 - val_accuracy: 0.7512\n",
      "Epoch 66/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4137 - accuracy: 0.8581 - val_loss: 0.6967 - val_accuracy: 0.7902\n",
      "Epoch 67/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4374 - accuracy: 0.8504 - val_loss: 0.7268 - val_accuracy: 0.7756\n",
      "Epoch 68/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4457 - accuracy: 0.8537 - val_loss: 0.6813 - val_accuracy: 0.8049\n",
      "Epoch 69/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.5061 - accuracy: 0.8349 - val_loss: 0.6503 - val_accuracy: 0.8049\n",
      "Epoch 70/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4501 - accuracy: 0.8454 - val_loss: 0.7851 - val_accuracy: 0.7659\n",
      "Epoch 71/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4233 - accuracy: 0.8548 - val_loss: 0.6085 - val_accuracy: 0.8049\n",
      "Epoch 72/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4364 - accuracy: 0.8377 - val_loss: 0.6457 - val_accuracy: 0.8049\n",
      "Epoch 73/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4348 - accuracy: 0.8493 - val_loss: 0.5473 - val_accuracy: 0.8146\n",
      "Epoch 74/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4403 - accuracy: 0.8465 - val_loss: 0.6288 - val_accuracy: 0.7951\n",
      "Epoch 75/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3982 - accuracy: 0.8620 - val_loss: 0.7626 - val_accuracy: 0.7805\n",
      "Epoch 76/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4101 - accuracy: 0.8686 - val_loss: 0.6279 - val_accuracy: 0.7951\n",
      "Epoch 77/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4333 - accuracy: 0.8493 - val_loss: 0.6301 - val_accuracy: 0.8049\n",
      "Epoch 78/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4099 - accuracy: 0.8614 - val_loss: 0.5502 - val_accuracy: 0.8634\n",
      "Epoch 79/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4178 - accuracy: 0.8548 - val_loss: 0.7486 - val_accuracy: 0.7707\n",
      "Epoch 80/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4109 - accuracy: 0.8586 - val_loss: 0.6545 - val_accuracy: 0.8098\n",
      "Epoch 81/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4139 - accuracy: 0.8603 - val_loss: 0.5584 - val_accuracy: 0.8390\n",
      "Epoch 82/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3938 - accuracy: 0.8620 - val_loss: 0.6108 - val_accuracy: 0.8390\n",
      "Epoch 83/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3893 - accuracy: 0.8581 - val_loss: 0.5122 - val_accuracy: 0.8537\n",
      "Epoch 84/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3891 - accuracy: 0.8624 - val_loss: 0.6234 - val_accuracy: 0.8000\n",
      "Epoch 85/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4025 - accuracy: 0.8631 - val_loss: 0.5974 - val_accuracy: 0.8195\n",
      "Epoch 86/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3951 - accuracy: 0.8625 - val_loss: 0.6657 - val_accuracy: 0.7902\n",
      "Epoch 87/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3653 - accuracy: 0.8821 - val_loss: 0.6508 - val_accuracy: 0.8049\n",
      "Epoch 88/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.4835 - accuracy: 0.8426 - val_loss: 0.5619 - val_accuracy: 0.8146\n",
      "Epoch 89/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3696 - accuracy: 0.8658 - val_loss: 0.6577 - val_accuracy: 0.8049\n",
      "Epoch 90/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3914 - accuracy: 0.8658 - val_loss: 0.6501 - val_accuracy: 0.7951\n",
      "Epoch 91/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3883 - accuracy: 0.8603 - val_loss: 0.7688 - val_accuracy: 0.7610\n",
      "Epoch 92/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3925 - accuracy: 0.8597 - val_loss: 0.5104 - val_accuracy: 0.8195\n",
      "Epoch 93/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3760 - accuracy: 0.8769 - val_loss: 0.8499 - val_accuracy: 0.7610\n",
      "Epoch 94/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3862 - accuracy: 0.8686 - val_loss: 0.6642 - val_accuracy: 0.8293\n",
      "Epoch 95/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3838 - accuracy: 0.8741 - val_loss: 0.6184 - val_accuracy: 0.8293\n",
      "Epoch 96/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3228 - accuracy: 0.8929 - val_loss: 0.6284 - val_accuracy: 0.8537\n",
      "Epoch 97/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3981 - accuracy: 0.8631 - val_loss: 0.6901 - val_accuracy: 0.7854\n",
      "Epoch 98/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3527 - accuracy: 0.8791 - val_loss: 0.8570 - val_accuracy: 0.7415\n",
      "Epoch 99/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3670 - accuracy: 0.8713 - val_loss: 0.5532 - val_accuracy: 0.7951\n",
      "Epoch 100/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3599 - accuracy: 0.8758 - val_loss: 0.7251 - val_accuracy: 0.7561\n",
      "Epoch 101/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3333 - accuracy: 0.8901 - val_loss: 0.7492 - val_accuracy: 0.7805\n",
      "Epoch 102/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3577 - accuracy: 0.8907 - val_loss: 0.6396 - val_accuracy: 0.7707\n",
      "Epoch 103/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3765 - accuracy: 0.8807 - val_loss: 0.6007 - val_accuracy: 0.7805\n",
      "Epoch 104/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3304 - accuracy: 0.8835 - val_loss: 0.6114 - val_accuracy: 0.8244\n",
      "Epoch 105/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.3224 - accuracy: 0.8912 - val_loss: 0.7470 - val_accuracy: 0.8049\n",
      "Epoch 106/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3289 - accuracy: 0.8840 - val_loss: 0.6060 - val_accuracy: 0.7951\n",
      "Epoch 107/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3974 - accuracy: 0.8669 - val_loss: 0.6742 - val_accuracy: 0.7512\n",
      "Epoch 108/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.3208 - accuracy: 0.8923 - val_loss: 0.6524 - val_accuracy: 0.8293\n",
      "Epoch 109/120\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.3221 - accuracy: 0.8890 - val_loss: 0.5417 - val_accuracy: 0.8146\n",
      "Epoch 110/120\n",
      "57/57 [==============================] - 5s 94ms/step - loss: 0.3431 - accuracy: 0.8796 - val_loss: 0.6656 - val_accuracy: 0.8098\n",
      "Epoch 111/120\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.3531 - accuracy: 0.8890 - val_loss: 0.6917 - val_accuracy: 0.7902\n",
      "Epoch 112/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3295 - accuracy: 0.8901 - val_loss: 0.6772 - val_accuracy: 0.8049\n",
      "Epoch 113/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3226 - accuracy: 0.8785 - val_loss: 0.8028 - val_accuracy: 0.8098\n",
      "Epoch 114/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3346 - accuracy: 0.8890 - val_loss: 0.6229 - val_accuracy: 0.8146\n",
      "Epoch 115/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3334 - accuracy: 0.8885 - val_loss: 0.3923 - val_accuracy: 0.8634\n",
      "Epoch 116/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3263 - accuracy: 0.8874 - val_loss: 0.6834 - val_accuracy: 0.7756\n",
      "Epoch 117/120\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.3583 - accuracy: 0.8807 - val_loss: 0.6107 - val_accuracy: 0.8146\n",
      "Epoch 118/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3388 - accuracy: 0.8868 - val_loss: 0.5205 - val_accuracy: 0.8293\n",
      "Epoch 119/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3063 - accuracy: 0.8945 - val_loss: 0.6261 - val_accuracy: 0.8195\n",
      "Epoch 120/120\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.3090 - accuracy: 0.8967 - val_loss: 0.5123 - val_accuracy: 0.8390\n",
      "CNN: Epochs=120, Train accuracy=0.89674, Validation accuracy=0.86341\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1\n",
    "    #callbacks=[annealer],\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1211,
     "status": "ok",
     "timestamp": 1598346862296,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "kB5QQWDY3yA_",
    "outputId": "79685c32-1913-43f3-c609-3bffa4d69a30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XF1MPFKyRF-w"
   },
   "outputs": [],
   "source": [
    "#Parameter 저장\n",
    "model.save_weights(f'/content/drive/My Drive/cvision_1/params_4.h5')\n",
    "\n",
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'/content/drive/My Drive/cvision_1/model_4.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fjm7D-7jRF-z"
   },
   "outputs": [],
   "source": [
    "#예측 진행\n",
    "res = model.predict(test_X)\n",
    "\n",
    "#submission 파일 생성\n",
    "for i in range(len(res)):\n",
    "    submission.digit[i] = int(res[i].argmax())\n",
    "submission.to_csv('/content/drive/My Drive/cvision_1/my_subm_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9V1sl_PZkAw"
   },
   "outputs": [],
   "source": [
    "#model_6\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10, #10도 돌림\n",
    "                            zoom_range=0.10, #10퍼센트 확대(crop?)\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape = train_X.shape[1:], filters = 32, kernel_size = (3,3), strides = 2, padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J4dDK7WcMjX"
   },
   "outputs": [],
   "source": [
    "#model_6\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10, #10도 돌림\n",
    "                            zoom_range=0.1, #10퍼센트 확대(crop?)\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape = train_X.shape[1:], filters = 32, kernel_size = (3,3), strides = 2, padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1598345875731,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "bG2wRBFqSufq",
    "outputId": "2c778347-c8f5-4999-ad0d-ed34a4ec96b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 133,648\n",
      "Trainable params: 133,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q73HFCn7TfqS"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjESPwIkTb3s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1\n",
    "    #callbacks=[annealer]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ppp3z5-lXawC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTtpmPijTjmk"
   },
   "outputs": [],
   "source": [
    "#Parameter 저장\n",
    "model.save_weights(f'/content/drive/My Drive/cvision_1/params_6.h5')\n",
    "\n",
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'/content/drive/My Drive/cvision_1/model_6.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFSBLgQKTmM2"
   },
   "outputs": [],
   "source": [
    "#예측 진행\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255.\n",
    "\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('/content/drive/My Drive/cvision_1/my_subm_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcD4AYCGQ9Np"
   },
   "outputs": [],
   "source": [
    "#model_8인데 데이터 부풀리기 x, 학습률 감소 x\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "'''\n",
    "datagen = ImageDataGenerator(rotation_range=10, #10도 돌림\n",
    "                            zoom_range=0.1, #10퍼센트 확대(crop?)\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "'''\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape = train_X.shape[1:], filters = 32, kernel_size = (3,3), strides = 2, padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnWl2qzoURfw"
   },
   "outputs": [],
   "source": [
    "#model_8인데 데이터 부풀리기 x, 학습률 감소 x\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "'''\n",
    "datagen = ImageDataGenerator(rotation_range=10, #10도 돌림\n",
    "                            zoom_range=0.1, #10퍼센트 확대(crop?)\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "'''\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(input_shape = train_X.shape[1:], filters = 32, kernel_size = (3,3), strides = 2, padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pO2aXT2wRIxn"
   },
   "outputs": [],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS8BcL7CRNa7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1\n",
    "    #callbacks=[annealer]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bseu0T8lRl3X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2moYVE1jTLc"
   },
   "outputs": [],
   "source": [
    "#Parameter 저장\n",
    "model.save_weights(f'/content/drive/My Drive/cvision_1/params_9.h5')\n",
    "\n",
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'/content/drive/My Drive/cvision_1/model_9.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDMU0LnTjiwo"
   },
   "outputs": [],
   "source": [
    "#예측 진행\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255.\n",
    "\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('/content/drive/My Drive/cvision_1/my_subm_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 934,
     "status": "ok",
     "timestamp": 1598347121753,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "sTKcFoyikcjV"
   },
   "outputs": [],
   "source": [
    "#model_9-3 9-2에서 optimizer만 RMSprop로 바꿔봄 - Adam보다 높게 나오더라\n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), Adagrad(lr=0.01, epsilon=None, decay=0.0), adadelta, adam\n",
    "    model.compile(optimizer=RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1598347122067,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "ySu34IFkkhNe",
    "outputId": "e3d46b6e-e956-4a26-f1bc-aa4c0e4c3b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 133,648\n",
      "Trainable params: 133,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1598347122783,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "tPFielyJ5yMH"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** x)\n",
    "\n",
    "#earlystopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537462,
     "status": "error",
     "timestamp": 1598347660504,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "yeNCbX2vko4s",
    "outputId": "012ce4a9-2ca8-41d3-d1de-9331032469e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.3137 - accuracy: 0.1050 - val_loss: 2.2994 - val_accuracy: 0.1024\n",
      "Epoch 2/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.2979 - accuracy: 0.1190 - val_loss: 2.2992 - val_accuracy: 0.0829\n",
      "Epoch 3/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.2917 - accuracy: 0.1149 - val_loss: 2.2879 - val_accuracy: 0.1073\n",
      "Epoch 4/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.2634 - accuracy: 0.1411 - val_loss: 2.2053 - val_accuracy: 0.1317\n",
      "Epoch 5/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.2351 - accuracy: 0.1656 - val_loss: 2.2253 - val_accuracy: 0.2000\n",
      "Epoch 6/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.1605 - accuracy: 0.2169 - val_loss: 2.1426 - val_accuracy: 0.3366\n",
      "Epoch 7/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.1125 - accuracy: 0.2315 - val_loss: 1.9375 - val_accuracy: 0.3463\n",
      "Epoch 8/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 2.0783 - accuracy: 0.2344 - val_loss: 1.7914 - val_accuracy: 0.4049\n",
      "Epoch 9/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.0334 - accuracy: 0.2536 - val_loss: 1.8990 - val_accuracy: 0.4488\n",
      "Epoch 10/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.9846 - accuracy: 0.2776 - val_loss: 1.8192 - val_accuracy: 0.3854\n",
      "Epoch 11/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.9631 - accuracy: 0.2980 - val_loss: 1.7459 - val_accuracy: 0.5171\n",
      "Epoch 12/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.8743 - accuracy: 0.3306 - val_loss: 1.6804 - val_accuracy: 0.4390\n",
      "Epoch 13/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.9031 - accuracy: 0.3259 - val_loss: 1.5386 - val_accuracy: 0.5366\n",
      "Epoch 14/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.8519 - accuracy: 0.3449 - val_loss: 1.6745 - val_accuracy: 0.4976\n",
      "Epoch 15/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.8122 - accuracy: 0.3580 - val_loss: 1.7570 - val_accuracy: 0.4732\n",
      "Epoch 16/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.8115 - accuracy: 0.3621 - val_loss: 1.5626 - val_accuracy: 0.5756\n",
      "Epoch 17/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.7353 - accuracy: 0.4070 - val_loss: 1.6524 - val_accuracy: 0.5024\n",
      "Epoch 18/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.7391 - accuracy: 0.3784 - val_loss: 1.3797 - val_accuracy: 0.6000\n",
      "Epoch 19/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7041 - accuracy: 0.4041 - val_loss: 1.3833 - val_accuracy: 0.6000\n",
      "Epoch 20/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.6650 - accuracy: 0.4332 - val_loss: 1.4773 - val_accuracy: 0.5707\n",
      "Epoch 21/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.6948 - accuracy: 0.4175 - val_loss: 1.3674 - val_accuracy: 0.5707\n",
      "Epoch 22/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.6400 - accuracy: 0.4315 - val_loss: 1.4132 - val_accuracy: 0.5756\n",
      "Epoch 23/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.5886 - accuracy: 0.4472 - val_loss: 1.4005 - val_accuracy: 0.5171\n",
      "Epoch 24/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.6244 - accuracy: 0.4513 - val_loss: 1.2447 - val_accuracy: 0.6000\n",
      "Epoch 25/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.5549 - accuracy: 0.4601 - val_loss: 1.2125 - val_accuracy: 0.6146\n",
      "Epoch 26/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.5409 - accuracy: 0.4706 - val_loss: 1.1969 - val_accuracy: 0.6195\n",
      "Epoch 27/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.5288 - accuracy: 0.4752 - val_loss: 1.2017 - val_accuracy: 0.6537\n",
      "Epoch 28/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4815 - accuracy: 0.4886 - val_loss: 1.1513 - val_accuracy: 0.6098\n",
      "Epoch 29/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.5217 - accuracy: 0.4898 - val_loss: 1.2059 - val_accuracy: 0.6439\n",
      "Epoch 30/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.4624 - accuracy: 0.5073 - val_loss: 1.2834 - val_accuracy: 0.6390\n",
      "Epoch 31/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4406 - accuracy: 0.5085 - val_loss: 1.4618 - val_accuracy: 0.5756\n",
      "Epoch 32/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4337 - accuracy: 0.5160 - val_loss: 1.2435 - val_accuracy: 0.5512\n",
      "Epoch 33/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4281 - accuracy: 0.4997 - val_loss: 1.1841 - val_accuracy: 0.6829\n",
      "Epoch 34/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.4177 - accuracy: 0.5236 - val_loss: 1.2278 - val_accuracy: 0.6000\n",
      "Epoch 35/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.3310 - accuracy: 0.5464 - val_loss: 1.0392 - val_accuracy: 0.6585\n",
      "Epoch 36/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3752 - accuracy: 0.5353 - val_loss: 1.0386 - val_accuracy: 0.7122\n",
      "Epoch 37/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3087 - accuracy: 0.5551 - val_loss: 1.1950 - val_accuracy: 0.5951\n",
      "Epoch 38/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3335 - accuracy: 0.5487 - val_loss: 1.1840 - val_accuracy: 0.6146\n",
      "Epoch 39/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2943 - accuracy: 0.5726 - val_loss: 1.0413 - val_accuracy: 0.6683\n",
      "Epoch 40/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.3015 - accuracy: 0.5569 - val_loss: 1.1696 - val_accuracy: 0.6293\n",
      "Epoch 41/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2630 - accuracy: 0.5528 - val_loss: 1.1251 - val_accuracy: 0.6390\n",
      "Epoch 42/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2691 - accuracy: 0.5633 - val_loss: 1.0323 - val_accuracy: 0.7073\n",
      "Epoch 43/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.2314 - accuracy: 0.5825 - val_loss: 1.0410 - val_accuracy: 0.6927\n",
      "Epoch 44/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1781 - accuracy: 0.5960 - val_loss: 0.9626 - val_accuracy: 0.6976\n",
      "Epoch 45/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2428 - accuracy: 0.5778 - val_loss: 1.0101 - val_accuracy: 0.6683\n",
      "Epoch 46/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.2050 - accuracy: 0.5819 - val_loss: 0.9217 - val_accuracy: 0.7122\n",
      "Epoch 47/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1894 - accuracy: 0.5930 - val_loss: 1.0405 - val_accuracy: 0.6927\n",
      "Epoch 48/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.1784 - accuracy: 0.6117 - val_loss: 0.9913 - val_accuracy: 0.6634\n",
      "Epoch 49/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.1850 - accuracy: 0.6076 - val_loss: 0.8585 - val_accuracy: 0.7463\n",
      "Epoch 50/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.1622 - accuracy: 0.5918 - val_loss: 0.9009 - val_accuracy: 0.7073\n",
      "Epoch 51/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.1472 - accuracy: 0.5936 - val_loss: 0.9652 - val_accuracy: 0.6878\n",
      "Epoch 52/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1393 - accuracy: 0.6058 - val_loss: 0.9623 - val_accuracy: 0.6976\n",
      "Epoch 53/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1472 - accuracy: 0.6082 - val_loss: 0.8001 - val_accuracy: 0.7463\n",
      "Epoch 54/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1248 - accuracy: 0.6157 - val_loss: 0.8392 - val_accuracy: 0.7463\n",
      "Epoch 55/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0758 - accuracy: 0.6338 - val_loss: 0.9767 - val_accuracy: 0.7171\n",
      "Epoch 56/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0931 - accuracy: 0.6350 - val_loss: 1.0289 - val_accuracy: 0.6780\n",
      "Epoch 57/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0596 - accuracy: 0.6542 - val_loss: 0.9533 - val_accuracy: 0.7171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0952 - accuracy: 0.6245 - val_loss: 0.7775 - val_accuracy: 0.7756\n",
      "Epoch 59/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0050 - accuracy: 0.6612 - val_loss: 0.8179 - val_accuracy: 0.7220\n",
      "Epoch 60/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0570 - accuracy: 0.6397 - val_loss: 0.8520 - val_accuracy: 0.7171\n",
      "Epoch 61/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0098 - accuracy: 0.6612 - val_loss: 0.8856 - val_accuracy: 0.7561\n",
      "Epoch 62/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0176 - accuracy: 0.6490 - val_loss: 0.7301 - val_accuracy: 0.8049\n",
      "Epoch 63/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9999 - accuracy: 0.6548 - val_loss: 0.8837 - val_accuracy: 0.7561\n",
      "Epoch 64/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0305 - accuracy: 0.6519 - val_loss: 0.8437 - val_accuracy: 0.7561\n",
      "Epoch 65/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9990 - accuracy: 0.6379 - val_loss: 0.8740 - val_accuracy: 0.7171\n",
      "Epoch 66/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0010 - accuracy: 0.6554 - val_loss: 0.7478 - val_accuracy: 0.7805\n",
      "Epoch 67/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9725 - accuracy: 0.6758 - val_loss: 1.0031 - val_accuracy: 0.7024\n",
      "Epoch 68/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9704 - accuracy: 0.6797 - val_loss: 1.1076 - val_accuracy: 0.6293\n",
      "Epoch 69/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9501 - accuracy: 0.6857 - val_loss: 0.6888 - val_accuracy: 0.8098\n",
      "Epoch 70/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9670 - accuracy: 0.6758 - val_loss: 0.7598 - val_accuracy: 0.8000\n",
      "Epoch 71/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9369 - accuracy: 0.6700 - val_loss: 0.7456 - val_accuracy: 0.7951\n",
      "Epoch 72/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9540 - accuracy: 0.6810 - val_loss: 0.6785 - val_accuracy: 0.8146\n",
      "Epoch 73/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9302 - accuracy: 0.6816 - val_loss: 0.7724 - val_accuracy: 0.7610\n",
      "Epoch 74/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9083 - accuracy: 0.6968 - val_loss: 0.7161 - val_accuracy: 0.7951\n",
      "Epoch 75/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9073 - accuracy: 0.6945 - val_loss: 0.7624 - val_accuracy: 0.7854\n",
      "Epoch 76/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8738 - accuracy: 0.6974 - val_loss: 0.7006 - val_accuracy: 0.7951\n",
      "Epoch 77/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9157 - accuracy: 0.7020 - val_loss: 0.6790 - val_accuracy: 0.8000\n",
      "Epoch 78/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8777 - accuracy: 0.7020 - val_loss: 0.7533 - val_accuracy: 0.7805\n",
      "Epoch 79/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9001 - accuracy: 0.6945 - val_loss: 0.6703 - val_accuracy: 0.8293\n",
      "Epoch 80/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8808 - accuracy: 0.7026 - val_loss: 0.7521 - val_accuracy: 0.8000\n",
      "Epoch 81/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8605 - accuracy: 0.7131 - val_loss: 0.6670 - val_accuracy: 0.8390\n",
      "Epoch 82/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8643 - accuracy: 0.7009 - val_loss: 0.6388 - val_accuracy: 0.8195\n",
      "Epoch 83/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8503 - accuracy: 0.7079 - val_loss: 0.6703 - val_accuracy: 0.8098\n",
      "Epoch 84/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8548 - accuracy: 0.7038 - val_loss: 0.8067 - val_accuracy: 0.7512\n",
      "Epoch 85/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8498 - accuracy: 0.7096 - val_loss: 1.1198 - val_accuracy: 0.6293\n",
      "Epoch 86/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8238 - accuracy: 0.7289 - val_loss: 0.6997 - val_accuracy: 0.7951\n",
      "Epoch 87/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8355 - accuracy: 0.7160 - val_loss: 0.6511 - val_accuracy: 0.8146\n",
      "Epoch 88/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8203 - accuracy: 0.7085 - val_loss: 0.9584 - val_accuracy: 0.7024\n",
      "Epoch 89/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7789 - accuracy: 0.7300 - val_loss: 0.7337 - val_accuracy: 0.7756\n",
      "Epoch 90/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7945 - accuracy: 0.7248 - val_loss: 0.5618 - val_accuracy: 0.8244\n",
      "Epoch 91/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7694 - accuracy: 0.7300 - val_loss: 0.5087 - val_accuracy: 0.8293\n",
      "Epoch 92/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7969 - accuracy: 0.7289 - val_loss: 0.7531 - val_accuracy: 0.7610\n",
      "Epoch 93/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7866 - accuracy: 0.7347 - val_loss: 0.6092 - val_accuracy: 0.8195\n",
      "Epoch 94/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8279 - accuracy: 0.7195 - val_loss: 0.6353 - val_accuracy: 0.8000\n",
      "Epoch 95/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8162 - accuracy: 0.7248 - val_loss: 0.6623 - val_accuracy: 0.7951\n",
      "Epoch 96/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7849 - accuracy: 0.7405 - val_loss: 0.5537 - val_accuracy: 0.8195\n",
      "Epoch 97/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7967 - accuracy: 0.7329 - val_loss: 0.7493 - val_accuracy: 0.7463\n",
      "Epoch 98/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7308 - accuracy: 0.7405 - val_loss: 0.6243 - val_accuracy: 0.7902\n",
      "Epoch 99/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8073 - accuracy: 0.7149 - val_loss: 0.7571 - val_accuracy: 0.7707\n",
      "Epoch 100/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7684 - accuracy: 0.7341 - val_loss: 0.7345 - val_accuracy: 0.7756\n",
      "Epoch 101/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8066 - accuracy: 0.7370 - val_loss: 0.6663 - val_accuracy: 0.7902\n",
      "Epoch 102/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7278 - accuracy: 0.7528 - val_loss: 0.8144 - val_accuracy: 0.7366\n",
      "Epoch 103/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7431 - accuracy: 0.7545 - val_loss: 0.6772 - val_accuracy: 0.7512\n",
      "Epoch 104/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7485 - accuracy: 0.7469 - val_loss: 0.6889 - val_accuracy: 0.7756\n",
      "Epoch 105/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7203 - accuracy: 0.7499 - val_loss: 0.5121 - val_accuracy: 0.8537\n",
      "Epoch 106/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7964 - accuracy: 0.7254 - val_loss: 0.5688 - val_accuracy: 0.8390\n",
      "Epoch 107/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7126 - accuracy: 0.7592 - val_loss: 0.6368 - val_accuracy: 0.8049\n",
      "Epoch 108/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6885 - accuracy: 0.7638 - val_loss: 0.6485 - val_accuracy: 0.7854\n",
      "Epoch 109/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6934 - accuracy: 0.7545 - val_loss: 0.8360 - val_accuracy: 0.7463\n",
      "Epoch 110/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7239 - accuracy: 0.7522 - val_loss: 0.5662 - val_accuracy: 0.8195\n",
      "Epoch 111/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6952 - accuracy: 0.7633 - val_loss: 0.5763 - val_accuracy: 0.8195\n",
      "Epoch 112/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7722 - accuracy: 0.7364 - val_loss: 0.7476 - val_accuracy: 0.7415\n",
      "Epoch 113/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6661 - accuracy: 0.7732 - val_loss: 0.5701 - val_accuracy: 0.8195\n",
      "Epoch 114/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7135 - accuracy: 0.7493 - val_loss: 0.5367 - val_accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6840 - accuracy: 0.7621 - val_loss: 0.6113 - val_accuracy: 0.8195\n",
      "Epoch 116/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6871 - accuracy: 0.7718 - val_loss: 0.6718 - val_accuracy: 0.7610\n",
      "Epoch 117/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7128 - accuracy: 0.7475 - val_loss: 0.5297 - val_accuracy: 0.8341\n",
      "Epoch 118/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6597 - accuracy: 0.7673 - val_loss: 0.5080 - val_accuracy: 0.8439\n",
      "Epoch 119/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6871 - accuracy: 0.7685 - val_loss: 0.7296 - val_accuracy: 0.7756\n",
      "Epoch 120/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6709 - accuracy: 0.7697 - val_loss: 0.5698 - val_accuracy: 0.8439\n",
      "Epoch 121/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7042 - accuracy: 0.7522 - val_loss: 0.5200 - val_accuracy: 0.8293\n",
      "Epoch 122/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6723 - accuracy: 0.7656 - val_loss: 0.5157 - val_accuracy: 0.8244\n",
      "Epoch 123/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6452 - accuracy: 0.7825 - val_loss: 0.5669 - val_accuracy: 0.8195\n",
      "Epoch 124/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6607 - accuracy: 0.7690 - val_loss: 0.5126 - val_accuracy: 0.8341\n",
      "Epoch 125/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6444 - accuracy: 0.7866 - val_loss: 0.6149 - val_accuracy: 0.7951\n",
      "Epoch 126/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6852 - accuracy: 0.7720 - val_loss: 0.4743 - val_accuracy: 0.8341\n",
      "Epoch 127/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7062 - accuracy: 0.7668 - val_loss: 0.5455 - val_accuracy: 0.8390\n",
      "Epoch 128/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6220 - accuracy: 0.7854 - val_loss: 0.5132 - val_accuracy: 0.8439\n",
      "Epoch 129/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6098 - accuracy: 0.7980 - val_loss: 0.5831 - val_accuracy: 0.8049\n",
      "Epoch 130/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6695 - accuracy: 0.7714 - val_loss: 0.5324 - val_accuracy: 0.8390\n",
      "Epoch 131/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6447 - accuracy: 0.7720 - val_loss: 0.5086 - val_accuracy: 0.8293\n",
      "Epoch 132/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6112 - accuracy: 0.7924 - val_loss: 0.4766 - val_accuracy: 0.8341\n",
      "Epoch 133/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6088 - accuracy: 0.7840 - val_loss: 0.5797 - val_accuracy: 0.8000\n",
      "Epoch 134/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6384 - accuracy: 0.7802 - val_loss: 0.4498 - val_accuracy: 0.8341\n",
      "Epoch 135/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6476 - accuracy: 0.7860 - val_loss: 0.4474 - val_accuracy: 0.8439\n",
      "Epoch 136/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6360 - accuracy: 0.7854 - val_loss: 0.6465 - val_accuracy: 0.7854\n",
      "Epoch 137/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6342 - accuracy: 0.7778 - val_loss: 0.5811 - val_accuracy: 0.8195\n",
      "Epoch 138/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6033 - accuracy: 0.7983 - val_loss: 0.5461 - val_accuracy: 0.8146\n",
      "Epoch 139/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6174 - accuracy: 0.7930 - val_loss: 0.4568 - val_accuracy: 0.8537\n",
      "Epoch 140/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5964 - accuracy: 0.7959 - val_loss: 0.5764 - val_accuracy: 0.8390\n",
      "Epoch 141/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6102 - accuracy: 0.7930 - val_loss: 0.5265 - val_accuracy: 0.8341\n",
      "Epoch 142/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6207 - accuracy: 0.7854 - val_loss: 0.5383 - val_accuracy: 0.8244\n",
      "Epoch 143/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6089 - accuracy: 0.7942 - val_loss: 0.4857 - val_accuracy: 0.8439\n",
      "Epoch 144/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5975 - accuracy: 0.7901 - val_loss: 0.4632 - val_accuracy: 0.8439\n",
      "Epoch 145/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5991 - accuracy: 0.7971 - val_loss: 0.4616 - val_accuracy: 0.8439\n",
      "Epoch 146/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5775 - accuracy: 0.7991 - val_loss: 0.5449 - val_accuracy: 0.8244\n",
      "Epoch 147/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5795 - accuracy: 0.8017 - val_loss: 0.5106 - val_accuracy: 0.8293\n",
      "Epoch 148/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5724 - accuracy: 0.7988 - val_loss: 0.5553 - val_accuracy: 0.8244\n",
      "Epoch 149/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5797 - accuracy: 0.8000 - val_loss: 0.5606 - val_accuracy: 0.8146\n",
      "Epoch 150/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5888 - accuracy: 0.7895 - val_loss: 0.4858 - val_accuracy: 0.8341\n",
      "Epoch 151/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5884 - accuracy: 0.7930 - val_loss: 0.5430 - val_accuracy: 0.8244\n",
      "Epoch 152/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5795 - accuracy: 0.7924 - val_loss: 0.5005 - val_accuracy: 0.8439\n",
      "Epoch 153/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5835 - accuracy: 0.8058 - val_loss: 0.4669 - val_accuracy: 0.8537\n",
      "Epoch 154/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5604 - accuracy: 0.8093 - val_loss: 0.5022 - val_accuracy: 0.8341\n",
      "Epoch 155/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5259 - accuracy: 0.8175 - val_loss: 0.4711 - val_accuracy: 0.8439\n",
      "Epoch 156/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5273 - accuracy: 0.8204 - val_loss: 0.5836 - val_accuracy: 0.8049\n",
      "Epoch 157/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5745 - accuracy: 0.7895 - val_loss: 0.5848 - val_accuracy: 0.8098\n",
      "Epoch 158/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5705 - accuracy: 0.8023 - val_loss: 0.4608 - val_accuracy: 0.8537\n",
      "Epoch 159/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5581 - accuracy: 0.8128 - val_loss: 0.4569 - val_accuracy: 0.8634\n",
      "Epoch 160/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5188 - accuracy: 0.8245 - val_loss: 0.4762 - val_accuracy: 0.8488\n",
      "Epoch 161/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5362 - accuracy: 0.8140 - val_loss: 0.5325 - val_accuracy: 0.8341\n",
      "Epoch 162/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5446 - accuracy: 0.8082 - val_loss: 0.5009 - val_accuracy: 0.8146\n",
      "Epoch 163/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5554 - accuracy: 0.8041 - val_loss: 0.5371 - val_accuracy: 0.8390\n",
      "Epoch 164/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5386 - accuracy: 0.8122 - val_loss: 0.4548 - val_accuracy: 0.8390\n",
      "Epoch 165/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5191 - accuracy: 0.8140 - val_loss: 0.6731 - val_accuracy: 0.7756\n",
      "Epoch 166/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5208 - accuracy: 0.8210 - val_loss: 0.4133 - val_accuracy: 0.8585\n",
      "Epoch 167/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5612 - accuracy: 0.8070 - val_loss: 0.4904 - val_accuracy: 0.8341\n",
      "Epoch 168/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5270 - accuracy: 0.8157 - val_loss: 0.4645 - val_accuracy: 0.8439\n",
      "Epoch 169/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5567 - accuracy: 0.8047 - val_loss: 0.5745 - val_accuracy: 0.8049\n",
      "Epoch 170/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5239 - accuracy: 0.8251 - val_loss: 0.4666 - val_accuracy: 0.8585\n",
      "Epoch 171/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4986 - accuracy: 0.8262 - val_loss: 0.5148 - val_accuracy: 0.8098\n",
      "Epoch 172/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5161 - accuracy: 0.8087 - val_loss: 0.4259 - val_accuracy: 0.8390\n",
      "Epoch 173/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4924 - accuracy: 0.8402 - val_loss: 0.4922 - val_accuracy: 0.8390\n",
      "Epoch 174/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4953 - accuracy: 0.8344 - val_loss: 0.5280 - val_accuracy: 0.8341\n",
      "Epoch 175/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4973 - accuracy: 0.8222 - val_loss: 0.4266 - val_accuracy: 0.8585\n",
      "Epoch 176/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5192 - accuracy: 0.8093 - val_loss: 0.5707 - val_accuracy: 0.8146\n",
      "Epoch 177/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5189 - accuracy: 0.8292 - val_loss: 0.4590 - val_accuracy: 0.8488\n",
      "Epoch 178/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5273 - accuracy: 0.8210 - val_loss: 0.5633 - val_accuracy: 0.8098\n",
      "Epoch 179/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5122 - accuracy: 0.8239 - val_loss: 0.5722 - val_accuracy: 0.8341\n",
      "Epoch 180/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4778 - accuracy: 0.8348 - val_loss: 0.5026 - val_accuracy: 0.8341\n",
      "Epoch 181/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4565 - accuracy: 0.8385 - val_loss: 0.5572 - val_accuracy: 0.8000\n",
      "Epoch 182/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4880 - accuracy: 0.8303 - val_loss: 0.4852 - val_accuracy: 0.8244\n",
      "Epoch 183/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5332 - accuracy: 0.8163 - val_loss: 0.5084 - val_accuracy: 0.8244\n",
      "Epoch 184/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5070 - accuracy: 0.8280 - val_loss: 0.4590 - val_accuracy: 0.8341\n",
      "Epoch 185/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4889 - accuracy: 0.8362 - val_loss: 0.4390 - val_accuracy: 0.8439\n",
      "Epoch 186/2000\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4866 - accuracy: 0.8350 - val_loss: 0.6588 - val_accuracy: 0.7610\n",
      "Epoch 187/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4665 - accuracy: 0.8391 - val_loss: 0.5379 - val_accuracy: 0.8146\n",
      "Epoch 188/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5096 - accuracy: 0.8253 - val_loss: 0.5010 - val_accuracy: 0.8049\n",
      "Epoch 189/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4955 - accuracy: 0.8210 - val_loss: 0.4613 - val_accuracy: 0.8341\n",
      "Epoch 190/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4895 - accuracy: 0.8356 - val_loss: 0.5363 - val_accuracy: 0.8195\n",
      "Epoch 191/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4942 - accuracy: 0.8227 - val_loss: 0.4927 - val_accuracy: 0.8341\n",
      "Epoch 192/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4729 - accuracy: 0.8332 - val_loss: 0.4587 - val_accuracy: 0.8488\n",
      "Epoch 193/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5112 - accuracy: 0.8175 - val_loss: 0.4780 - val_accuracy: 0.8244\n",
      "Epoch 194/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4805 - accuracy: 0.8344 - val_loss: 0.5292 - val_accuracy: 0.8098\n",
      "Epoch 195/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4666 - accuracy: 0.8385 - val_loss: 0.4407 - val_accuracy: 0.8634\n",
      "Epoch 196/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4866 - accuracy: 0.8385 - val_loss: 0.4912 - val_accuracy: 0.8341\n",
      "Epoch 197/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4599 - accuracy: 0.8420 - val_loss: 0.4252 - val_accuracy: 0.8585\n",
      "Epoch 198/2000\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4979 - accuracy: 0.8309 - val_loss: 0.4971 - val_accuracy: 0.8146\n",
      "Epoch 199/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4833 - accuracy: 0.8356 - val_loss: 0.5474 - val_accuracy: 0.8000\n",
      "Epoch 200/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4599 - accuracy: 0.8315 - val_loss: 0.8842 - val_accuracy: 0.7122\n",
      "Epoch 201/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4850 - accuracy: 0.8280 - val_loss: 0.5354 - val_accuracy: 0.8293\n",
      "Epoch 202/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4359 - accuracy: 0.8426 - val_loss: 0.4999 - val_accuracy: 0.8341\n",
      "Epoch 203/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4662 - accuracy: 0.8431 - val_loss: 0.4768 - val_accuracy: 0.8488\n",
      "Epoch 204/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4348 - accuracy: 0.8519 - val_loss: 0.4616 - val_accuracy: 0.8585\n",
      "Epoch 205/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4679 - accuracy: 0.8391 - val_loss: 0.4999 - val_accuracy: 0.8390\n",
      "Epoch 206/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4651 - accuracy: 0.8391 - val_loss: 0.6559 - val_accuracy: 0.7854\n",
      "Epoch 207/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4862 - accuracy: 0.8420 - val_loss: 0.4862 - val_accuracy: 0.8585\n",
      "Epoch 208/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4712 - accuracy: 0.8338 - val_loss: 0.5174 - val_accuracy: 0.8341\n",
      "Epoch 209/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4075 - accuracy: 0.8525 - val_loss: 0.5091 - val_accuracy: 0.8439\n",
      "Epoch 210/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4380 - accuracy: 0.8402 - val_loss: 0.4828 - val_accuracy: 0.8488\n",
      "Epoch 211/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4445 - accuracy: 0.8414 - val_loss: 0.4719 - val_accuracy: 0.8439\n",
      "Epoch 212/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4492 - accuracy: 0.8437 - val_loss: 0.4759 - val_accuracy: 0.8439\n",
      "Epoch 213/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4491 - accuracy: 0.8385 - val_loss: 0.4942 - val_accuracy: 0.8341\n",
      "Epoch 214/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4418 - accuracy: 0.8466 - val_loss: 0.4686 - val_accuracy: 0.8439\n",
      "Epoch 215/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4197 - accuracy: 0.8560 - val_loss: 0.4097 - val_accuracy: 0.8537\n",
      "Epoch 216/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4193 - accuracy: 0.8595 - val_loss: 0.5501 - val_accuracy: 0.8146\n",
      "Epoch 217/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4563 - accuracy: 0.8385 - val_loss: 0.4357 - val_accuracy: 0.8439\n",
      "Epoch 218/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4258 - accuracy: 0.8583 - val_loss: 0.4974 - val_accuracy: 0.8439\n",
      "Epoch 219/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4054 - accuracy: 0.8595 - val_loss: 0.4737 - val_accuracy: 0.8439\n",
      "Epoch 220/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3875 - accuracy: 0.8700 - val_loss: 0.4807 - val_accuracy: 0.8439\n",
      "Epoch 221/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4351 - accuracy: 0.8536 - val_loss: 0.4578 - val_accuracy: 0.8390\n",
      "Epoch 222/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4386 - accuracy: 0.8490 - val_loss: 0.5594 - val_accuracy: 0.8000\n",
      "Epoch 223/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4303 - accuracy: 0.8577 - val_loss: 0.4683 - val_accuracy: 0.8488\n",
      "Epoch 224/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4695 - accuracy: 0.8385 - val_loss: 0.4698 - val_accuracy: 0.8585\n",
      "Epoch 225/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4003 - accuracy: 0.8566 - val_loss: 0.4212 - val_accuracy: 0.8537\n",
      "Epoch 226/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4539 - accuracy: 0.8443 - val_loss: 0.4103 - val_accuracy: 0.8537\n",
      "Epoch 227/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4502 - accuracy: 0.8431 - val_loss: 0.4726 - val_accuracy: 0.8439\n",
      "Epoch 228/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3993 - accuracy: 0.8577 - val_loss: 0.5526 - val_accuracy: 0.8293\n",
      "Epoch 229/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4472 - accuracy: 0.8449 - val_loss: 0.4715 - val_accuracy: 0.8341\n",
      "Epoch 230/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4177 - accuracy: 0.8612 - val_loss: 0.4843 - val_accuracy: 0.8537\n",
      "Epoch 231/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4523 - accuracy: 0.8484 - val_loss: 0.6254 - val_accuracy: 0.7854\n",
      "Epoch 232/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4148 - accuracy: 0.8548 - val_loss: 0.4734 - val_accuracy: 0.8683\n",
      "Epoch 233/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3949 - accuracy: 0.8606 - val_loss: 0.4445 - val_accuracy: 0.8537\n",
      "Epoch 234/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4327 - accuracy: 0.8455 - val_loss: 0.4424 - val_accuracy: 0.8537\n",
      "Epoch 235/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3702 - accuracy: 0.8706 - val_loss: 0.4028 - val_accuracy: 0.8780\n",
      "Epoch 236/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4271 - accuracy: 0.8461 - val_loss: 0.4698 - val_accuracy: 0.8585\n",
      "Epoch 237/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4457 - accuracy: 0.8449 - val_loss: 0.4951 - val_accuracy: 0.8341\n",
      "Epoch 238/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4279 - accuracy: 0.8513 - val_loss: 0.5233 - val_accuracy: 0.8390\n",
      "Epoch 239/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3914 - accuracy: 0.8606 - val_loss: 0.5864 - val_accuracy: 0.8000\n",
      "Epoch 240/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4111 - accuracy: 0.8595 - val_loss: 0.4637 - val_accuracy: 0.8341\n",
      "Epoch 241/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4141 - accuracy: 0.8571 - val_loss: 0.4969 - val_accuracy: 0.8049\n",
      "Epoch 242/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3948 - accuracy: 0.8618 - val_loss: 0.4840 - val_accuracy: 0.8293\n",
      "Epoch 243/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4015 - accuracy: 0.8618 - val_loss: 0.5050 - val_accuracy: 0.8390\n",
      "Epoch 244/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3884 - accuracy: 0.8671 - val_loss: 0.4802 - val_accuracy: 0.8244\n",
      "Epoch 245/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4237 - accuracy: 0.8490 - val_loss: 0.4139 - val_accuracy: 0.8634\n",
      "Epoch 246/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4124 - accuracy: 0.8577 - val_loss: 0.4735 - val_accuracy: 0.8537\n",
      "Epoch 247/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3648 - accuracy: 0.8741 - val_loss: 0.5269 - val_accuracy: 0.8439\n",
      "Epoch 248/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4036 - accuracy: 0.8641 - val_loss: 0.5246 - val_accuracy: 0.8244\n",
      "Epoch 249/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3845 - accuracy: 0.8676 - val_loss: 0.5113 - val_accuracy: 0.8537\n",
      "Epoch 250/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3748 - accuracy: 0.8671 - val_loss: 0.4668 - val_accuracy: 0.8439\n",
      "Epoch 251/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3977 - accuracy: 0.8630 - val_loss: 0.4549 - val_accuracy: 0.8390\n",
      "Epoch 252/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3800 - accuracy: 0.8618 - val_loss: 0.4478 - val_accuracy: 0.8634\n",
      "Epoch 253/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3861 - accuracy: 0.8723 - val_loss: 0.4760 - val_accuracy: 0.8439\n",
      "Epoch 254/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3244 - accuracy: 0.8840 - val_loss: 0.4588 - val_accuracy: 0.8390\n",
      "Epoch 255/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3391 - accuracy: 0.8787 - val_loss: 0.4246 - val_accuracy: 0.8488\n",
      "Epoch 256/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3668 - accuracy: 0.8700 - val_loss: 0.4460 - val_accuracy: 0.8927\n",
      "Epoch 257/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3952 - accuracy: 0.8694 - val_loss: 0.6944 - val_accuracy: 0.7854\n",
      "Epoch 258/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3577 - accuracy: 0.8688 - val_loss: 0.5049 - val_accuracy: 0.8488\n",
      "Epoch 259/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3650 - accuracy: 0.8659 - val_loss: 0.4610 - val_accuracy: 0.8537\n",
      "Epoch 260/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3721 - accuracy: 0.8653 - val_loss: 0.4645 - val_accuracy: 0.8537\n",
      "Epoch 261/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3884 - accuracy: 0.8688 - val_loss: 0.4370 - val_accuracy: 0.8585\n",
      "Epoch 262/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3664 - accuracy: 0.8653 - val_loss: 0.4505 - val_accuracy: 0.8390\n",
      "Epoch 263/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3612 - accuracy: 0.8776 - val_loss: 0.4442 - val_accuracy: 0.8488\n",
      "Epoch 264/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3938 - accuracy: 0.8531 - val_loss: 0.6142 - val_accuracy: 0.7951\n",
      "Epoch 265/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3433 - accuracy: 0.8892 - val_loss: 0.4245 - val_accuracy: 0.8585\n",
      "Epoch 266/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3677 - accuracy: 0.8799 - val_loss: 0.4642 - val_accuracy: 0.8439\n",
      "Epoch 267/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3640 - accuracy: 0.8723 - val_loss: 0.4886 - val_accuracy: 0.8390\n",
      "Epoch 268/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3773 - accuracy: 0.8571 - val_loss: 0.4370 - val_accuracy: 0.8634\n",
      "Epoch 269/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3445 - accuracy: 0.8840 - val_loss: 0.5640 - val_accuracy: 0.8244\n",
      "Epoch 270/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3531 - accuracy: 0.8787 - val_loss: 0.4183 - val_accuracy: 0.8683\n",
      "Epoch 271/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3658 - accuracy: 0.8776 - val_loss: 0.4219 - val_accuracy: 0.8732\n",
      "Epoch 272/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3836 - accuracy: 0.8653 - val_loss: 0.4384 - val_accuracy: 0.8829\n",
      "Epoch 273/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3446 - accuracy: 0.8776 - val_loss: 0.5174 - val_accuracy: 0.8341\n",
      "Epoch 274/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3310 - accuracy: 0.8805 - val_loss: 0.4334 - val_accuracy: 0.8585\n",
      "Epoch 275/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3699 - accuracy: 0.8746 - val_loss: 0.4998 - val_accuracy: 0.8390\n",
      "Epoch 276/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3658 - accuracy: 0.8752 - val_loss: 0.4266 - val_accuracy: 0.8537\n",
      "Epoch 277/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3662 - accuracy: 0.8787 - val_loss: 0.4245 - val_accuracy: 0.8585\n",
      "Epoch 278/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3684 - accuracy: 0.8612 - val_loss: 0.3909 - val_accuracy: 0.8537\n",
      "Epoch 279/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3367 - accuracy: 0.8880 - val_loss: 0.3999 - val_accuracy: 0.8732\n",
      "Epoch 280/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3486 - accuracy: 0.8781 - val_loss: 0.5016 - val_accuracy: 0.8634\n",
      "Epoch 281/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3584 - accuracy: 0.8741 - val_loss: 0.4730 - val_accuracy: 0.8341\n",
      "Epoch 282/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3717 - accuracy: 0.8706 - val_loss: 0.4656 - val_accuracy: 0.8537\n",
      "Epoch 283/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3369 - accuracy: 0.8857 - val_loss: 0.4944 - val_accuracy: 0.8390\n",
      "Epoch 284/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3566 - accuracy: 0.8834 - val_loss: 0.4633 - val_accuracy: 0.8341\n",
      "Epoch 285/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3509 - accuracy: 0.8741 - val_loss: 0.4328 - val_accuracy: 0.8732\n",
      "Epoch 286/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3703 - accuracy: 0.8758 - val_loss: 0.4814 - val_accuracy: 0.8488\n",
      "Epoch 287/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3477 - accuracy: 0.8711 - val_loss: 0.4584 - val_accuracy: 0.8537\n",
      "Epoch 288/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3534 - accuracy: 0.8776 - val_loss: 0.4404 - val_accuracy: 0.8585\n",
      "Epoch 289/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3427 - accuracy: 0.8752 - val_loss: 0.4622 - val_accuracy: 0.8634\n",
      "Epoch 290/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3463 - accuracy: 0.8845 - val_loss: 0.4076 - val_accuracy: 0.8732\n",
      "Epoch 291/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3545 - accuracy: 0.8746 - val_loss: 0.4636 - val_accuracy: 0.8634\n",
      "Epoch 292/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3343 - accuracy: 0.8787 - val_loss: 0.4704 - val_accuracy: 0.8537\n",
      "Epoch 293/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3428 - accuracy: 0.8857 - val_loss: 0.4825 - val_accuracy: 0.8537\n",
      "Epoch 294/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3173 - accuracy: 0.8892 - val_loss: 0.5250 - val_accuracy: 0.8146\n",
      "Epoch 295/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3458 - accuracy: 0.8776 - val_loss: 0.4693 - val_accuracy: 0.8634\n",
      "Epoch 296/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3441 - accuracy: 0.8863 - val_loss: 0.4291 - val_accuracy: 0.8537\n",
      "Epoch 297/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3344 - accuracy: 0.8799 - val_loss: 0.4641 - val_accuracy: 0.8634\n",
      "Epoch 298/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3081 - accuracy: 0.8886 - val_loss: 0.4838 - val_accuracy: 0.8439\n",
      "Epoch 299/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3181 - accuracy: 0.8834 - val_loss: 0.4557 - val_accuracy: 0.8683\n",
      "Epoch 300/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3319 - accuracy: 0.8857 - val_loss: 0.4408 - val_accuracy: 0.8439\n",
      "Epoch 301/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3170 - accuracy: 0.8962 - val_loss: 0.4753 - val_accuracy: 0.8634\n",
      "Epoch 302/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3252 - accuracy: 0.8856 - val_loss: 0.5378 - val_accuracy: 0.8244\n",
      "Epoch 303/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3563 - accuracy: 0.8793 - val_loss: 0.4821 - val_accuracy: 0.8439\n",
      "Epoch 304/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3116 - accuracy: 0.8892 - val_loss: 0.4735 - val_accuracy: 0.8341\n",
      "Epoch 305/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3247 - accuracy: 0.8828 - val_loss: 0.4681 - val_accuracy: 0.8585\n",
      "Epoch 306/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3200 - accuracy: 0.8880 - val_loss: 0.4004 - val_accuracy: 0.8780\n",
      "Epoch 307/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3278 - accuracy: 0.8851 - val_loss: 0.5283 - val_accuracy: 0.8293\n",
      "Epoch 308/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3395 - accuracy: 0.8810 - val_loss: 0.6286 - val_accuracy: 0.7951\n",
      "Epoch 309/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3237 - accuracy: 0.8845 - val_loss: 0.4253 - val_accuracy: 0.8634\n",
      "Epoch 310/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2876 - accuracy: 0.8974 - val_loss: 0.4571 - val_accuracy: 0.8585\n",
      "Epoch 311/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3081 - accuracy: 0.8991 - val_loss: 0.3861 - val_accuracy: 0.8683\n",
      "Epoch 312/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3072 - accuracy: 0.8939 - val_loss: 0.4515 - val_accuracy: 0.8585\n",
      "Epoch 313/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3198 - accuracy: 0.8776 - val_loss: 0.4226 - val_accuracy: 0.8780\n",
      "Epoch 314/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3339 - accuracy: 0.8851 - val_loss: 0.4510 - val_accuracy: 0.8732\n",
      "Epoch 315/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3311 - accuracy: 0.8898 - val_loss: 0.4836 - val_accuracy: 0.8439\n",
      "Epoch 316/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2959 - accuracy: 0.9038 - val_loss: 0.4751 - val_accuracy: 0.8488\n",
      "Epoch 317/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3197 - accuracy: 0.8875 - val_loss: 0.4482 - val_accuracy: 0.8780\n",
      "Epoch 318/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3463 - accuracy: 0.8828 - val_loss: 0.3753 - val_accuracy: 0.8927\n",
      "Epoch 319/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2922 - accuracy: 0.8915 - val_loss: 0.4005 - val_accuracy: 0.8829\n",
      "Epoch 320/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3394 - accuracy: 0.8869 - val_loss: 0.4223 - val_accuracy: 0.8585\n",
      "Epoch 321/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3163 - accuracy: 0.8840 - val_loss: 0.4068 - val_accuracy: 0.8927\n",
      "Epoch 322/2000\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3275 - accuracy: 0.8840 - val_loss: 0.4882 - val_accuracy: 0.8488\n",
      "Epoch 323/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3240 - accuracy: 0.8910 - val_loss: 0.4145 - val_accuracy: 0.8634\n",
      "Epoch 324/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3192 - accuracy: 0.8869 - val_loss: 0.4395 - val_accuracy: 0.8732\n",
      "Epoch 325/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3137 - accuracy: 0.8950 - val_loss: 0.4489 - val_accuracy: 0.8537\n",
      "Epoch 326/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3064 - accuracy: 0.8956 - val_loss: 0.4454 - val_accuracy: 0.8780\n",
      "Epoch 327/2000\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3198 - accuracy: 0.8851 - val_loss: 0.4462 - val_accuracy: 0.8780\n",
      "Epoch 328/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3080 - accuracy: 0.8974 - val_loss: 0.4813 - val_accuracy: 0.8439\n",
      "Epoch 329/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3512 - accuracy: 0.8845 - val_loss: 0.4911 - val_accuracy: 0.8341\n",
      "Epoch 330/2000\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2937 - accuracy: 0.8997 - val_loss: 0.4621 - val_accuracy: 0.8439\n",
      "Epoch 331/2000\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2783 - accuracy: 0.9026 - val_loss: 0.4660 - val_accuracy: 0.8634\n",
      "Epoch 00331: early stopping\n",
      "CNN: Epochs=2000, Train accuracy=0.90379, Validation accuracy=0.89268\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1598336814187,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "mDdwZDDFkv5W",
    "outputId": "30b0e4a2-9738-4989-eb06-1c54ecef6b41"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#맥스풀링 마지막에만 하고 FC두번 거쳐서 하니 느리지만 꾸준히 정확도 증가 80에서 거의 멈춤 - 해결 방안?\n",
    "#epoch 180정도로 한 결과 = model_9\n",
    "#model_9는 0.89166, 0.87805\n",
    "#model_9-2를 eopch 230, relu 대신 leaky relu\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQCvPGZks9v"
   },
   "outputs": [],
   "source": [
    "#Parameter 저장\n",
    "model.save_weights(f'data/cvision/params_9-5.h5')\n",
    "\n",
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'data/cvision/model_9-5.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8nb5jokyny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-dbea146f3fce>:6: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "#예측 진행\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255.\n",
    "\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('data/cvision/my_subm_9-5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_9-6: \n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), Adam(0.001)\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1598347122067,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "ySu34IFkkhNe",
    "outputId": "e3d46b6e-e956-4a26-f1bc-aa4c0e4c3b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_644 (Conv2D)          (None, 28, 28, 24)        240       \n",
      "_________________________________________________________________\n",
      "conv2d_645 (Conv2D)          (None, 28, 28, 24)        5208      \n",
      "_________________________________________________________________\n",
      "conv2d_646 (Conv2D)          (None, 28, 28, 24)        5208      \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 28, 28, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_647 (Conv2D)          (None, 28, 28, 32)        6944      \n",
      "_________________________________________________________________\n",
      "conv2d_648 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_649 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_650 (Conv2D)          (None, 14, 14, 48)        13872     \n",
      "_________________________________________________________________\n",
      "conv2d_651 (Conv2D)          (None, 7, 7, 48)          20784     \n",
      "_________________________________________________________________\n",
      "conv2d_652 (Conv2D)          (None, 4, 4, 48)          20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 50)                9650      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 101,696\n",
      "Trainable params: 101,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1598347122783,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "tPFielyJ5yMH"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "lrs = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** x)\n",
    "\n",
    "#earlystopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=150)\n",
    "\n",
    "#modelcheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "model_num = 'model_9-6'\n",
    "MODEL_SAVE_FOLDER_PATH = './data/cvision/' + model_num + '/'\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + model_num +'ep{epoch:02d}-vl{val_loss:.4f}.hdf5'\n",
    "\n",
    "cp = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537462,
     "status": "error",
     "timestamp": 1598347660504,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "yeNCbX2vko4s",
    "outputId": "012ce4a9-2ca8-41d3-d1de-9331032469e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 2.9636 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0170s). Check your callbacks.\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.3801 - accuracy: 0.0901\n",
      "Epoch 00001: val_loss improved from inf to 2.30458, saving model to ./data/cvision/model\\model_9-6ep01-vl2.3046.hdf5\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.3748 - accuracy: 0.0898 - val_loss: 2.3046 - val_accuracy: 0.0634\n",
      "Epoch 2/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2995 - accuracy: 0.1131\n",
      "Epoch 00002: val_loss improved from 2.30458 to 2.30275, saving model to ./data/cvision/model\\model_9-6ep02-vl2.3028.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2995 - accuracy: 0.1131 - val_loss: 2.3028 - val_accuracy: 0.0829\n",
      "Epoch 3/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2969 - accuracy: 0.1285\n",
      "Epoch 00003: val_loss improved from 2.30275 to 2.29880, saving model to ./data/cvision/model\\model_9-6ep03-vl2.2988.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2956 - accuracy: 0.1306 - val_loss: 2.2988 - val_accuracy: 0.0829\n",
      "Epoch 4/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2925 - accuracy: 0.1254\n",
      "Epoch 00004: val_loss improved from 2.29880 to 2.29391, saving model to ./data/cvision/model\\model_9-6ep04-vl2.2939.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2916 - accuracy: 0.1277 - val_loss: 2.2939 - val_accuracy: 0.0878\n",
      "Epoch 5/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2916 - accuracy: 0.1348\n",
      "Epoch 00005: val_loss improved from 2.29391 to 2.28430, saving model to ./data/cvision/model\\model_9-6ep05-vl2.2843.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2921 - accuracy: 0.1318 - val_loss: 2.2843 - val_accuracy: 0.1463\n",
      "Epoch 6/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2762 - accuracy: 0.1329\n",
      "Epoch 00006: val_loss improved from 2.28430 to 2.24984, saving model to ./data/cvision/model\\model_9-6ep06-vl2.2498.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2762 - accuracy: 0.1329 - val_loss: 2.2498 - val_accuracy: 0.1659\n",
      "Epoch 7/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2680 - accuracy: 0.1312\n",
      "Epoch 00007: val_loss improved from 2.24984 to 2.22940, saving model to ./data/cvision/model\\model_9-6ep07-vl2.2294.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2680 - accuracy: 0.1312 - val_loss: 2.2294 - val_accuracy: 0.1659\n",
      "Epoch 8/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2175 - accuracy: 0.1720\n",
      "Epoch 00008: val_loss improved from 2.22940 to 2.19005, saving model to ./data/cvision/model\\model_9-6ep08-vl2.1900.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2147 - accuracy: 0.1749 - val_loss: 2.1900 - val_accuracy: 0.1707\n",
      "Epoch 9/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2024 - accuracy: 0.1872\n",
      "Epoch 00009: val_loss improved from 2.19005 to 2.09029, saving model to ./data/cvision/model\\model_9-6ep09-vl2.0903.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2024 - accuracy: 0.1872 - val_loss: 2.0903 - val_accuracy: 0.2634\n",
      "Epoch 10/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.1505 - accuracy: 0.2134\n",
      "Epoch 00010: val_loss improved from 2.09029 to 1.99955, saving model to ./data/cvision/model\\model_9-6ep10-vl1.9995.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.1505 - accuracy: 0.2134 - val_loss: 1.9995 - val_accuracy: 0.2829\n",
      "Epoch 11/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0941 - accuracy: 0.2224\n",
      "Epoch 00011: val_loss improved from 1.99955 to 1.96215, saving model to ./data/cvision/model\\model_9-6ep11-vl1.9621.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.0983 - accuracy: 0.2192 - val_loss: 1.9621 - val_accuracy: 0.3756\n",
      "Epoch 12/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0782 - accuracy: 0.2552\n",
      "Epoch 00012: val_loss improved from 1.96215 to 1.83818, saving model to ./data/cvision/model\\model_9-6ep12-vl1.8382.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.0730 - accuracy: 0.2554 - val_loss: 1.8382 - val_accuracy: 0.3366\n",
      "Epoch 13/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0463 - accuracy: 0.2554\n",
      "Epoch 00013: val_loss did not improve from 1.83818\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.0463 - accuracy: 0.2554 - val_loss: 1.9674 - val_accuracy: 0.3707\n",
      "Epoch 14/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0006 - accuracy: 0.2700\n",
      "Epoch 00014: val_loss improved from 1.83818 to 1.76315, saving model to ./data/cvision/model\\model_9-6ep14-vl1.7631.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.0006 - accuracy: 0.2700 - val_loss: 1.7631 - val_accuracy: 0.4585\n",
      "Epoch 15/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.9748 - accuracy: 0.3037\n",
      "Epoch 00015: val_loss did not improve from 1.76315\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.9680 - accuracy: 0.3090 - val_loss: 1.7852 - val_accuracy: 0.4585\n",
      "Epoch 16/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9281 - accuracy: 0.3172\n",
      "Epoch 00016: val_loss improved from 1.76315 to 1.60469, saving model to ./data/cvision/model\\model_9-6ep16-vl1.6047.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.9281 - accuracy: 0.3172 - val_loss: 1.6047 - val_accuracy: 0.5171\n",
      "Epoch 17/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9102 - accuracy: 0.3353\n",
      "Epoch 00017: val_loss did not improve from 1.60469\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.9102 - accuracy: 0.3353 - val_loss: 1.6786 - val_accuracy: 0.4976\n",
      "Epoch 18/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.8177 - accuracy: 0.3606\n",
      "Epoch 00018: val_loss improved from 1.60469 to 1.54540, saving model to ./data/cvision/model\\model_9-6ep18-vl1.5454.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.8214 - accuracy: 0.3622 - val_loss: 1.5454 - val_accuracy: 0.5024\n",
      "Epoch 19/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7811 - accuracy: 0.3697\n",
      "Epoch 00019: val_loss did not improve from 1.54540\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.7811 - accuracy: 0.3697 - val_loss: 1.5505 - val_accuracy: 0.4829\n",
      "Epoch 20/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7695 - accuracy: 0.3749\n",
      "Epoch 00020: val_loss improved from 1.54540 to 1.46456, saving model to ./data/cvision/model\\model_9-6ep20-vl1.4646.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.7695 - accuracy: 0.3749 - val_loss: 1.4646 - val_accuracy: 0.5756\n",
      "Epoch 21/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7191 - accuracy: 0.3808\n",
      "Epoch 00021: val_loss improved from 1.46456 to 1.39482, saving model to ./data/cvision/model\\model_9-6ep21-vl1.3948.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.7191 - accuracy: 0.3808 - val_loss: 1.3948 - val_accuracy: 0.5902\n",
      "Epoch 22/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6862 - accuracy: 0.4192\n",
      "Epoch 00022: val_loss improved from 1.39482 to 1.38550, saving model to ./data/cvision/model\\model_9-6ep22-vl1.3855.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.6862 - accuracy: 0.4192 - val_loss: 1.3855 - val_accuracy: 0.5951\n",
      "Epoch 23/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6571 - accuracy: 0.4134\n",
      "Epoch 00023: val_loss improved from 1.38550 to 1.31887, saving model to ./data/cvision/model\\model_9-6ep23-vl1.3189.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.6571 - accuracy: 0.4134 - val_loss: 1.3189 - val_accuracy: 0.6293\n",
      "Epoch 24/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6358 - accuracy: 0.4362\n",
      "Epoch 00024: val_loss did not improve from 1.31887\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.6358 - accuracy: 0.4362 - val_loss: 1.3429 - val_accuracy: 0.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6369 - accuracy: 0.4321\n",
      "Epoch 00025: val_loss improved from 1.31887 to 1.22061, saving model to ./data/cvision/model\\model_9-6ep25-vl1.2206.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.6369 - accuracy: 0.4321 - val_loss: 1.2206 - val_accuracy: 0.6390\n",
      "Epoch 26/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.5385 - accuracy: 0.4531\n",
      "Epoch 00026: val_loss improved from 1.22061 to 1.12461, saving model to ./data/cvision/model\\model_9-6ep26-vl1.1246.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.5385 - accuracy: 0.4554 - val_loss: 1.1246 - val_accuracy: 0.6780\n",
      "Epoch 27/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.4671\n",
      "Epoch 00027: val_loss improved from 1.12461 to 1.10593, saving model to ./data/cvision/model\\model_9-6ep27-vl1.1059.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.5321 - accuracy: 0.4671 - val_loss: 1.1059 - val_accuracy: 0.6878\n",
      "Epoch 28/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5176 - accuracy: 0.4618\n",
      "Epoch 00028: val_loss did not improve from 1.10593\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.5176 - accuracy: 0.4618 - val_loss: 1.1271 - val_accuracy: 0.6683\n",
      "Epoch 29/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4736 - accuracy: 0.4845\n",
      "Epoch 00029: val_loss improved from 1.10593 to 1.09061, saving model to ./data/cvision/model\\model_9-6ep29-vl1.0906.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4736 - accuracy: 0.4845 - val_loss: 1.0906 - val_accuracy: 0.6976\n",
      "Epoch 30/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4645 - accuracy: 0.4840\n",
      "Epoch 00030: val_loss improved from 1.09061 to 1.05029, saving model to ./data/cvision/model\\model_9-6ep30-vl1.0503.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4645 - accuracy: 0.4840 - val_loss: 1.0503 - val_accuracy: 0.6634\n",
      "Epoch 31/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4053 - accuracy: 0.4962\n",
      "Epoch 00031: val_loss improved from 1.05029 to 0.97869, saving model to ./data/cvision/model\\model_9-6ep31-vl0.9787.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4053 - accuracy: 0.4962 - val_loss: 0.9787 - val_accuracy: 0.7220\n",
      "Epoch 32/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3583 - accuracy: 0.5294\n",
      "Epoch 00032: val_loss did not improve from 0.97869\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3583 - accuracy: 0.5294 - val_loss: 0.9830 - val_accuracy: 0.7024\n",
      "Epoch 33/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3883 - accuracy: 0.5195\n",
      "Epoch 00033: val_loss did not improve from 0.97869\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3883 - accuracy: 0.5195 - val_loss: 1.1238 - val_accuracy: 0.7024\n",
      "Epoch 34/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.4202 - accuracy: 0.4922\n",
      "Epoch 00034: val_loss did not improve from 0.97869\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.4286 - accuracy: 0.4915 - val_loss: 0.9964 - val_accuracy: 0.6976\n",
      "Epoch 35/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3325 - accuracy: 0.5318\n",
      "Epoch 00035: val_loss improved from 0.97869 to 0.89970, saving model to ./data/cvision/model\\model_9-6ep35-vl0.8997.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.3325 - accuracy: 0.5318 - val_loss: 0.8997 - val_accuracy: 0.7366\n",
      "Epoch 36/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2861 - accuracy: 0.5522\n",
      "Epoch 00036: val_loss improved from 0.89970 to 0.84696, saving model to ./data/cvision/model\\model_9-6ep36-vl0.8470.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2861 - accuracy: 0.5522 - val_loss: 0.8470 - val_accuracy: 0.7659\n",
      "Epoch 37/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2926 - accuracy: 0.5475\n",
      "Epoch 00037: val_loss did not improve from 0.84696\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2926 - accuracy: 0.5475 - val_loss: 0.8553 - val_accuracy: 0.7415\n",
      "Epoch 38/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2589 - accuracy: 0.5703\n",
      "Epoch 00038: val_loss did not improve from 0.84696\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2550 - accuracy: 0.5697 - val_loss: 0.9305 - val_accuracy: 0.7220\n",
      "Epoch 39/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2317 - accuracy: 0.5791\n",
      "Epoch 00039: val_loss improved from 0.84696 to 0.82108, saving model to ./data/cvision/model\\model_9-6ep39-vl0.8211.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2175 - accuracy: 0.5802 - val_loss: 0.8211 - val_accuracy: 0.7561\n",
      "Epoch 40/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2355 - accuracy: 0.5854\n",
      "Epoch 00040: val_loss improved from 0.82108 to 0.81490, saving model to ./data/cvision/model\\model_9-6ep40-vl0.8149.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2355 - accuracy: 0.5854 - val_loss: 0.8149 - val_accuracy: 0.7561\n",
      "Epoch 41/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2135 - accuracy: 0.5816\n",
      "Epoch 00041: val_loss did not improve from 0.81490\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2167 - accuracy: 0.5767 - val_loss: 0.8322 - val_accuracy: 0.7659\n",
      "Epoch 42/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2178 - accuracy: 0.5749\n",
      "Epoch 00042: val_loss did not improve from 0.81490\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2178 - accuracy: 0.5749 - val_loss: 0.8217 - val_accuracy: 0.7707\n",
      "Epoch 43/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1493 - accuracy: 0.6047\n",
      "Epoch 00043: val_loss improved from 0.81490 to 0.74599, saving model to ./data/cvision/model\\model_9-6ep43-vl0.7460.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1493 - accuracy: 0.6047 - val_loss: 0.7460 - val_accuracy: 0.8146\n",
      "Epoch 44/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1529 - accuracy: 0.6134\n",
      "Epoch 00044: val_loss did not improve from 0.74599\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1529 - accuracy: 0.6134 - val_loss: 0.7880 - val_accuracy: 0.7610\n",
      "Epoch 45/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1325 - accuracy: 0.6046\n",
      "Epoch 00045: val_loss did not improve from 0.74599\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.1386 - accuracy: 0.6035 - val_loss: 0.7773 - val_accuracy: 0.7805\n",
      "Epoch 46/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.6204\n",
      "Epoch 00046: val_loss did not improve from 0.74599\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1247 - accuracy: 0.6204 - val_loss: 0.7512 - val_accuracy: 0.7707\n",
      "Epoch 47/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1380 - accuracy: 0.6068\n",
      "Epoch 00047: val_loss improved from 0.74599 to 0.73529, saving model to ./data/cvision/model\\model_9-6ep47-vl0.7353.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1291 - accuracy: 0.6087 - val_loss: 0.7353 - val_accuracy: 0.7707\n",
      "Epoch 48/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0942 - accuracy: 0.6232\n",
      "Epoch 00048: val_loss did not improve from 0.73529\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.0993 - accuracy: 0.6233 - val_loss: 0.7470 - val_accuracy: 0.7854\n",
      "Epoch 49/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0870 - accuracy: 0.6216\n",
      "Epoch 00049: val_loss did not improve from 0.73529\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0870 - accuracy: 0.6216 - val_loss: 0.7671 - val_accuracy: 0.7561\n",
      "Epoch 50/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1008 - accuracy: 0.6345\n",
      "Epoch 00050: val_loss improved from 0.73529 to 0.71781, saving model to ./data/cvision/model\\model_9-6ep50-vl0.7178.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0849 - accuracy: 0.6391 - val_loss: 0.7178 - val_accuracy: 0.7756\n",
      "Epoch 51/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0781 - accuracy: 0.6144\n",
      "Epoch 00051: val_loss improved from 0.71781 to 0.69134, saving model to ./data/cvision/model\\model_9-6ep51-vl0.6913.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0766 - accuracy: 0.6163 - val_loss: 0.6913 - val_accuracy: 0.8000\n",
      "Epoch 52/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.6449\n",
      "Epoch 00052: val_loss did not improve from 0.69134\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0366 - accuracy: 0.6449 - val_loss: 0.6969 - val_accuracy: 0.7854\n",
      "Epoch 53/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9925 - accuracy: 0.6520\n",
      "Epoch 00053: val_loss did not improve from 0.69134\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9977 - accuracy: 0.6523 - val_loss: 0.7034 - val_accuracy: 0.7951\n",
      "Epoch 54/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9985 - accuracy: 0.6496\n",
      "Epoch 00054: val_loss did not improve from 0.69134\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9985 - accuracy: 0.6496 - val_loss: 0.7368 - val_accuracy: 0.7805\n",
      "Epoch 55/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0237 - accuracy: 0.6352\n",
      "Epoch 00055: val_loss improved from 0.69134 to 0.66234, saving model to ./data/cvision/model\\model_9-6ep55-vl0.6623.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0132 - accuracy: 0.6408 - val_loss: 0.6623 - val_accuracy: 0.7902\n",
      "Epoch 56/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0024 - accuracy: 0.6478\n",
      "Epoch 00056: val_loss did not improve from 0.66234\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0024 - accuracy: 0.6478 - val_loss: 0.6861 - val_accuracy: 0.7902\n",
      "Epoch 57/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9562 - accuracy: 0.6774\n",
      "Epoch 00057: val_loss did not improve from 0.66234\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9565 - accuracy: 0.6776 - val_loss: 0.6811 - val_accuracy: 0.7951\n",
      "Epoch 58/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9450 - accuracy: 0.6755\n",
      "Epoch 00058: val_loss improved from 0.66234 to 0.66199, saving model to ./data/cvision/model\\model_9-6ep58-vl0.6620.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9533 - accuracy: 0.6735 - val_loss: 0.6620 - val_accuracy: 0.7854\n",
      "Epoch 59/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9294 - accuracy: 0.6711\n",
      "Epoch 00059: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9294 - accuracy: 0.6711 - val_loss: 0.7295 - val_accuracy: 0.7610\n",
      "Epoch 60/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9649 - accuracy: 0.6534\n",
      "Epoch 00060: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9634 - accuracy: 0.6554 - val_loss: 0.6981 - val_accuracy: 0.7707\n",
      "Epoch 61/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9560 - accuracy: 0.6793\n",
      "Epoch 00061: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9560 - accuracy: 0.6793 - val_loss: 0.7086 - val_accuracy: 0.7610\n",
      "Epoch 62/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9334 - accuracy: 0.6840\n",
      "Epoch 00062: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9334 - accuracy: 0.6840 - val_loss: 0.6979 - val_accuracy: 0.7610\n",
      "Epoch 63/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9393 - accuracy: 0.6717\n",
      "Epoch 00063: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9354 - accuracy: 0.6746 - val_loss: 0.7537 - val_accuracy: 0.7463\n",
      "Epoch 64/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9300 - accuracy: 0.6803\n",
      "Epoch 00064: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9267 - accuracy: 0.6819 - val_loss: 0.6739 - val_accuracy: 0.7707\n",
      "Epoch 65/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8754 - accuracy: 0.6957\n",
      "Epoch 00065: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8753 - accuracy: 0.6950 - val_loss: 0.6787 - val_accuracy: 0.7512\n",
      "Epoch 66/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.6805\n",
      "Epoch 00066: val_loss did not improve from 0.66199\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9113 - accuracy: 0.6805 - val_loss: 0.6655 - val_accuracy: 0.7805\n",
      "Epoch 67/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9067 - accuracy: 0.6985\n",
      "Epoch 00067: val_loss improved from 0.66199 to 0.64505, saving model to ./data/cvision/model\\model_9-6ep67-vl0.6451.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9067 - accuracy: 0.6985 - val_loss: 0.6451 - val_accuracy: 0.8000\n",
      "Epoch 68/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8784 - accuracy: 0.7007\n",
      "Epoch 00068: val_loss improved from 0.64505 to 0.61615, saving model to ./data/cvision/model\\model_9-6ep68-vl0.6162.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8819 - accuracy: 0.6980 - val_loss: 0.6162 - val_accuracy: 0.7854\n",
      "Epoch 69/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.6991\n",
      "Epoch 00069: val_loss did not improve from 0.61615\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8779 - accuracy: 0.6991 - val_loss: 0.6267 - val_accuracy: 0.7902\n",
      "Epoch 70/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8690 - accuracy: 0.7102\n",
      "Epoch 00070: val_loss did not improve from 0.61615\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8690 - accuracy: 0.7102 - val_loss: 0.6330 - val_accuracy: 0.7756\n",
      "Epoch 71/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8490 - accuracy: 0.7149\n",
      "Epoch 00071: val_loss did not improve from 0.61615\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8490 - accuracy: 0.7149 - val_loss: 0.6778 - val_accuracy: 0.7512\n",
      "Epoch 72/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8606 - accuracy: 0.7073\n",
      "Epoch 00072: val_loss did not improve from 0.61615\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8606 - accuracy: 0.7073 - val_loss: 0.6803 - val_accuracy: 0.7805\n",
      "Epoch 73/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.7067\n",
      "Epoch 00073: val_loss did not improve from 0.61615\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8303 - accuracy: 0.7067 - val_loss: 0.6523 - val_accuracy: 0.7951\n",
      "Epoch 74/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8369 - accuracy: 0.7190\n",
      "Epoch 00074: val_loss improved from 0.61615 to 0.58622, saving model to ./data/cvision/model\\model_9-6ep74-vl0.5862.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8369 - accuracy: 0.7190 - val_loss: 0.5862 - val_accuracy: 0.8049\n",
      "Epoch 75/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8443 - accuracy: 0.7096\n",
      "Epoch 00075: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8443 - accuracy: 0.7096 - val_loss: 0.6358 - val_accuracy: 0.8000\n",
      "Epoch 76/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7802 - accuracy: 0.7386\n",
      "Epoch 00076: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7940 - accuracy: 0.7360 - val_loss: 0.6051 - val_accuracy: 0.8098\n",
      "Epoch 77/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7895 - accuracy: 0.7242\n",
      "Epoch 00077: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7895 - accuracy: 0.7242 - val_loss: 0.6477 - val_accuracy: 0.7805\n",
      "Epoch 78/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7706 - accuracy: 0.7137\n",
      "Epoch 00078: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7706 - accuracy: 0.7137 - val_loss: 0.6345 - val_accuracy: 0.7854\n",
      "Epoch 79/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.7073\n",
      "Epoch 00079: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8223 - accuracy: 0.7073 - val_loss: 0.6117 - val_accuracy: 0.7805\n",
      "Epoch 80/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7822 - accuracy: 0.7324\n",
      "Epoch 00080: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7822 - accuracy: 0.7324 - val_loss: 0.6231 - val_accuracy: 0.8146\n",
      "Epoch 81/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.7242\n",
      "Epoch 00081: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8074 - accuracy: 0.7242 - val_loss: 0.6594 - val_accuracy: 0.7707\n",
      "Epoch 82/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7851 - accuracy: 0.7364\n",
      "Epoch 00082: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7851 - accuracy: 0.7364 - val_loss: 0.6686 - val_accuracy: 0.7805\n",
      "Epoch 83/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7766 - accuracy: 0.7297\n",
      "Epoch 00083: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7648 - accuracy: 0.7347 - val_loss: 0.6094 - val_accuracy: 0.7756\n",
      "Epoch 84/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7789 - accuracy: 0.7388\n",
      "Epoch 00084: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7789 - accuracy: 0.7388 - val_loss: 0.6280 - val_accuracy: 0.7805\n",
      "Epoch 85/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7257 - accuracy: 0.7476 ETA: 0s - loss: 0.7349 - accuracy\n",
      "Epoch 00085: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7277 - accuracy: 0.7452 - val_loss: 0.6574 - val_accuracy: 0.7951\n",
      "Epoch 86/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7123 - accuracy: 0.7662\n",
      "Epoch 00086: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7149 - accuracy: 0.7644 - val_loss: 0.6872 - val_accuracy: 0.7756\n",
      "Epoch 87/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7431 - accuracy: 0.7464\n",
      "Epoch 00087: val_loss did not improve from 0.58622\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7431 - accuracy: 0.7464 - val_loss: 0.6064 - val_accuracy: 0.7951\n",
      "Epoch 88/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7222 - accuracy: 0.7627\n",
      "Epoch 00088: val_loss improved from 0.58622 to 0.54979, saving model to ./data/cvision/model\\model_9-6ep88-vl0.5498.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7222 - accuracy: 0.7627 - val_loss: 0.5498 - val_accuracy: 0.8341\n",
      "Epoch 89/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7527 - accuracy: 0.7493\n",
      "Epoch 00089: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7527 - accuracy: 0.7493 - val_loss: 0.6596 - val_accuracy: 0.7707\n",
      "Epoch 90/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7304 - accuracy: 0.7530\n",
      "Epoch 00090: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7254 - accuracy: 0.7499 - val_loss: 0.7006 - val_accuracy: 0.7610\n",
      "Epoch 91/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7349 - accuracy: 0.7543\n",
      "Epoch 00091: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7450 - accuracy: 0.7510 - val_loss: 0.6147 - val_accuracy: 0.7805\n",
      "Epoch 92/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7303 - accuracy: 0.7530\n",
      "Epoch 00092: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7257 - accuracy: 0.7539 - val_loss: 0.6311 - val_accuracy: 0.8098\n",
      "Epoch 93/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7263 - accuracy: 0.7549\n",
      "Epoch 00093: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7185 - accuracy: 0.7569 - val_loss: 0.5765 - val_accuracy: 0.8341\n",
      "Epoch 94/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7088 - accuracy: 0.7500\n",
      "Epoch 00094: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7171 - accuracy: 0.7499 - val_loss: 0.5903 - val_accuracy: 0.8049\n",
      "Epoch 95/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7317 - accuracy: 0.7505\n",
      "Epoch 00095: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7361 - accuracy: 0.7475 - val_loss: 0.6291 - val_accuracy: 0.7659\n",
      "Epoch 96/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.7615\n",
      "Epoch 00096: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7132 - accuracy: 0.7615 - val_loss: 0.5681 - val_accuracy: 0.8390\n",
      "Epoch 97/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6963 - accuracy: 0.7632\n",
      "Epoch 00097: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6931 - accuracy: 0.7638 - val_loss: 0.5928 - val_accuracy: 0.8049\n",
      "Epoch 98/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6931 - accuracy: 0.7574\n",
      "Epoch 00098: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7031 - accuracy: 0.7534 - val_loss: 0.6416 - val_accuracy: 0.7805\n",
      "Epoch 99/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6836 - accuracy: 0.7710\n",
      "Epoch 00099: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6852 - accuracy: 0.7714 - val_loss: 0.6006 - val_accuracy: 0.8098\n",
      "Epoch 100/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7185 - accuracy: 0.7473\n",
      "Epoch 00100: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7293 - accuracy: 0.7452 - val_loss: 0.6167 - val_accuracy: 0.8000\n",
      "Epoch 101/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 0.7708\n",
      "Epoch 00101: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6750 - accuracy: 0.7708 - val_loss: 0.5885 - val_accuracy: 0.8000\n",
      "Epoch 102/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.7534\n",
      "Epoch 00102: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7093 - accuracy: 0.7534 - val_loss: 0.6123 - val_accuracy: 0.8049\n",
      "Epoch 103/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6532 - accuracy: 0.7820\n",
      "Epoch 00103: val_loss did not improve from 0.54979\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6580 - accuracy: 0.7796 - val_loss: 0.6264 - val_accuracy: 0.7951\n",
      "Epoch 104/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.7650\n",
      "Epoch 00104: val_loss improved from 0.54979 to 0.54762, saving model to ./data/cvision/model\\model_9-6ep104-vl0.5476.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6707 - accuracy: 0.7650 - val_loss: 0.5476 - val_accuracy: 0.8341\n",
      "Epoch 105/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6638 - accuracy: 0.7750\n",
      "Epoch 00105: val_loss did not improve from 0.54762\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6514 - accuracy: 0.7790 - val_loss: 0.6192 - val_accuracy: 0.8049\n",
      "Epoch 106/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6625 - accuracy: 0.7675\n",
      "Epoch 00106: val_loss improved from 0.54762 to 0.50155, saving model to ./data/cvision/model\\model_9-6ep106-vl0.5016.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6715 - accuracy: 0.7673 - val_loss: 0.5016 - val_accuracy: 0.8341\n",
      "Epoch 107/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6172 - accuracy: 0.7860\n",
      "Epoch 00107: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6172 - accuracy: 0.7860 - val_loss: 0.5143 - val_accuracy: 0.8537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6470 - accuracy: 0.7755\n",
      "Epoch 00108: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6470 - accuracy: 0.7755 - val_loss: 0.5674 - val_accuracy: 0.8244\n",
      "Epoch 109/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6292 - accuracy: 0.7927\n",
      "Epoch 00109: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6274 - accuracy: 0.7918 - val_loss: 0.5529 - val_accuracy: 0.8146\n",
      "Epoch 110/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6629 - accuracy: 0.7738\n",
      "Epoch 00110: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6639 - accuracy: 0.7743 - val_loss: 0.5948 - val_accuracy: 0.8146\n",
      "Epoch 111/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6193 - accuracy: 0.7933 ETA: 0s - loss: 0.6240 - accura\n",
      "Epoch 00111: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6193 - accuracy: 0.7930 - val_loss: 0.5329 - val_accuracy: 0.8390\n",
      "Epoch 112/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6241 - accuracy: 0.7940\n",
      "Epoch 00112: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6254 - accuracy: 0.7918 - val_loss: 0.5525 - val_accuracy: 0.8146\n",
      "Epoch 113/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6516 - accuracy: 0.7808\n",
      "Epoch 00113: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6516 - accuracy: 0.7808 - val_loss: 0.5089 - val_accuracy: 0.8488\n",
      "Epoch 114/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6627 - accuracy: 0.7713\n",
      "Epoch 00114: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6636 - accuracy: 0.7720 - val_loss: 0.5436 - val_accuracy: 0.8341\n",
      "Epoch 115/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.7813\n",
      "Epoch 00115: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6446 - accuracy: 0.7813 - val_loss: 0.5572 - val_accuracy: 0.8293\n",
      "Epoch 116/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6291 - accuracy: 0.7914\n",
      "Epoch 00116: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6336 - accuracy: 0.7889 - val_loss: 0.5649 - val_accuracy: 0.8000\n",
      "Epoch 117/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6179 - accuracy: 0.7849\n",
      "Epoch 00117: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6063 - accuracy: 0.7879 - val_loss: 0.5633 - val_accuracy: 0.8049\n",
      "Epoch 118/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.7761\n",
      "Epoch 00118: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6357 - accuracy: 0.7761 - val_loss: 0.5476 - val_accuracy: 0.8293\n",
      "Epoch 119/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.7895\n",
      "Epoch 00119: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6129 - accuracy: 0.7895 - val_loss: 0.5096 - val_accuracy: 0.8488\n",
      "Epoch 120/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.7837\n",
      "Epoch 00120: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6322 - accuracy: 0.7837 - val_loss: 0.6534 - val_accuracy: 0.8049\n",
      "Epoch 121/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.7936\n",
      "Epoch 00121: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6071 - accuracy: 0.7936 - val_loss: 0.5431 - val_accuracy: 0.8244\n",
      "Epoch 122/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.7901\n",
      "Epoch 00122: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5967 - accuracy: 0.7901 - val_loss: 0.5302 - val_accuracy: 0.8439\n",
      "Epoch 123/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.7988\n",
      "Epoch 00123: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5939 - accuracy: 0.7988 - val_loss: 0.5045 - val_accuracy: 0.8293\n",
      "Epoch 124/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6016 - accuracy: 0.7819\n",
      "Epoch 00124: val_loss did not improve from 0.50155\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6016 - accuracy: 0.7819 - val_loss: 0.5985 - val_accuracy: 0.8195\n",
      "Epoch 125/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5696 - accuracy: 0.8029\n",
      "Epoch 00125: val_loss improved from 0.50155 to 0.44163, saving model to ./data/cvision/model\\model_9-6ep125-vl0.4416.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5733 - accuracy: 0.8013 - val_loss: 0.4416 - val_accuracy: 0.8683\n",
      "Epoch 126/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6051 - accuracy: 0.8009\n",
      "Epoch 00126: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6031 - accuracy: 0.8000 - val_loss: 0.5226 - val_accuracy: 0.8293\n",
      "Epoch 127/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6174 - accuracy: 0.7788\n",
      "Epoch 00127: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6165 - accuracy: 0.7790 - val_loss: 0.5321 - val_accuracy: 0.8293\n",
      "Epoch 128/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.7918\n",
      "Epoch 00128: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5993 - accuracy: 0.7918 - val_loss: 0.5575 - val_accuracy: 0.8244\n",
      "Epoch 129/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.8017\n",
      "Epoch 00129: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5620 - accuracy: 0.8017 - val_loss: 0.5289 - val_accuracy: 0.8488\n",
      "Epoch 130/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.8128\n",
      "Epoch 00130: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5283 - accuracy: 0.8128 - val_loss: 0.5561 - val_accuracy: 0.8293\n",
      "Epoch 131/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5323 - accuracy: 0.8040\n",
      "Epoch 00131: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5498 - accuracy: 0.8000 - val_loss: 0.4986 - val_accuracy: 0.8341\n",
      "Epoch 132/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.7901\n",
      "Epoch 00132: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5875 - accuracy: 0.7901 - val_loss: 0.5287 - val_accuracy: 0.8195\n",
      "Epoch 133/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.8157\n",
      "Epoch 00133: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5405 - accuracy: 0.8157 - val_loss: 0.5248 - val_accuracy: 0.8341\n",
      "Epoch 134/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5431 - accuracy: 0.8146\n",
      "Epoch 00134: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5431 - accuracy: 0.8146 - val_loss: 0.5499 - val_accuracy: 0.8195\n",
      "Epoch 135/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.8227\n",
      "Epoch 00135: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5353 - accuracy: 0.8227 - val_loss: 0.4934 - val_accuracy: 0.8244\n",
      "Epoch 136/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.8017\n",
      "Epoch 00136: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6055 - accuracy: 0.8017 - val_loss: 0.5231 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5448 - accuracy: 0.8131\n",
      "Epoch 00137: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5411 - accuracy: 0.8128 - val_loss: 0.4795 - val_accuracy: 0.8585\n",
      "Epoch 138/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5753 - accuracy: 0.8003\n",
      "Epoch 00138: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5752 - accuracy: 0.8023 - val_loss: 0.5439 - val_accuracy: 0.8390\n",
      "Epoch 139/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.8099\n",
      "Epoch 00139: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5484 - accuracy: 0.8099 - val_loss: 0.5125 - val_accuracy: 0.8537\n",
      "Epoch 140/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.8000\n",
      "Epoch 00140: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5858 - accuracy: 0.8000 - val_loss: 0.5509 - val_accuracy: 0.8293\n",
      "Epoch 141/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5444 - accuracy: 0.8066\n",
      "Epoch 00141: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5446 - accuracy: 0.8093 - val_loss: 0.4938 - val_accuracy: 0.8244\n",
      "Epoch 142/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.8204\n",
      "Epoch 00142: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5321 - accuracy: 0.8204 - val_loss: 0.5013 - val_accuracy: 0.8439\n",
      "Epoch 143/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5658 - accuracy: 0.8006\n",
      "Epoch 00143: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5658 - accuracy: 0.8006 - val_loss: 0.5337 - val_accuracy: 0.8293\n",
      "Epoch 144/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.8239\n",
      "Epoch 00144: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5041 - accuracy: 0.8239 - val_loss: 0.5103 - val_accuracy: 0.8634\n",
      "Epoch 145/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5231 - accuracy: 0.8143\n",
      "Epoch 00145: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5227 - accuracy: 0.8117 - val_loss: 0.5156 - val_accuracy: 0.8439\n",
      "Epoch 146/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5308 - accuracy: 0.8137\n",
      "Epoch 00146: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5294 - accuracy: 0.8152 - val_loss: 0.6082 - val_accuracy: 0.8146\n",
      "Epoch 147/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5080 - accuracy: 0.8215\n",
      "Epoch 00147: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5066 - accuracy: 0.8210 - val_loss: 0.4975 - val_accuracy: 0.8146\n",
      "Epoch 148/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.8192\n",
      "Epoch 00148: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5558 - accuracy: 0.8192 - val_loss: 0.5225 - val_accuracy: 0.8390\n",
      "Epoch 149/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.8128\n",
      "Epoch 00149: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5364 - accuracy: 0.8128 - val_loss: 0.5348 - val_accuracy: 0.8390\n",
      "Epoch 150/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5365 - accuracy: 0.8125\n",
      "Epoch 00150: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5363 - accuracy: 0.8117 - val_loss: 0.5150 - val_accuracy: 0.8341\n",
      "Epoch 151/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.82 - ETA: 0s - loss: 0.5130 - accuracy: 0.8216\n",
      "Epoch 00151: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5130 - accuracy: 0.8216 - val_loss: 0.5162 - val_accuracy: 0.8244\n",
      "Epoch 152/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.8402\n",
      "Epoch 00152: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4792 - accuracy: 0.8402 - val_loss: 0.5042 - val_accuracy: 0.8195\n",
      "Epoch 153/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5551 - accuracy: 0.8116\n",
      "Epoch 00153: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5521 - accuracy: 0.8122 - val_loss: 0.5539 - val_accuracy: 0.8146\n",
      "Epoch 154/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5371 - accuracy: 0.8179\n",
      "Epoch 00154: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5310 - accuracy: 0.8216 - val_loss: 0.4958 - val_accuracy: 0.8488\n",
      "Epoch 155/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5004 - accuracy: 0.8269\n",
      "Epoch 00155: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5053 - accuracy: 0.8257 - val_loss: 0.4726 - val_accuracy: 0.8683\n",
      "Epoch 156/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8117\n",
      "Epoch 00156: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5329 - accuracy: 0.8117 - val_loss: 0.5232 - val_accuracy: 0.8146\n",
      "Epoch 157/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.8198\n",
      "Epoch 00157: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5143 - accuracy: 0.8198 - val_loss: 0.4747 - val_accuracy: 0.8683\n",
      "Epoch 158/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5245 - accuracy: 0.8122\n",
      "Epoch 00158: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5195 - accuracy: 0.8134 - val_loss: 0.4924 - val_accuracy: 0.8537\n",
      "Epoch 159/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5137 - accuracy: 0.8223 ETA: 0s - loss: 0.5281 - accuracy\n",
      "Epoch 00159: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5158 - accuracy: 0.8198 - val_loss: 0.4950 - val_accuracy: 0.8390\n",
      "Epoch 160/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.8367\n",
      "Epoch 00160: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4587 - accuracy: 0.8367 - val_loss: 0.4931 - val_accuracy: 0.8293\n",
      "Epoch 161/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.8216\n",
      "Epoch 00161: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5160 - accuracy: 0.8216 - val_loss: 0.6013 - val_accuracy: 0.8049\n",
      "Epoch 162/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5234 - accuracy: 0.8149\n",
      "Epoch 00162: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5201 - accuracy: 0.8163 - val_loss: 0.4667 - val_accuracy: 0.8537\n",
      "Epoch 163/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.8344\n",
      "Epoch 00163: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4750 - accuracy: 0.8344 - val_loss: 0.4735 - val_accuracy: 0.8683\n",
      "Epoch 164/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.8251\n",
      "Epoch 00164: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4901 - accuracy: 0.8251 - val_loss: 0.5930 - val_accuracy: 0.8244\n",
      "Epoch 165/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.8257\n",
      "Epoch 00165: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4837 - accuracy: 0.8257 - val_loss: 0.4862 - val_accuracy: 0.8341\n",
      "Epoch 166/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4721 - accuracy: 0.8336\n",
      "Epoch 00166: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4701 - accuracy: 0.8332 - val_loss: 0.5045 - val_accuracy: 0.8537\n",
      "Epoch 167/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4760 - accuracy: 0.8198\n",
      "Epoch 00167: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4760 - accuracy: 0.8198 - val_loss: 0.5258 - val_accuracy: 0.8244\n",
      "Epoch 168/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4695 - accuracy: 0.8431\n",
      "Epoch 00168: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4770 - accuracy: 0.8391 - val_loss: 0.4937 - val_accuracy: 0.8439\n",
      "Epoch 169/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4825 - accuracy: 0.8387\n",
      "Epoch 00169: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4879 - accuracy: 0.8367 - val_loss: 0.5309 - val_accuracy: 0.8341\n",
      "Epoch 170/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4815 - accuracy: 0.8280\n",
      "Epoch 00170: val_loss did not improve from 0.44163\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4879 - accuracy: 0.8257 - val_loss: 0.4837 - val_accuracy: 0.8732\n",
      "Epoch 171/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.8280\n",
      "Epoch 00171: val_loss improved from 0.44163 to 0.43500, saving model to ./data/cvision/model\\model_9-6ep171-vl0.4350.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4921 - accuracy: 0.8280 - val_loss: 0.4350 - val_accuracy: 0.8732\n",
      "Epoch 172/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8303\n",
      "Epoch 00172: val_loss did not improve from 0.43500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5037 - accuracy: 0.8303 - val_loss: 0.4420 - val_accuracy: 0.8634\n",
      "Epoch 173/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4993 - accuracy: 0.8229\n",
      "Epoch 00173: val_loss improved from 0.43500 to 0.43267, saving model to ./data/cvision/model\\model_9-6ep173-vl0.4327.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4999 - accuracy: 0.8210 - val_loss: 0.4327 - val_accuracy: 0.8634\n",
      "Epoch 174/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5318 - accuracy: 0.8131\n",
      "Epoch 00174: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5257 - accuracy: 0.8153 - val_loss: 0.5445 - val_accuracy: 0.8146\n",
      "Epoch 175/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.8402\n",
      "Epoch 00175: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4803 - accuracy: 0.8402 - val_loss: 0.4982 - val_accuracy: 0.8341\n",
      "Epoch 176/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4608 - accuracy: 0.8444\n",
      "Epoch 00176: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4626 - accuracy: 0.8431 - val_loss: 0.5134 - val_accuracy: 0.8488\n",
      "Epoch 177/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8426\n",
      "Epoch 00177: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4713 - accuracy: 0.8426 - val_loss: 0.5246 - val_accuracy: 0.8293\n",
      "Epoch 178/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.8455\n",
      "Epoch 00178: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4735 - accuracy: 0.8455 - val_loss: 0.4733 - val_accuracy: 0.8634\n",
      "Epoch 179/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4642 - accuracy: 0.8419\n",
      "Epoch 00179: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4596 - accuracy: 0.8449 - val_loss: 0.4905 - val_accuracy: 0.8293\n",
      "Epoch 180/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4400 - accuracy: 0.8425\n",
      "Epoch 00180: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4459 - accuracy: 0.8426 - val_loss: 0.4739 - val_accuracy: 0.8488\n",
      "Epoch 181/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.8501\n",
      "Epoch 00181: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4322 - accuracy: 0.8501 - val_loss: 0.4590 - val_accuracy: 0.8732\n",
      "Epoch 182/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4678 - accuracy: 0.8362\n",
      "Epoch 00182: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4653 - accuracy: 0.8362 - val_loss: 0.4767 - val_accuracy: 0.8390\n",
      "Epoch 183/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4784 - accuracy: 0.8393\n",
      "Epoch 00183: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4695 - accuracy: 0.8420 - val_loss: 0.5275 - val_accuracy: 0.8439\n",
      "Epoch 184/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4389 - accuracy: 0.8472\n",
      "Epoch 00184: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4389 - accuracy: 0.8472 - val_loss: 0.4907 - val_accuracy: 0.8390\n",
      "Epoch 185/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4281 - accuracy: 0.8519\n",
      "Epoch 00185: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4256 - accuracy: 0.8519 - val_loss: 0.4834 - val_accuracy: 0.8390\n",
      "Epoch 186/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.8443\n",
      "Epoch 00186: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4522 - accuracy: 0.8443 - val_loss: 0.5196 - val_accuracy: 0.8244\n",
      "Epoch 187/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.8455\n",
      "Epoch 00187: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4401 - accuracy: 0.8455 - val_loss: 0.4614 - val_accuracy: 0.8439\n",
      "Epoch 188/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4354 - accuracy: 0.8444\n",
      "Epoch 00188: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4295 - accuracy: 0.8466 - val_loss: 0.4734 - val_accuracy: 0.8683\n",
      "Epoch 189/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4505 - accuracy: 0.8365 ETA: 0s - loss: 0.4611 - accuracy\n",
      "Epoch 00189: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4571 - accuracy: 0.8365 - val_loss: 0.5326 - val_accuracy: 0.8195\n",
      "Epoch 190/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4344 - accuracy: 0.8455\n",
      "Epoch 00190: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4344 - accuracy: 0.8455 - val_loss: 0.4679 - val_accuracy: 0.8634\n",
      "Epoch 191/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8431\n",
      "Epoch 00191: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4310 - accuracy: 0.8431 - val_loss: 0.5478 - val_accuracy: 0.8244\n",
      "Epoch 192/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4758 - accuracy: 0.8362\n",
      "Epoch 00192: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4800 - accuracy: 0.8344 - val_loss: 0.4544 - val_accuracy: 0.8488\n",
      "Epoch 193/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4286 - accuracy: 0.8516\n",
      "Epoch 00193: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4379 - accuracy: 0.8465 - val_loss: 0.4568 - val_accuracy: 0.8537\n",
      "Epoch 194/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8315\n",
      "Epoch 00194: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4932 - accuracy: 0.8315 - val_loss: 0.5211 - val_accuracy: 0.8439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4368 - accuracy: 0.8475\n",
      "Epoch 00195: val_loss did not improve from 0.43267\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4357 - accuracy: 0.8490 - val_loss: 0.4481 - val_accuracy: 0.8488\n",
      "Epoch 196/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4134 - accuracy: 0.8419\n",
      "Epoch 00196: val_loss improved from 0.43267 to 0.43114, saving model to ./data/cvision/model\\model_9-6ep196-vl0.4311.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4127 - accuracy: 0.8437 - val_loss: 0.4311 - val_accuracy: 0.8732\n",
      "Epoch 197/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4080 - accuracy: 0.8576\n",
      "Epoch 00197: val_loss did not improve from 0.43114\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4072 - accuracy: 0.8577 - val_loss: 0.4471 - val_accuracy: 0.8683\n",
      "Epoch 198/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.8472\n",
      "Epoch 00198: val_loss did not improve from 0.43114\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4397 - accuracy: 0.8472 - val_loss: 0.4771 - val_accuracy: 0.8439\n",
      "Epoch 199/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.8496\n",
      "Epoch 00199: val_loss did not improve from 0.43114\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4329 - accuracy: 0.8496 - val_loss: 0.4571 - val_accuracy: 0.8488\n",
      "Epoch 200/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3879 - accuracy: 0.8626\n",
      "Epoch 00200: val_loss did not improve from 0.43114\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3977 - accuracy: 0.8601 - val_loss: 0.4891 - val_accuracy: 0.8293\n",
      "Epoch 201/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4378 - accuracy: 0.8426\n",
      "Epoch 00201: val_loss did not improve from 0.43114\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4378 - accuracy: 0.8426 - val_loss: 0.5428 - val_accuracy: 0.8195\n",
      "Epoch 202/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4382 - accuracy: 0.8425\n",
      "Epoch 00202: val_loss improved from 0.43114 to 0.43042, saving model to ./data/cvision/model\\model_9-6ep202-vl0.4304.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4398 - accuracy: 0.8437 - val_loss: 0.4304 - val_accuracy: 0.8878\n",
      "Epoch 203/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4498 - accuracy: 0.8414\n",
      "Epoch 00203: val_loss did not improve from 0.43042\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4498 - accuracy: 0.8414 - val_loss: 0.4330 - val_accuracy: 0.8878\n",
      "Epoch 204/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4216 - accuracy: 0.8519\n",
      "Epoch 00204: val_loss improved from 0.43042 to 0.38848, saving model to ./data/cvision/model\\model_9-6ep204-vl0.3885.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.4265 - accuracy: 0.8525 - val_loss: 0.3885 - val_accuracy: 0.8927\n",
      "Epoch 205/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.8490\n",
      "Epoch 00205: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4258 - accuracy: 0.8490 - val_loss: 0.5054 - val_accuracy: 0.8634\n",
      "Epoch 206/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4510 - accuracy: 0.8431\n",
      "Epoch 00206: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4597 - accuracy: 0.8397 - val_loss: 0.5100 - val_accuracy: 0.8146\n",
      "Epoch 207/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4103 - accuracy: 0.8526\n",
      "Epoch 00207: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4131 - accuracy: 0.8519 - val_loss: 0.4490 - val_accuracy: 0.8634\n",
      "Epoch 208/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8566\n",
      "Epoch 00208: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4094 - accuracy: 0.8566 - val_loss: 0.4383 - val_accuracy: 0.8537\n",
      "Epoch 209/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4095 - accuracy: 0.8614\n",
      "Epoch 00209: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4160 - accuracy: 0.8589 - val_loss: 0.4430 - val_accuracy: 0.8829\n",
      "Epoch 210/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3938 - accuracy: 0.8626\n",
      "Epoch 00210: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3986 - accuracy: 0.8624 - val_loss: 0.5271 - val_accuracy: 0.8293\n",
      "Epoch 211/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4075 - accuracy: 0.8639\n",
      "Epoch 00211: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4049 - accuracy: 0.8647 - val_loss: 0.4236 - val_accuracy: 0.8390\n",
      "Epoch 212/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8490\n",
      "Epoch 00212: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4404 - accuracy: 0.8490 - val_loss: 0.4662 - val_accuracy: 0.8341\n",
      "Epoch 213/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.8344\n",
      "Epoch 00213: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4455 - accuracy: 0.8344 - val_loss: 0.5121 - val_accuracy: 0.8146\n",
      "Epoch 214/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4438 - accuracy: 0.8513\n",
      "Epoch 00214: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4438 - accuracy: 0.8513 - val_loss: 0.4278 - val_accuracy: 0.8585\n",
      "Epoch 215/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4278 - accuracy: 0.8462\n",
      "Epoch 00215: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4266 - accuracy: 0.8466 - val_loss: 0.5436 - val_accuracy: 0.8293\n",
      "Epoch 216/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4370 - accuracy: 0.8466\n",
      "Epoch 00216: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4370 - accuracy: 0.8466 - val_loss: 0.4310 - val_accuracy: 0.8537\n",
      "Epoch 217/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4114 - accuracy: 0.8620\n",
      "Epoch 00217: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4133 - accuracy: 0.8595 - val_loss: 0.4633 - val_accuracy: 0.8390\n",
      "Epoch 218/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8606\n",
      "Epoch 00218: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4241 - accuracy: 0.8606 - val_loss: 0.4248 - val_accuracy: 0.8683\n",
      "Epoch 219/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4087 - accuracy: 0.8612\n",
      "Epoch 00219: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4087 - accuracy: 0.8612 - val_loss: 0.3991 - val_accuracy: 0.8634\n",
      "Epoch 220/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4090 - accuracy: 0.8544\n",
      "Epoch 00220: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4060 - accuracy: 0.8560 - val_loss: 0.5372 - val_accuracy: 0.8244\n",
      "Epoch 221/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.8630\n",
      "Epoch 00221: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3994 - accuracy: 0.8630 - val_loss: 0.4882 - val_accuracy: 0.8537\n",
      "Epoch 222/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.8531\n",
      "Epoch 00222: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4011 - accuracy: 0.8531 - val_loss: 0.4684 - val_accuracy: 0.8341\n",
      "Epoch 223/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4067 - accuracy: 0.8576\n",
      "Epoch 00223: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4048 - accuracy: 0.8595 - val_loss: 0.4204 - val_accuracy: 0.8732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.8525\n",
      "Epoch 00224: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4157 - accuracy: 0.8525 - val_loss: 0.4674 - val_accuracy: 0.8439\n",
      "Epoch 225/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8665\n",
      "Epoch 00225: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3929 - accuracy: 0.8665 - val_loss: 0.4806 - val_accuracy: 0.8439\n",
      "Epoch 226/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8536\n",
      "Epoch 00226: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4001 - accuracy: 0.8536 - val_loss: 0.4471 - val_accuracy: 0.8293\n",
      "Epoch 227/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8571\n",
      "Epoch 00227: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4055 - accuracy: 0.8571 - val_loss: 0.4568 - val_accuracy: 0.8488\n",
      "Epoch 228/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3708 - accuracy: 0.8576\n",
      "Epoch 00228: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3662 - accuracy: 0.8594 - val_loss: 0.4094 - val_accuracy: 0.8537\n",
      "Epoch 229/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4003 - accuracy: 0.8636\n",
      "Epoch 00229: val_loss did not improve from 0.38848\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4003 - accuracy: 0.8636 - val_loss: 0.4849 - val_accuracy: 0.8341\n",
      "Epoch 230/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8566\n",
      "Epoch 00230: val_loss improved from 0.38848 to 0.37440, saving model to ./data/cvision/model\\model_9-6ep230-vl0.3744.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3562 - accuracy: 0.8566 - val_loss: 0.3744 - val_accuracy: 0.8780\n",
      "Epoch 231/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3749 - accuracy: 0.8702\n",
      "Epoch 00231: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3804 - accuracy: 0.8694 - val_loss: 0.4844 - val_accuracy: 0.8390\n",
      "Epoch 232/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3954 - accuracy: 0.8671\n",
      "Epoch 00232: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3954 - accuracy: 0.8671 - val_loss: 0.4334 - val_accuracy: 0.8439\n",
      "Epoch 233/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3915 - accuracy: 0.8670\n",
      "Epoch 00233: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3894 - accuracy: 0.8671 - val_loss: 0.4443 - val_accuracy: 0.8683\n",
      "Epoch 234/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3899 - accuracy: 0.8595\n",
      "Epoch 00234: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3861 - accuracy: 0.8589 - val_loss: 0.4689 - val_accuracy: 0.8634\n",
      "Epoch 235/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4210 - accuracy: 0.8407\n",
      "Epoch 00235: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4221 - accuracy: 0.8408 - val_loss: 0.4195 - val_accuracy: 0.8537\n",
      "Epoch 236/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3819 - accuracy: 0.8633\n",
      "Epoch 00236: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3837 - accuracy: 0.8636 - val_loss: 0.4128 - val_accuracy: 0.8634\n",
      "Epoch 237/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3976 - accuracy: 0.8526\n",
      "Epoch 00237: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3904 - accuracy: 0.8566 - val_loss: 0.4591 - val_accuracy: 0.8439\n",
      "Epoch 238/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3793 - accuracy: 0.8683\n",
      "Epoch 00238: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3756 - accuracy: 0.8700 - val_loss: 0.4385 - val_accuracy: 0.8683\n",
      "Epoch 239/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3940 - accuracy: 0.8600\n",
      "Epoch 00239: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3936 - accuracy: 0.8588 - val_loss: 0.4278 - val_accuracy: 0.8488\n",
      "Epoch 240/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.8787\n",
      "Epoch 00240: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3600 - accuracy: 0.8787 - val_loss: 0.4820 - val_accuracy: 0.8293\n",
      "Epoch 241/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3596 - accuracy: 0.8677\n",
      "Epoch 00241: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3551 - accuracy: 0.8682 - val_loss: 0.4860 - val_accuracy: 0.8390\n",
      "Epoch 242/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3684 - accuracy: 0.8652\n",
      "Epoch 00242: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3663 - accuracy: 0.8688 - val_loss: 0.4352 - val_accuracy: 0.8537\n",
      "Epoch 243/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.8700\n",
      "Epoch 00243: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3787 - accuracy: 0.8700 - val_loss: 0.4374 - val_accuracy: 0.8585\n",
      "Epoch 244/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3834 - accuracy: 0.8738\n",
      "Epoch 00244: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3795 - accuracy: 0.8761 - val_loss: 0.4603 - val_accuracy: 0.8732\n",
      "Epoch 245/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.8717\n",
      "Epoch 00245: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3745 - accuracy: 0.8717 - val_loss: 0.4608 - val_accuracy: 0.8488\n",
      "Epoch 246/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8764\n",
      "Epoch 00246: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3724 - accuracy: 0.8764 - val_loss: 0.4030 - val_accuracy: 0.8780\n",
      "Epoch 247/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.8688\n",
      "Epoch 00247: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3750 - accuracy: 0.8688 - val_loss: 0.4821 - val_accuracy: 0.8537\n",
      "Epoch 248/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.8706\n",
      "Epoch 00248: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3680 - accuracy: 0.8706 - val_loss: 0.4434 - val_accuracy: 0.8780\n",
      "Epoch 249/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3513 - accuracy: 0.8666\n",
      "Epoch 00249: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3472 - accuracy: 0.8683 - val_loss: 0.4048 - val_accuracy: 0.8829\n",
      "Epoch 250/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8805\n",
      "Epoch 00250: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3420 - accuracy: 0.8805 - val_loss: 0.4362 - val_accuracy: 0.8585\n",
      "Epoch 251/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.8776\n",
      "Epoch 00251: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3507 - accuracy: 0.8776 - val_loss: 0.4421 - val_accuracy: 0.8439\n",
      "Epoch 252/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3330 - accuracy: 0.8790\n",
      "Epoch 00252: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3268 - accuracy: 0.8810 - val_loss: 0.5088 - val_accuracy: 0.8341\n",
      "Epoch 253/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8711\n",
      "Epoch 00253: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3771 - accuracy: 0.8711 - val_loss: 0.4229 - val_accuracy: 0.8683\n",
      "Epoch 254/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3374 - accuracy: 0.8891\n",
      "Epoch 00254: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3467 - accuracy: 0.8863 - val_loss: 0.4597 - val_accuracy: 0.8732\n",
      "Epoch 255/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3489 - accuracy: 0.8822\n",
      "Epoch 00255: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3521 - accuracy: 0.8805 - val_loss: 0.4186 - val_accuracy: 0.8683\n",
      "Epoch 256/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3619 - accuracy: 0.8738\n",
      "Epoch 00256: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3620 - accuracy: 0.8728 - val_loss: 0.4672 - val_accuracy: 0.8439\n",
      "Epoch 257/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3665 - accuracy: 0.8727\n",
      "Epoch 00257: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3695 - accuracy: 0.8729 - val_loss: 0.3956 - val_accuracy: 0.8829\n",
      "Epoch 258/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.8770\n",
      "Epoch 00258: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3699 - accuracy: 0.8770 - val_loss: 0.4132 - val_accuracy: 0.8780\n",
      "Epoch 259/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3417 - accuracy: 0.8828\n",
      "Epoch 00259: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3419 - accuracy: 0.8816 - val_loss: 0.4588 - val_accuracy: 0.8732\n",
      "Epoch 260/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8741\n",
      "Epoch 00260: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3637 - accuracy: 0.8741 - val_loss: 0.4444 - val_accuracy: 0.8537\n",
      "Epoch 261/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3422 - accuracy: 0.8822\n",
      "Epoch 00261: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3430 - accuracy: 0.8810 - val_loss: 0.4584 - val_accuracy: 0.8439\n",
      "Epoch 262/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3530 - accuracy: 0.8780\n",
      "Epoch 00262: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3541 - accuracy: 0.8795 - val_loss: 0.4635 - val_accuracy: 0.8537\n",
      "Epoch 263/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3447 - accuracy: 0.8828\n",
      "Epoch 00263: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3514 - accuracy: 0.8793 - val_loss: 0.4715 - val_accuracy: 0.8341\n",
      "Epoch 264/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8840\n",
      "Epoch 00264: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3720 - accuracy: 0.8840 - val_loss: 0.4049 - val_accuracy: 0.8829\n",
      "Epoch 265/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8781\n",
      "Epoch 00265: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3515 - accuracy: 0.8781 - val_loss: 0.4110 - val_accuracy: 0.8683\n",
      "Epoch 266/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.8770\n",
      "Epoch 00266: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3485 - accuracy: 0.8770 - val_loss: 0.4504 - val_accuracy: 0.8585\n",
      "Epoch 267/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3720 - accuracy: 0.8690\n",
      "Epoch 00267: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3792 - accuracy: 0.8672 - val_loss: 0.4079 - val_accuracy: 0.8683\n",
      "Epoch 268/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3447 - accuracy: 0.8765\n",
      "Epoch 00268: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3391 - accuracy: 0.8776 - val_loss: 0.4497 - val_accuracy: 0.8537\n",
      "Epoch 269/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3257 - accuracy: 0.8784\n",
      "Epoch 00269: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3367 - accuracy: 0.8752 - val_loss: 0.4559 - val_accuracy: 0.8634\n",
      "Epoch 270/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3701 - accuracy: 0.8683\n",
      "Epoch 00270: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3734 - accuracy: 0.8682 - val_loss: 0.4883 - val_accuracy: 0.8488\n",
      "Epoch 271/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3639 - accuracy: 0.8727\n",
      "Epoch 00271: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3539 - accuracy: 0.8764 - val_loss: 0.4390 - val_accuracy: 0.8439\n",
      "Epoch 272/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3530 - accuracy: 0.8721\n",
      "Epoch 00272: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3492 - accuracy: 0.8746 - val_loss: 0.4828 - val_accuracy: 0.8439\n",
      "Epoch 273/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8822\n",
      "Epoch 00273: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3299 - accuracy: 0.8822 - val_loss: 0.4039 - val_accuracy: 0.8634\n",
      "Epoch 274/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8834\n",
      "Epoch 00274: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3405 - accuracy: 0.8834 - val_loss: 0.4027 - val_accuracy: 0.8829\n",
      "Epoch 275/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3327 - accuracy: 0.8859\n",
      "Epoch 00275: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3465 - accuracy: 0.8816 - val_loss: 0.4222 - val_accuracy: 0.8683\n",
      "Epoch 276/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3224 - accuracy: 0.8864\n",
      "Epoch 00276: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3206 - accuracy: 0.8875 - val_loss: 0.4468 - val_accuracy: 0.8634\n",
      "Epoch 277/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.8892\n",
      "Epoch 00277: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3076 - accuracy: 0.8892 - val_loss: 0.3922 - val_accuracy: 0.8878\n",
      "Epoch 278/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8764\n",
      "Epoch 00278: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3627 - accuracy: 0.8764 - val_loss: 0.3962 - val_accuracy: 0.8829\n",
      "Epoch 279/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3424 - accuracy: 0.8803\n",
      "Epoch 00279: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3423 - accuracy: 0.8799 - val_loss: 0.4681 - val_accuracy: 0.8634\n",
      "Epoch 280/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8857\n",
      "Epoch 00280: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3365 - accuracy: 0.8857 - val_loss: 0.4506 - val_accuracy: 0.8439\n",
      "Epoch 281/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2953 - accuracy: 0.8929\n",
      "Epoch 00281: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2993 - accuracy: 0.8921 - val_loss: 0.4351 - val_accuracy: 0.8683\n",
      "Epoch 282/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3457 - accuracy: 0.8857\n",
      "Epoch 00282: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3457 - accuracy: 0.8857 - val_loss: 0.4519 - val_accuracy: 0.8537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.8834\n",
      "Epoch 00283: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3369 - accuracy: 0.8834 - val_loss: 0.3909 - val_accuracy: 0.8976\n",
      "Epoch 284/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3589 - accuracy: 0.8784\n",
      "Epoch 00284: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3562 - accuracy: 0.8781 - val_loss: 0.4195 - val_accuracy: 0.8585\n",
      "Epoch 285/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8886\n",
      "Epoch 00285: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3312 - accuracy: 0.8886 - val_loss: 0.4202 - val_accuracy: 0.8878\n",
      "Epoch 286/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8840\n",
      "Epoch 00286: val_loss did not improve from 0.37440\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3420 - accuracy: 0.8840 - val_loss: 0.4624 - val_accuracy: 0.8683\n",
      "Epoch 287/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3788 - accuracy: 0.8626\n",
      "Epoch 00287: val_loss improved from 0.37440 to 0.34740, saving model to ./data/cvision/model\\model_9-6ep287-vl0.3474.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3722 - accuracy: 0.8636 - val_loss: 0.3474 - val_accuracy: 0.8976\n",
      "Epoch 288/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3637 - accuracy: 0.8696 ETA: 0s - loss: 0.3138 - accuracy\n",
      "Epoch 00288: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3616 - accuracy: 0.8700 - val_loss: 0.4018 - val_accuracy: 0.8634\n",
      "Epoch 289/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3526 - accuracy: 0.8784\n",
      "Epoch 00289: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3549 - accuracy: 0.8781 - val_loss: 0.3894 - val_accuracy: 0.8732\n",
      "Epoch 290/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.8927\n",
      "Epoch 00290: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3284 - accuracy: 0.8927 - val_loss: 0.4715 - val_accuracy: 0.8585\n",
      "Epoch 291/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8723\n",
      "Epoch 00291: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3517 - accuracy: 0.8723 - val_loss: 0.4713 - val_accuracy: 0.8390\n",
      "Epoch 292/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3658 - accuracy: 0.8810\n",
      "Epoch 00292: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3632 - accuracy: 0.8811 - val_loss: 0.4384 - val_accuracy: 0.8683\n",
      "Epoch 293/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3487 - accuracy: 0.8793\n",
      "Epoch 00293: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3487 - accuracy: 0.8793 - val_loss: 0.4020 - val_accuracy: 0.8488\n",
      "Epoch 294/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3452 - accuracy: 0.8803\n",
      "Epoch 00294: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3352 - accuracy: 0.8845 - val_loss: 0.4664 - val_accuracy: 0.8537\n",
      "Epoch 295/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8892\n",
      "Epoch 00295: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3049 - accuracy: 0.8892 - val_loss: 0.4077 - val_accuracy: 0.8780\n",
      "Epoch 296/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8764\n",
      "Epoch 00296: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3371 - accuracy: 0.8764 - val_loss: 0.3954 - val_accuracy: 0.8634\n",
      "Epoch 297/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8857\n",
      "Epoch 00297: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3221 - accuracy: 0.8857 - val_loss: 0.3556 - val_accuracy: 0.8780\n",
      "Epoch 298/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3292 - accuracy: 0.8866\n",
      "Epoch 00298: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3275 - accuracy: 0.8863 - val_loss: 0.4136 - val_accuracy: 0.8585\n",
      "Epoch 299/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.8840\n",
      "Epoch 00299: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3262 - accuracy: 0.8840 - val_loss: 0.4313 - val_accuracy: 0.8683\n",
      "Epoch 300/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3258 - accuracy: 0.8916\n",
      "Epoch 00300: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3277 - accuracy: 0.8904 - val_loss: 0.4159 - val_accuracy: 0.8780\n",
      "Epoch 301/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3315 - accuracy: 0.8778\n",
      "Epoch 00301: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3250 - accuracy: 0.8793 - val_loss: 0.3481 - val_accuracy: 0.8927\n",
      "Epoch 302/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8904\n",
      "Epoch 00302: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3094 - accuracy: 0.8904 - val_loss: 0.3925 - val_accuracy: 0.8927\n",
      "Epoch 303/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3156 - accuracy: 0.8852\n",
      "Epoch 00303: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3143 - accuracy: 0.8851 - val_loss: 0.4335 - val_accuracy: 0.8537\n",
      "Epoch 304/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8892\n",
      "Epoch 00304: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3270 - accuracy: 0.8892 - val_loss: 0.3604 - val_accuracy: 0.8683\n",
      "Epoch 305/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3112 - accuracy: 0.8878\n",
      "Epoch 00305: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3172 - accuracy: 0.8851 - val_loss: 0.3909 - val_accuracy: 0.8878\n",
      "Epoch 306/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3048 - accuracy: 0.8870\n",
      "Epoch 00306: val_loss did not improve from 0.34740\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3065 - accuracy: 0.8856 - val_loss: 0.3744 - val_accuracy: 0.8829\n",
      "Epoch 307/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8921\n",
      "Epoch 00307: val_loss improved from 0.34740 to 0.34660, saving model to ./data/cvision/model\\model_9-6ep307-vl0.3466.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3065 - accuracy: 0.8921 - val_loss: 0.3466 - val_accuracy: 0.8878\n",
      "Epoch 308/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3499 - accuracy: 0.8815\n",
      "Epoch 00308: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3471 - accuracy: 0.8816 - val_loss: 0.4495 - val_accuracy: 0.8439\n",
      "Epoch 309/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8898\n",
      "Epoch 00309: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3104 - accuracy: 0.8898 - val_loss: 0.5130 - val_accuracy: 0.8390\n",
      "Epoch 310/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3025 - accuracy: 0.8866\n",
      "Epoch 00310: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2981 - accuracy: 0.8875 - val_loss: 0.3494 - val_accuracy: 0.8829\n",
      "Epoch 311/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3026 - accuracy: 0.8840\n",
      "Epoch 00311: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3013 - accuracy: 0.8851 - val_loss: 0.3867 - val_accuracy: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2878 - accuracy: 0.8960\n",
      "Epoch 00312: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2898 - accuracy: 0.8950 - val_loss: 0.4473 - val_accuracy: 0.8537\n",
      "Epoch 313/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8857\n",
      "Epoch 00313: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3168 - accuracy: 0.8857 - val_loss: 0.3937 - val_accuracy: 0.8634\n",
      "Epoch 314/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8880\n",
      "Epoch 00314: val_loss did not improve from 0.34660\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3066 - accuracy: 0.8880 - val_loss: 0.3925 - val_accuracy: 0.8732\n",
      "Epoch 315/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3174 - accuracy: 0.8916\n",
      "Epoch 00315: val_loss improved from 0.34660 to 0.33276, saving model to ./data/cvision/model\\model_9-6ep315-vl0.3328.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3186 - accuracy: 0.8904 - val_loss: 0.3328 - val_accuracy: 0.9073\n",
      "Epoch 316/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2811 - accuracy: 0.9030\n",
      "Epoch 00316: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2817 - accuracy: 0.9020 - val_loss: 0.3978 - val_accuracy: 0.8585\n",
      "Epoch 317/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.8875\n",
      "Epoch 00317: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2952 - accuracy: 0.8875 - val_loss: 0.4100 - val_accuracy: 0.8634\n",
      "Epoch 318/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8968\n",
      "Epoch 00318: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3033 - accuracy: 0.8968 - val_loss: 0.4090 - val_accuracy: 0.8488\n",
      "Epoch 319/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2987 - accuracy: 0.8942\n",
      "Epoch 00319: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2997 - accuracy: 0.8945 - val_loss: 0.4472 - val_accuracy: 0.8537\n",
      "Epoch 320/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8828\n",
      "Epoch 00320: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3248 - accuracy: 0.8828 - val_loss: 0.4415 - val_accuracy: 0.8780\n",
      "Epoch 321/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8845\n",
      "Epoch 00321: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3107 - accuracy: 0.8845 - val_loss: 0.3664 - val_accuracy: 0.8878\n",
      "Epoch 322/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3134 - accuracy: 0.8897\n",
      "Epoch 00322: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3139 - accuracy: 0.8892 - val_loss: 0.4512 - val_accuracy: 0.8634\n",
      "Epoch 323/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2919 - accuracy: 0.9036\n",
      "Epoch 00323: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2957 - accuracy: 0.9003 - val_loss: 0.3940 - val_accuracy: 0.8732\n",
      "Epoch 324/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8886\n",
      "Epoch 00324: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3146 - accuracy: 0.8886 - val_loss: 0.4021 - val_accuracy: 0.8585\n",
      "Epoch 325/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8892\n",
      "Epoch 00325: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3191 - accuracy: 0.8892 - val_loss: 0.3577 - val_accuracy: 0.8683\n",
      "Epoch 326/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.8933\n",
      "Epoch 00326: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2993 - accuracy: 0.8933 - val_loss: 0.4306 - val_accuracy: 0.8683\n",
      "Epoch 327/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.8939\n",
      "Epoch 00327: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3115 - accuracy: 0.8939 - val_loss: 0.4470 - val_accuracy: 0.8585\n",
      "Epoch 328/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3051 - accuracy: 0.8916\n",
      "Epoch 00328: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3071 - accuracy: 0.8915 - val_loss: 0.3560 - val_accuracy: 0.8732\n",
      "Epoch 329/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2688 - accuracy: 0.9118\n",
      "Epoch 00329: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2788 - accuracy: 0.9096 - val_loss: 0.4143 - val_accuracy: 0.8683\n",
      "Epoch 330/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8915\n",
      "Epoch 00330: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3046 - accuracy: 0.8915 - val_loss: 0.4475 - val_accuracy: 0.8537\n",
      "Epoch 331/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8892\n",
      "Epoch 00331: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3041 - accuracy: 0.8892 - val_loss: 0.4689 - val_accuracy: 0.8732\n",
      "Epoch 332/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2786 - accuracy: 0.8973\n",
      "Epoch 00332: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2817 - accuracy: 0.8991 - val_loss: 0.4545 - val_accuracy: 0.8537\n",
      "Epoch 333/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.8886\n",
      "Epoch 00333: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3164 - accuracy: 0.8886 - val_loss: 0.4370 - val_accuracy: 0.8634\n",
      "Epoch 334/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8924\n",
      "Epoch 00334: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3107 - accuracy: 0.8929 - val_loss: 0.4078 - val_accuracy: 0.8732\n",
      "Epoch 335/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2974 - accuracy: 0.8935\n",
      "Epoch 00335: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2967 - accuracy: 0.8950 - val_loss: 0.4649 - val_accuracy: 0.8390\n",
      "Epoch 336/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9055\n",
      "Epoch 00336: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2790 - accuracy: 0.9055 - val_loss: 0.4380 - val_accuracy: 0.8732\n",
      "Epoch 337/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9003\n",
      "Epoch 00337: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2775 - accuracy: 0.9003 - val_loss: 0.3939 - val_accuracy: 0.8683\n",
      "Epoch 338/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.8985\n",
      "Epoch 00338: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2862 - accuracy: 0.8985 - val_loss: 0.3400 - val_accuracy: 0.8829\n",
      "Epoch 339/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2682 - accuracy: 0.9074\n",
      "Epoch 00339: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2653 - accuracy: 0.9073 - val_loss: 0.3879 - val_accuracy: 0.8780\n",
      "Epoch 340/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.8898\n",
      "Epoch 00340: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3080 - accuracy: 0.8898 - val_loss: 0.4134 - val_accuracy: 0.8439\n",
      "Epoch 341/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8851\n",
      "Epoch 00341: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3199 - accuracy: 0.8851 - val_loss: 0.4280 - val_accuracy: 0.8585\n",
      "Epoch 342/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3114 - accuracy: 0.8997\n",
      "Epoch 00342: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3114 - accuracy: 0.8997 - val_loss: 0.4060 - val_accuracy: 0.8732\n",
      "Epoch 343/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.90 - ETA: 0s - loss: 0.2965 - accuracy: 0.9003\n",
      "Epoch 00343: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2965 - accuracy: 0.9003 - val_loss: 0.4605 - val_accuracy: 0.8683\n",
      "Epoch 344/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2748 - accuracy: 0.8998\n",
      "Epoch 00344: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2720 - accuracy: 0.9009 - val_loss: 0.4543 - val_accuracy: 0.8683\n",
      "Epoch 345/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.8997 ETA: 0s - loss: 0.3246 - accura\n",
      "Epoch 00345: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2941 - accuracy: 0.8997 - val_loss: 0.4116 - val_accuracy: 0.8780\n",
      "Epoch 346/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8950\n",
      "Epoch 00346: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2883 - accuracy: 0.8950 - val_loss: 0.4237 - val_accuracy: 0.8634\n",
      "Epoch 347/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2980 - accuracy: 0.9049\n",
      "Epoch 00347: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2983 - accuracy: 0.9032 - val_loss: 0.4548 - val_accuracy: 0.8732\n",
      "Epoch 348/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2687 - accuracy: 0.9062\n",
      "Epoch 00348: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2781 - accuracy: 0.9051 - val_loss: 0.4869 - val_accuracy: 0.8537\n",
      "Epoch 349/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.8921\n",
      "Epoch 00349: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3172 - accuracy: 0.8921 - val_loss: 0.4517 - val_accuracy: 0.8634\n",
      "Epoch 350/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8997\n",
      "Epoch 00350: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2982 - accuracy: 0.8997 - val_loss: 0.4434 - val_accuracy: 0.8585\n",
      "Epoch 351/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.89 - ETA: 0s - loss: 0.2819 - accuracy: 0.9026\n",
      "Epoch 00351: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2819 - accuracy: 0.9026 - val_loss: 0.4262 - val_accuracy: 0.8829\n",
      "Epoch 352/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2937 - accuracy: 0.8948\n",
      "Epoch 00352: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2877 - accuracy: 0.8974 - val_loss: 0.4278 - val_accuracy: 0.8634\n",
      "Epoch 353/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.8968\n",
      "Epoch 00353: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2896 - accuracy: 0.8968 - val_loss: 0.4585 - val_accuracy: 0.8585\n",
      "Epoch 354/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2676 - accuracy: 0.9137\n",
      "Epoch 00354: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2674 - accuracy: 0.9143 - val_loss: 0.4160 - val_accuracy: 0.8878\n",
      "Epoch 355/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9003\n",
      "Epoch 00355: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2861 - accuracy: 0.9003 - val_loss: 0.4182 - val_accuracy: 0.8683\n",
      "Epoch 356/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2877 - accuracy: 0.9036\n",
      "Epoch 00356: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2926 - accuracy: 0.9009 - val_loss: 0.4734 - val_accuracy: 0.8537\n",
      "Epoch 357/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9055\n",
      "Epoch 00357: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2545 - accuracy: 0.9055 - val_loss: 0.5119 - val_accuracy: 0.8390\n",
      "Epoch 358/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2987 - accuracy: 0.8922\n",
      "Epoch 00358: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3024 - accuracy: 0.8927 - val_loss: 0.3963 - val_accuracy: 0.8829\n",
      "Epoch 359/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2817 - accuracy: 0.9017\n",
      "Epoch 00359: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2839 - accuracy: 0.9009 - val_loss: 0.4165 - val_accuracy: 0.8829\n",
      "Epoch 360/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3095 - accuracy: 0.8904\n",
      "Epoch 00360: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3066 - accuracy: 0.8910 - val_loss: 0.4678 - val_accuracy: 0.8585\n",
      "Epoch 361/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.9044\n",
      "Epoch 00361: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3003 - accuracy: 0.9044 - val_loss: 0.4160 - val_accuracy: 0.8537\n",
      "Epoch 362/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9009\n",
      "Epoch 00362: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2820 - accuracy: 0.9009 - val_loss: 0.3879 - val_accuracy: 0.8585\n",
      "Epoch 363/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2728 - accuracy: 0.8996\n",
      "Epoch 00363: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2730 - accuracy: 0.8984 - val_loss: 0.4540 - val_accuracy: 0.8634\n",
      "Epoch 364/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.8933\n",
      "Epoch 00364: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3043 - accuracy: 0.8933 - val_loss: 0.4024 - val_accuracy: 0.8829\n",
      "Epoch 365/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.8921\n",
      "Epoch 00365: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3272 - accuracy: 0.8921 - val_loss: 0.4200 - val_accuracy: 0.8683\n",
      "Epoch 366/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2899 - accuracy: 0.9004\n",
      "Epoch 00366: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2858 - accuracy: 0.9003 - val_loss: 0.5104 - val_accuracy: 0.8488\n",
      "Epoch 367/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9032\n",
      "Epoch 00367: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2847 - accuracy: 0.9032 - val_loss: 0.5254 - val_accuracy: 0.8488\n",
      "Epoch 368/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.8974\n",
      "Epoch 00368: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2789 - accuracy: 0.8974 - val_loss: 0.4268 - val_accuracy: 0.8683\n",
      "Epoch 369/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9114\n",
      "Epoch 00369: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2554 - accuracy: 0.9114 - val_loss: 0.4603 - val_accuracy: 0.8488\n",
      "Epoch 370/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2864 - accuracy: 0.9055\n",
      "Epoch 00370: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2872 - accuracy: 0.9032 - val_loss: 0.4844 - val_accuracy: 0.8488\n",
      "Epoch 371/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3021 - accuracy: 0.8922\n",
      "Epoch 00371: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2985 - accuracy: 0.8933 - val_loss: 0.4524 - val_accuracy: 0.8683\n",
      "Epoch 372/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.8956\n",
      "Epoch 00372: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2858 - accuracy: 0.8956 - val_loss: 0.4366 - val_accuracy: 0.8488\n",
      "Epoch 373/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3035 - accuracy: 0.8967\n",
      "Epoch 00373: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3019 - accuracy: 0.8985 - val_loss: 0.4392 - val_accuracy: 0.8780\n",
      "Epoch 374/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2603 - accuracy: 0.9055\n",
      "Epoch 00374: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2715 - accuracy: 0.9026 - val_loss: 0.4898 - val_accuracy: 0.8585\n",
      "Epoch 375/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9160\n",
      "Epoch 00375: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2617 - accuracy: 0.9160 - val_loss: 0.4420 - val_accuracy: 0.8585\n",
      "Epoch 376/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9120\n",
      "Epoch 00376: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2489 - accuracy: 0.9120 - val_loss: 0.3794 - val_accuracy: 0.8634\n",
      "Epoch 377/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2643 - accuracy: 0.9061\n",
      "Epoch 00377: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2735 - accuracy: 0.9050 - val_loss: 0.3941 - val_accuracy: 0.8585\n",
      "Epoch 378/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2654 - accuracy: 0.9074\n",
      "Epoch 00378: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2627 - accuracy: 0.9061 - val_loss: 0.4653 - val_accuracy: 0.8390\n",
      "Epoch 379/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2563 - accuracy: 0.9081\n",
      "Epoch 00379: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2583 - accuracy: 0.9068 - val_loss: 0.3995 - val_accuracy: 0.8683\n",
      "Epoch 380/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2443 - accuracy: 0.9149\n",
      "Epoch 00380: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2406 - accuracy: 0.9160 - val_loss: 0.3798 - val_accuracy: 0.8878\n",
      "Epoch 381/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.90 - ETA: 0s - loss: 0.2691 - accuracy: 0.9032\n",
      "Epoch 00381: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2691 - accuracy: 0.9032 - val_loss: 0.4091 - val_accuracy: 0.8634\n",
      "Epoch 382/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9026\n",
      "Epoch 00382: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3024 - accuracy: 0.9026 - val_loss: 0.5189 - val_accuracy: 0.8439\n",
      "Epoch 383/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9038\n",
      "Epoch 00383: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2779 - accuracy: 0.9038 - val_loss: 0.3857 - val_accuracy: 0.8732\n",
      "Epoch 384/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8945\n",
      "Epoch 00384: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2948 - accuracy: 0.8945 - val_loss: 0.3825 - val_accuracy: 0.8829\n",
      "Epoch 385/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2985 - accuracy: 0.8904\n",
      "Epoch 00385: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3011 - accuracy: 0.8886 - val_loss: 0.4360 - val_accuracy: 0.8683\n",
      "Epoch 386/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2782 - accuracy: 0.9004\n",
      "Epoch 00386: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2721 - accuracy: 0.9032 - val_loss: 0.4012 - val_accuracy: 0.8927\n",
      "Epoch 387/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9125\n",
      "Epoch 00387: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2600 - accuracy: 0.9125 - val_loss: 0.3930 - val_accuracy: 0.8829\n",
      "Epoch 388/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2348 - accuracy: 0.9200\n",
      "Epoch 00388: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2427 - accuracy: 0.9160 - val_loss: 0.3832 - val_accuracy: 0.8878\n",
      "Epoch 389/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9137\n",
      "Epoch 00389: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2430 - accuracy: 0.9137 - val_loss: 0.3770 - val_accuracy: 0.8927\n",
      "Epoch 390/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2571 - accuracy: 0.9153\n",
      "Epoch 00390: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2528 - accuracy: 0.9174 - val_loss: 0.4256 - val_accuracy: 0.8732\n",
      "Epoch 391/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9085\n",
      "Epoch 00391: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2637 - accuracy: 0.9085 - val_loss: 0.4027 - val_accuracy: 0.8878\n",
      "Epoch 392/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9061\n",
      "Epoch 00392: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2601 - accuracy: 0.9061 - val_loss: 0.4109 - val_accuracy: 0.8732\n",
      "Epoch 393/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2609 - accuracy: 0.9168\n",
      "Epoch 00393: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2594 - accuracy: 0.9166 - val_loss: 0.3941 - val_accuracy: 0.8780\n",
      "Epoch 394/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2575 - accuracy: 0.9080\n",
      "Epoch 00394: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2599 - accuracy: 0.9061 - val_loss: 0.3734 - val_accuracy: 0.8829\n",
      "Epoch 395/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2679 - accuracy: 0.8992\n",
      "Epoch 00395: val_loss did not improve from 0.33276\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2705 - accuracy: 0.8985 - val_loss: 0.3483 - val_accuracy: 0.8829\n",
      "Epoch 396/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9114\n",
      "Epoch 00396: val_loss improved from 0.33276 to 0.33210, saving model to ./data/cvision/model\\model_9-6ep396-vl0.3321.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2640 - accuracy: 0.9114 - val_loss: 0.3321 - val_accuracy: 0.8927\n",
      "Epoch 397/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2434 - accuracy: 0.9137\n",
      "Epoch 00397: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2504 - accuracy: 0.9096 - val_loss: 0.3401 - val_accuracy: 0.8683\n",
      "Epoch 398/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9085\n",
      "Epoch 00398: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2526 - accuracy: 0.9085 - val_loss: 0.3533 - val_accuracy: 0.9024\n",
      "Epoch 399/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2218 - accuracy: 0.9187\n",
      "Epoch 00399: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2224 - accuracy: 0.9190 - val_loss: 0.3333 - val_accuracy: 0.8976\n",
      "Epoch 400/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9032\n",
      "Epoch 00400: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2613 - accuracy: 0.9032 - val_loss: 0.4279 - val_accuracy: 0.8683\n",
      "Epoch 401/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2724 - accuracy: 0.8973 ETA: 0s - loss: 0.2544 - accuracy\n",
      "Epoch 00401: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2718 - accuracy: 0.8985 - val_loss: 0.3804 - val_accuracy: 0.8732\n",
      "Epoch 402/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9166\n",
      "Epoch 00402: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2384 - accuracy: 0.9166 - val_loss: 0.3612 - val_accuracy: 0.8732\n",
      "Epoch 403/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.8991\n",
      "Epoch 00403: val_loss did not improve from 0.33210\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2584 - accuracy: 0.8991 - val_loss: 0.3921 - val_accuracy: 0.8634\n",
      "Epoch 404/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2539 - accuracy: 0.9105\n",
      "Epoch 00404: val_loss improved from 0.33210 to 0.31901, saving model to ./data/cvision/model\\model_9-6ep404-vl0.3190.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2594 - accuracy: 0.9079 - val_loss: 0.3190 - val_accuracy: 0.8829\n",
      "Epoch 405/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2553 - accuracy: 0.9069\n",
      "Epoch 00405: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2539 - accuracy: 0.9090 - val_loss: 0.3871 - val_accuracy: 0.8780\n",
      "Epoch 406/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2109 - accuracy: 0.9267\n",
      "Epoch 00406: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2086 - accuracy: 0.9277 - val_loss: 0.3915 - val_accuracy: 0.8780\n",
      "Epoch 407/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9055\n",
      "Epoch 00407: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2566 - accuracy: 0.9055 - val_loss: 0.4157 - val_accuracy: 0.8585\n",
      "Epoch 408/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.8945\n",
      "Epoch 00408: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2795 - accuracy: 0.8945 - val_loss: 0.4431 - val_accuracy: 0.8537\n",
      "Epoch 409/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2437 - accuracy: 0.9143\n",
      "Epoch 00409: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2437 - accuracy: 0.9143 - val_loss: 0.5492 - val_accuracy: 0.8488\n",
      "Epoch 410/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2759 - accuracy: 0.9008\n",
      "Epoch 00410: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2806 - accuracy: 0.8980 - val_loss: 0.3856 - val_accuracy: 0.8780\n",
      "Epoch 411/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2836 - accuracy: 0.9042\n",
      "Epoch 00411: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2840 - accuracy: 0.9050 - val_loss: 0.3595 - val_accuracy: 0.8634\n",
      "Epoch 412/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2749 - accuracy: 0.9014\n",
      "Epoch 00412: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2721 - accuracy: 0.9032 - val_loss: 0.3744 - val_accuracy: 0.8829\n",
      "Epoch 413/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2305 - accuracy: 0.9225\n",
      "Epoch 00413: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2376 - accuracy: 0.9201 - val_loss: 0.3245 - val_accuracy: 0.8878\n",
      "Epoch 414/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2142 - accuracy: 0.9207\n",
      "Epoch 00414: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2172 - accuracy: 0.9191 - val_loss: 0.3726 - val_accuracy: 0.8878\n",
      "Epoch 415/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9219\n",
      "Epoch 00415: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2214 - accuracy: 0.9219 - val_loss: 0.3754 - val_accuracy: 0.8878\n",
      "Epoch 416/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2631 - accuracy: 0.9183\n",
      "Epoch 00416: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2597 - accuracy: 0.9190 - val_loss: 0.4564 - val_accuracy: 0.8488\n",
      "Epoch 417/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9137\n",
      "Epoch 00417: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2501 - accuracy: 0.9137 - val_loss: 0.3783 - val_accuracy: 0.8829\n",
      "Epoch 418/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9015\n",
      "Epoch 00418: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2673 - accuracy: 0.9015 - val_loss: 0.3934 - val_accuracy: 0.8780\n",
      "Epoch 419/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9125\n",
      "Epoch 00419: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2446 - accuracy: 0.9125 - val_loss: 0.3668 - val_accuracy: 0.8829\n",
      "Epoch 420/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9038\n",
      "Epoch 00420: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2597 - accuracy: 0.9038 - val_loss: 0.5707 - val_accuracy: 0.8146\n",
      "Epoch 421/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2713 - accuracy: 0.9042\n",
      "Epoch 00421: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2667 - accuracy: 0.9050 - val_loss: 0.4253 - val_accuracy: 0.8683\n",
      "Epoch 422/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3074 - accuracy: 0.8948\n",
      "Epoch 00422: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3038 - accuracy: 0.8962 - val_loss: 0.3506 - val_accuracy: 0.8780\n",
      "Epoch 423/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2715 - accuracy: 0.9056\n",
      "Epoch 00423: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2682 - accuracy: 0.9062 - val_loss: 0.3687 - val_accuracy: 0.8634\n",
      "Epoch 424/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9149\n",
      "Epoch 00424: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2413 - accuracy: 0.9149 - val_loss: 0.4063 - val_accuracy: 0.8780\n",
      "Epoch 425/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2227 - accuracy: 0.9162\n",
      "Epoch 00425: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2248 - accuracy: 0.9166 - val_loss: 0.3907 - val_accuracy: 0.8683\n",
      "Epoch 426/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2537 - accuracy: 0.9099\n",
      "Epoch 00426: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2585 - accuracy: 0.9090 - val_loss: 0.4627 - val_accuracy: 0.8537\n",
      "Epoch 427/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2223 - accuracy: 0.9288\n",
      "Epoch 00427: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2195 - accuracy: 0.9289 - val_loss: 0.4284 - val_accuracy: 0.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9073\n",
      "Epoch 00428: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2558 - accuracy: 0.9073 - val_loss: 0.4198 - val_accuracy: 0.8732\n",
      "Epoch 429/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2464 - accuracy: 0.9162\n",
      "Epoch 00429: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2587 - accuracy: 0.9125 - val_loss: 0.4162 - val_accuracy: 0.8780\n",
      "Epoch 430/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2625 - accuracy: 0.9118\n",
      "Epoch 00430: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2624 - accuracy: 0.9125 - val_loss: 0.4106 - val_accuracy: 0.8927\n",
      "Epoch 431/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9172\n",
      "Epoch 00431: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2214 - accuracy: 0.9172 - val_loss: 0.4737 - val_accuracy: 0.8488\n",
      "Epoch 432/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2350 - accuracy: 0.9230\n",
      "Epoch 00432: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2350 - accuracy: 0.9230 - val_loss: 0.4774 - val_accuracy: 0.8683\n",
      "Epoch 433/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2223 - accuracy: 0.9219\n",
      "Epoch 00433: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2250 - accuracy: 0.9196 - val_loss: 0.3841 - val_accuracy: 0.8634\n",
      "Epoch 434/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2195 - accuracy: 0.9189\n",
      "Epoch 00434: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2256 - accuracy: 0.9163 - val_loss: 0.3989 - val_accuracy: 0.8829\n",
      "Epoch 435/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2327 - accuracy: 0.9162\n",
      "Epoch 00435: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2362 - accuracy: 0.9143 - val_loss: 0.4511 - val_accuracy: 0.8537\n",
      "Epoch 436/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9044\n",
      "Epoch 00436: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2601 - accuracy: 0.9044 - val_loss: 0.4282 - val_accuracy: 0.8585\n",
      "Epoch 437/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2372 - accuracy: 0.9137\n",
      "Epoch 00437: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2418 - accuracy: 0.9143 - val_loss: 0.4576 - val_accuracy: 0.8390\n",
      "Epoch 438/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2540 - accuracy: 0.9143\n",
      "Epoch 00438: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2442 - accuracy: 0.9178 - val_loss: 0.3842 - val_accuracy: 0.8732\n",
      "Epoch 439/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.9137\n",
      "Epoch 00439: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2367 - accuracy: 0.9137 - val_loss: 0.4756 - val_accuracy: 0.8537\n",
      "Epoch 440/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9079\n",
      "Epoch 00440: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2640 - accuracy: 0.9079 - val_loss: 0.4114 - val_accuracy: 0.8634\n",
      "Epoch 441/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9079\n",
      "Epoch 00441: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2387 - accuracy: 0.9079 - val_loss: 0.3732 - val_accuracy: 0.8878\n",
      "Epoch 442/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2543 - accuracy: 0.9212\n",
      "Epoch 00442: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2494 - accuracy: 0.9236 - val_loss: 0.4471 - val_accuracy: 0.8732\n",
      "Epoch 443/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2526 - accuracy: 0.9093\n",
      "Epoch 00443: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2482 - accuracy: 0.9096 - val_loss: 0.3672 - val_accuracy: 0.8829\n",
      "Epoch 444/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9166\n",
      "Epoch 00444: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2287 - accuracy: 0.9166 - val_loss: 0.4753 - val_accuracy: 0.8732\n",
      "Epoch 445/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2622 - accuracy: 0.9124\n",
      "Epoch 00445: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2604 - accuracy: 0.9125 - val_loss: 0.3950 - val_accuracy: 0.8927\n",
      "Epoch 446/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2667 - accuracy: 0.9026 ETA: 0s - loss: 0.2419 - accuracy\n",
      "Epoch 00446: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2640 - accuracy: 0.9046 - val_loss: 0.4203 - val_accuracy: 0.8683\n",
      "Epoch 447/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2137 - accuracy: 0.9175\n",
      "Epoch 00447: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2249 - accuracy: 0.9155 - val_loss: 0.4104 - val_accuracy: 0.8732\n",
      "Epoch 448/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2490 - accuracy: 0.9143\n",
      "Epoch 00448: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2437 - accuracy: 0.9155 - val_loss: 0.4583 - val_accuracy: 0.8488\n",
      "Epoch 449/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.1956 - accuracy: 0.9332\n",
      "Epoch 00449: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.1963 - accuracy: 0.9335 - val_loss: 0.4602 - val_accuracy: 0.8634\n",
      "Epoch 450/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2305 - accuracy: 0.9155\n",
      "Epoch 00450: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2305 - accuracy: 0.9155 - val_loss: 0.3685 - val_accuracy: 0.8927\n",
      "Epoch 451/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2465 - accuracy: 0.9207\n",
      "Epoch 00451: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2481 - accuracy: 0.9196 - val_loss: 0.4022 - val_accuracy: 0.8732\n",
      "Epoch 452/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2199 - accuracy: 0.9254\n",
      "Epoch 00452: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2199 - accuracy: 0.9254 - val_loss: 0.3938 - val_accuracy: 0.8683\n",
      "Epoch 453/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9248\n",
      "Epoch 00453: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2025 - accuracy: 0.9248 - val_loss: 0.4145 - val_accuracy: 0.8683\n",
      "Epoch 454/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9166\n",
      "Epoch 00454: val_loss did not improve from 0.31901\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2459 - accuracy: 0.9166 - val_loss: 0.3667 - val_accuracy: 0.8927\n",
      "Epoch 455/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9184\n",
      "Epoch 00455: val_loss improved from 0.31901 to 0.31577, saving model to ./data/cvision/model\\model_9-6ep455-vl0.3158.hdf5\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2290 - accuracy: 0.9184 - val_loss: 0.3158 - val_accuracy: 0.8878\n",
      "Epoch 456/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9294\n",
      "Epoch 00456: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2066 - accuracy: 0.9294 - val_loss: 0.3383 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2008 - accuracy: 0.9231\n",
      "Epoch 00457: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.1984 - accuracy: 0.9242 - val_loss: 0.4080 - val_accuracy: 0.8829\n",
      "Epoch 458/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9230\n",
      "Epoch 00458: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2306 - accuracy: 0.9230 - val_loss: 0.4130 - val_accuracy: 0.8683\n",
      "Epoch 459/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9137\n",
      "Epoch 00459: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2521 - accuracy: 0.9137 - val_loss: 0.4006 - val_accuracy: 0.8829\n",
      "Epoch 460/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9364\n",
      "Epoch 00460: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2011 - accuracy: 0.9364 - val_loss: 0.3846 - val_accuracy: 0.8927\n",
      "Epoch 461/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2245 - accuracy: 0.9213\n",
      "Epoch 00461: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2209 - accuracy: 0.9230 - val_loss: 0.3596 - val_accuracy: 0.9024\n",
      "Epoch 462/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9149\n",
      "Epoch 00462: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2369 - accuracy: 0.9149 - val_loss: 0.3813 - val_accuracy: 0.8927\n",
      "Epoch 463/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9219 ETA: 0s - loss: 0.2186 - accuracy: 0.91\n",
      "Epoch 00463: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2122 - accuracy: 0.9219 - val_loss: 0.4316 - val_accuracy: 0.8634\n",
      "Epoch 464/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2030 - accuracy: 0.9345\n",
      "Epoch 00464: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2050 - accuracy: 0.9306 - val_loss: 0.4314 - val_accuracy: 0.8585\n",
      "Epoch 465/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.9254\n",
      "Epoch 00465: val_loss did not improve from 0.31577\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2398 - accuracy: 0.9254 - val_loss: 0.4942 - val_accuracy: 0.8439\n",
      "Epoch 00465: early stopping\n",
      "CNN: Epochs=3000, Train accuracy=0.93644, Validation accuracy=0.90732\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[es, cp]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1598336814187,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "mDdwZDDFkv5W",
    "outputId": "30b0e4a2-9738-4989-eb06-1c54ecef6b41"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUVfrHP3fu9Ex6hSQEEkKoIkUElCYIiIqiou7acO11Xbv7c13L2uvaFfu6KGBHsaEiRURAqtQUSALpZVKmz9zfH2fmzgxJAE2Crt7P8/CQueXcM2dm7ve+5bxHUhQFDQ0NDQ0NjV8P3a/dAQ0NDQ0NjT86mhhraGhoaGj8ymhirKGhoaGh8SujibGGhoaGhsavjCbGGhoaGhoavzKaGGtoaGhoaPzKHFSMJUl6RZKkakmStnSwX5Ik6UlJkgolSdokSdLwru+mhoaGhobG75dDsYxfA6YfYP8JQH7w36XAc53vloaGhoaGxh+Hg4qxoijLgPoDHHIK8IYi+B5IkCSpR1d1UENDQ0ND4/dOV8SMM4GyiNflwW0aGhoaGhoah4C+C9qQ2tnWbo1NSZIuRbiysVgsI7Kzs7vg8oJAIIBO1735aK6Ai2pfNQB6SY9P8ZFpzERG/kXtVbQGcPshxiDR6lVIMkvEGdsbzl+fwzG+f1S0se1etPHtPrSx/fns3LmzVlGU1P23d4UYlwORqpoF7GvvQEVRXgReBBg5cqSydu3aLri8YOnSpUycOLHL2muPTTWbOGfxOQAUJBawo2EHX5z+BT1sv8wrX1TTwr5GJyNzkhhwx2dcNyWf66b068oudxmHY3z/qGhj271o49t9aGP785EkaU9727vikeYj4PxgVvVowK4oSkUXtPubw6K3qH/bjDYAXH7XL24vL9XGuPxULEaZWLOeRoe3033U0NDQ0Pjf46CWsSRJbwETgRRJksqBfwIGAEVRngcWAzOAQsABXNhdnf21sRqs6t82gxBjj9/TJW0nWA3YnZoYa2hoaPwROagYK4ryp4PsV4CruqxHv2EiLeN4UzzQOcs4kgSLkUZH1wi7hoaGhsb/Flrk/Wdg1Yct46k5U4GutYwbNctYQ0ND4w9JVyRw/WEw683cPfZuRmaMpN4lpl67fF1jGcdbDOxtcHZJWxoaGhoa/1toYvwzmZU/CwCH1wFolrGGhoaGRufR3NS/EKNsBLo+ZhwItDtFW0NDQ0Pjd4wmxr8Qs2wGwO13d0l7CVYDAQVaPD4+WL+XRz7f0SXtamhoaGj89tHc1L+QkGXcVWIcbzEAYHd4uW7+BgBOHZZJ3zRbl7SvoaGhofHbRbOMfyFmfdAy9nWVZSzEffNeO+lxJgDeWLW7S9rW0NDQ0Phto1nGv5CutozH5CVTkB7LDQs2ogRLe/9Y2tAlbWtoaGho/LbRLONfiEFnQJbkLhNjm0nPDVP74fT6cXkDAOyudSBqqmhoaGho/J7RxLgTGGVjl4kxQE5yjPp3bkoMLW4fda1aVS4NDQ2N3zuaGHcCs2zuUjHOTgqX2zyqdxIAu2tbu6x9DQ0NDY3fJpoYd4KutoytxnAI/6g+QozX7WnQ5h5raGhodDdla6B62692eU2MO4FZb+6ybOr9GdYrAYD7P93OC8uKu+UaGhoaGr97agthyV0QCBz4uJenwLOjD0+f2kET404QaRnftvw23t/1fpe13TPewnmjcwCY98MeLZFLQ0Pjt0/AD11xr6ovgcotnW8HYMN/YcVjUF8Uvd3ZCMsfBa8T/BGliL0uce0XJ0HD7q7pwyGgiXEniIwZf1z8MXd8d0en27zn1MH0z4jFYpS559TBPDJ7KGX1Tn4sbex02xoaGhrdhscBj/SDTQs639ant8DCOZ1vB6Dqp+D/W2D3Slj/X9HXNS/BV3fDV/eAvSx8/L4fYeW/xf+L/to1DxeHgCbGnaCrY8YA543O4bPrxquvRwUTuYprWrr0OhoaGhpdSs12cNRC2er293sc8MNcaKk5eFvVW6G+GHzB2STV2+HNM8T/HbHmJXhlelvxDIlx0dfw3zPgwyvh47/B7uVi+/fPCCs4ROES2Pm5+Lt87WGzjjUx7gQhy9gX8HXbNUJlMptc3XcNDQ0NjShqd4E/eM/x73fv8bTCR9eCfa8QvlAsNpT8VFfYtr0XJ8KDvWHxjfDKVGit7fjaLruwVBW/EEJFgU9vhsIv4Y1ToC7oblYUIbDOoNdw28dQukqIeEiQnY3QVC7+/vEN8Lmhz3jY9DYUL4WRF8HwC8AVbCOxj7CK3XY47wO4ei0k9Tn0cesEmhh3gpBl7PR13zrENrPIsG7SllfU0Pjj0FwJRd90vp3WOnh2TNjSA/j4euGa3R+fG94+B16dAU+PhK/ugs3vwEO50eK58W348XVY/ghZ5R/CU8OF+FVvFfvri4Wgbl8sXrfUwL714HfD8XdDYyl88Y+213fUw2snwQO9wtvqCuGn96DkWxh1KQS88OoJUPgVfPck/GcWvP1nYUHvWy/OeWo4vHmaeO8vToi+xoSbYdp94dcj5sDkiPDi+Jsg4IOMIyB3IsT1OMgAdx1aOcxOELKMu1OMZZ1ErEnPT/vsPPjZdq6bko9JL3fb9TQ0NH4GiiJEJCa5a9td/phwu96yG8xxh3ZOw25Y9xpMuh3k4K29+BshkvPOhL9tBdkojrEmwXG3gySFz//yDtj+MciiNj4b5olz3XbY+ZnY3lIJu1eI/c4GEhtKoaFECGzIMraXw+oX4Jt74a8bw5byBYuEVdpaK4RU8YvkqbP+I/aveSnsOg7x9p/E/5kjYNr9QjwXzoF3LwZ3M6QNgj0r4bNbwtYtCIv5jZliTLJGwTF/FVbz+JtAJwurNyUf4rPE8bYM8LTAoFki4Wvc9dFjcxjQxLgTGGUjLp+rW8UYIM5iYMm2apZsq8bp8XPnzEHdej0NjT8qrm3b8FZWEjtp0sEPBvjpfXj/crhuM8SmR+/z+4Qo7v0RWqqg4IRD70jlJiFW5WugzwQhIAcThy/vgK0fQs9hMPAUsW3Pd+H9S+8XIqz4obUGandCaoHY53PDxrdgyJnCctz1hYitFi4R+7cvhh2LgYh4bMVGYlqbxN+LroXS78EUB+6m4LEIa7W+RPydMUT8P/RsIcab5ovXtYWQnCfEv/e4oNhHXGfM1TDmKjGW6YNg0t/DyV0n/xu+fQDWvhI9FrkThRs6tgdc9IUYuwEnhffn7ff5XvujsIiNVrhw8YHHuZvQ3NSdwKw34/F7DosYh5j3Qyl+rQiIhka3UDLrNMqvuDJ6qsuBKFst3K8VG6O3710H96QId+rcSfDW2e1n5VZvg5enwef/J15XbRXTg0JJR7u+hMf6w9dBt3JtIXxwlXDBhvD7hAiHEo3WvxneV7oK8o6DI86C9f8R8dCUoAAvuVPMv22tFZakyw5HnAm2VBh8OmSOFMf1HAY7PgEUGDEHb/+/oAz5E9QXY3YH3dfFS6HHkXDqs+J1yGW8cI5wd1uSwJIotqUNhJjUcB+fHgEfXi0s7CP/DNdtgnPeFVbwiAth2r0Q1zN8fN/jQW+G2J7CYj7hISG+/U+C67eJB6OjrxDH5k89NAvXGAPmeLzV1QRcroMf3w1olnEnMMpGXH4XLl/3fnhx5vDH5PEFqGxykZlgOcAZGhoaneKdC+GsN6O3FX8rBG/EBeFtkdNm0vqDpBOuzw1vAYqIXYZo2itEq3ipSH4qmC5EdO86IeqSTliMQ/8krEuA1c+J/5c/KoTa5xTnV2yEP78NcZmw4PygWAYpXCLitJUbhZt5yGwhxom9hWhljoAXJgjrdcdikA3CcjfFi/0ABrOwKOtLwNsKL4gZHoGRV1A4aRbWIwrIGbjfmE39F/Q8Eoyx4GmO3jfo1PDfkiTc1VvegyFnwOaFsOFN0BmgYAZYEiChF+RPaf+zMdlg8j+Fla/TCav6/A+jj4nLhLHXwJHntt9GOwTcbkpOnkncSSeR8Y/bD/m8rkKzjDuBRW/B5XPh8Dq69TqRljFAWX33Xk9D47dCw8KFVPxDJNgEHA52n3Muzp9+6voLNZZGvVR2fiMsx22Lwhu/uktk9XqDnjBFCYtx9VaYd7YQRkURmb/6/R6Yi76GD6+CkmVC4L68Qwhx+hBAEUJsTRHuYoAjzxHWaUo/8Xrnp0KIQViRr54gLOdIIT7mOlAC8NmtsPBCSB8MR18G8ZnCvZszFvQmuGwZXPk9ZB0FW94VpSCzRgphDqGTIaUv9BgqhBLwea0AODbtwCkPFcelBVU5dH5qv+j3ffU6OOnx6G2T/g9mvwqnvwSzXhDbcicIIT4Uxlwp3N0doZPFw0Fa/0NrD2hdsQK/3U7Tp5+i+A7/7BVNjDtBrCEWBYU6V93BD+4EcWbxQ8hPswFQqonx75KW5StoXrr01+7GbwN3C2z9iNblK2j67DOxqbgE57p1ONasEdNb1r560GbsH35I7Pz5uEtKwhvri6Hsh/Drom/giSHC8g3ib3XAisdh/rkiE7mlRsR+fS4hqu9dBvdng7NenPDT+1D9kxDXN08XFvQJD8JNRXDRl+KYUH/PehMu/Va4fwFmPhnuy0VfwBFnQ8GJQsAu+QpOfS76TY27Ec5ZKB4g5s0Wluh1W2DstSJb2JIEW94Rrt0/LxAu2P3RGyFtAAw7F+oKce/aQWOpEMLGd9/FtXVr9PHXb6V5xAs0Lf5U3eTqcwnrhj8Cly6F2/aG3cF5x4n/T3wM/jRfCPr+JOeJZCkQ7uX0ITDyL20OUxSF+nnz8FZWtm3jANgXLcK9a5f62r1rF/ZFi9Q26157DU9ZWdQ5TZ+LjHN/fT2OtWt/1vW6As1N3QlijbEAVDmquvU6cRbxMQ3JiqeopoVyTYx/X/h8tCxbRtmllwEwYPuvV6z+V8PZiPeLJ9DlH4s8cAp8/S9Y/Rz+feMINDfj/f5dPIWiWIS/tha+ewrWvSrcn5ZEUcLw2wfhqIvUDFnl/auouGMpVm8Au6me1BkD8Gafgv7NSfi9EobjrgBzvBBnILA8LHp+jw69OQCSDmXLB3iLdmIMJRW9/Wfxf1Ie1DcL4Yy0Tou+EhbqkeeIpKOYFEjIERWddAYxbUaShEi3VIn+JvaB3scKkTrtBQJuN/7aBgwZGSIWm3Ms2NLENJ/exwgL97h/iKzrkRdCQjZMDcaVT3xUFKuYdBuYYtsMta+hAUmnQ46PFy7sRX+lfGUinqZVKHnzqbzzTuJmzCD12msw5OQgSRKKKZHym+6KasdbU0dz1pHC0tabwjsm3CLe44CTVYH2VlXhKdmNZDJiGToUSRdhB5pscMWKdr8WnqIiqu6+h5ZvltJr7osdfn0UrxdvVTXGrEwUj4d9t/2d+FNm0vPeewEoOetsFIeDuGnT8NXVUf3Ag1Q/8CB9PvoQc79+BDweWr7+htjp02n+7DOcGzYSM/rw1qnWxLgThMS42lHdvdcxiY8pLdZMj3gLZQ3dmzCmcXixLF9B2fz5v3Y3fh6KIpJzBp8OA2d2fFxrrRCjA7WjiNhq0eP7QJpPwbLFSGtfBsBfISzawjnhGJ6vphZigpZL7S7IHiWEeMVj0FotrNjyNbj3NaF408Q5O1dhr1tGxZr5QA8knUJ/6zNRXfFt+gIQGdEBjwQxaZAzluo3FlO/fSl95/TF0PdIkWk8/kZhiTbtE7HLJXcK9/C+9cLinHJndOLQoFmw8gmxzWAW22RDeGrNteuj+rLvpptp/uIL+m/ZjKTXw4WfiOIaw86B3GAm8Pgb4dh2puAMPk3864Dya65Bn5hI1lNPCat55tMoi4VoVd55JwAtK1fS9OmnZD7+OHHTp9H6ww9RbUgmE76OrFXZEPWd8JTvpWTmTAIOYURkPf8csRMndti/SJxbDq0+dc2TT1I39yX6fvstgeYm8PnwlpWr+5XgtT179uBraFC3l8w8hczHHkWyWAi0tJBw2iwcq1bhq+5eA6s9NDd1J7AZhdu4xnEI5d06QSh52mzQkZ1koVhb4/h3ha5ZJOsknn8eAP4W8fk6N2xg5zHHRt08fi1avv2W4pmn4C/bCjU7RZLQ1g9gwXk4f/yB4lNn4Vn7mUjKKV8L+9bj/ep5dh07Fvenz7ffaG0hzTcexZ6JfQmUrkMJSCh+sD98jdg/4xH8jrZZzb6qSuESBuEe/v55kSUMIpO4+BtwN+GsNQKgMwTw+2Jw6gerbSgBCUXSs2fzaJobe8GMR/CljlX3+6c9LWKqfSdTv13EST2Dr4MzXoZbS8W8VUkSsViDRbikZzwsrN2Tn1QF0v7JJ+z+05/xH3UtZB8dXWAiEknC39xMyWmn49q6leYvvhDXLC1D8fspOWM2O8eNx+lMV9uuuv8Bqp966mfNh1UCAVw/bcW1cycg3LfFd76jxoLR6bCOHk3AbgdFwbFmDQ1vvUXZRReHG5FlTP0L8FaFBavy7nuovPvudq9Z/eADAPR8+GExJu9/QPGps9h7401U3PHPA/bXuVFkqeuTRFnghrfeYs+557U5rmXlSgDKr7iC3WeLucme8rI2x7mLilSRznr2WcxDhlB5z79o+vgTdHFxxIwejT4jA2/Fz3OLdwWaGHeCOKOYjB9pGXfH6koevyg3Z9TrOCYvhY1ljbyzrvwgZ2n8ryC53OhsNixDxDzM0FN5zZNP4a+rw7VpU6ev0fjOO7iLD7AUp6NexEebKsTrQAACARrmvYlz0ybKLrsc986duO+fAM8cJWr8BnF+PBf39u1U3HQtyjsXw8vHw4sTaXntLnxOmdpnnhQx2opN4XmvLjt8+wDNOxpwVJtokcJC6NiwGY66GEZdgt9vbtNVX2W5sEIBvn8WPrsFxRRHbeOxuBr11FYORbl+B87UWcjmAPo0E/7EoXjl3uisVvQ9RFUl/6A5OH4qxS5NgVGX4DvyGvUafmwQk4w3LVwn3usKJmQdSPwkKWp/3Qsv4ly/npqnnxfx4LHiGt6qampfeBElYlk/1/btuLZupWX5CiSrEEd3USF+ux3Xli346+pwrF2nHt+0eDH29zpeKc7+0Udtkt28+/ahOJ1491Wg+P1UP/Io7p07UZxOLCNGkPnoI8TPDFu1zV9/TdV992MZOhTbcSIWrE9KwtCzZ5Rl3DBvHg3z3mLvzTfj2bNHvJ+dO2lYuBDH2nXEnTiDuOnTRJuff457+3aaPv6YxgULRAz3lVdxbdtGzTPPEHCH6/07128AwN8ssrMr77obx9q1UccAyHHx4ppbtxJoEXX8fZVVKB5R21oyigczd2ER3r3loNNhG3csKZddir+hgabPPyfm6FFIRiP6jHS8VZoY/0+huqmdYTH2BDxdfh2jLD6mWLOBKybmMbxXAk8s2UlAm2/cLQQ8HlpXrYpO+mkHb3U1frv9Z7Xta2jAWx0d1pDcbiES6cJFGrrJBVqFhSwZorPpD4S7pKRNJqji81HxjztoPJArfNsi8W+JsFR8r56N9/FJVN59L3uvCltFntZw9TdPi4zScxT+n74GwFEh01hkFkI5/AJ0g08UxzX44bUTRRH/V0+AT28VJQ83L8TtFe+5qUlkvUo6BXezESbeSsDpRPG0YxlXV+NusYY3ZI/GNfIhaj4rpuSzNGq+2ourpJKWdZuxDh+KIzUPX0MD7sJCbFMmk3zxRaJfuWLai3ODsL58teGSj/4m8bk6t4WTgFxbt0UlEimKgmvHThRFwd/cjLdKfK6esjICLheunTvx1YsEr5Cl6y4uIeDxsPeG66l5/HFav1ulthmy2NyFheiCYuwpLMTfGP6OeYNJR367HV9NDb6qqnaTmxRFYd/Nt7D79DOitrsLg9WwvF58lZWqhQwQP3MmcSecgKlvXnisKyrQ2WxkPfsMyReL74GcmoIhPQPvvn0YduyMSnZq+mgRje++B0DphX+h8h934G9owNS3L5LBgC4+Pqo/uthYXJs2Uf3QQ5TMOo3ap56mcb5Y9al5yRLc28XCEP7G6FXrfMHfkGfPHhSvF19NO97JQICW5cvxt7SqouwuKsRTVo4hIwPJYMAyNJgR7vNhOfJIAAzpGfgqNTf1/xQ2g3BTR1rGHn/Xi/HlE/O4alIeZ47MQi/rOG9MDuUNTtaV/vruy98jjfMXUHrhX9h9xuw2T+CRlF1+OVX33dfh/nbPueRSCsdPwN/UpG7TBcXYkJEBgDd4IwjF2Pwth7Zil6+hgeKTZ2L/+OOo7f7mZlAUYVG2V8xiz6pwXeHKzbDlXXY9vJnCueJ77a0Jzxn1Jo+HGwtxNeopWpxG+Yo4fK1eZGMA64AcanemiSpOM5/EnyVim+5WC/g9YjoPqHNnlZT+eGrE+DYHM6Zt48bidsSiGGPb3IDV9+PwUvxxAm6/GC/OfRcfSVHHNC5ciL+mlrgz5uBJzMBbWoqvshJTXl/kBJE1HLLgfBUVIsFo927Vqg0EP5+QQOri4miYN4/CieHKTa0rVlByyinUvfQSVffdT+mFF+Krr6f4xJPYe93fKJl5Cv7aWnTx8fjq6vA3NVE8YwZ7zjkXZ9DCLbv4YrVN796wGIeu7y4sihoHT+iYovDavM4NG9qMUaCD74wn4ryW5cvxVVSorw1ZmQCY8vKQrFZsE0Rd5/Tb/w99cjLGbBHf1qekYOiRgeLxkPT446rbOHvuXIx989S+BZrD3xtjX5FRrU8UhT9M+X1JmjMHxe+nKfigoo7r6tX4GxupuOsuTAMGYJs8GX9jIwFP+N7qq6zE39xM0bTpVPzzTrzl7XsKy6+6moZ589TXju9X49q8GUN2tuhPaiqGTPG+Q8Js6JGBv77+gL/97kAT404QsowDStjV1NVLKgLYTHpumtZfrUk9dWAGZoOOxZsrDnLmb5fKu++m9rnnDn5gJ6mdO5eSM8+i4o5/UvNMdMJO7Ytz2X1u26IA3uANKtDaSuvK79rsBxF78+wqxFPaNi5V8Y87KP/b39o9zxVMSKl9Nvze97eMvZXh6wMEmtu/sTZ/9RVFJ52kxpR9lZXg8+Ep2iVit8GQib9B3Mz9Gxfj/PsRFI07CvdOYXEoTjt7zj8X+4LXRaPVW1EWtJ1iIskB9BY/XncM2FKpKh4CyLT8sJWmvUnIGVnYTjoTn92Nf7KIDYYERXG5qfwpm7LliXDmG6LBghn4zvhQPHCE3LqSRMykqShOJ02ffELhpOPafd8hnAU3UGE/m313/AvPfjfjxvffRzKZsE2YQMAWzig29c2LEOPd4bbWb6B5yRJsEyciWSz47UExLi9HFxuLsXdv9djik09mx4iRVN4tspdrHn8Cxw8/4Ckuxv7RRygeD46IhKfYyZPB76dluai77Nq8uc17CTgceEKW8bZtYUtu1y5VjPU9e0RZz6Exc27YiLukhKITT8JbWUndSy9ROudCtW3FG34AcxcWgV4khLauFksdhjwvhp6iypUuJoa8Tz8l65mnyVuyhPgThYdDTklBsljQp6YiJ4la3AFTOIvalN8XU9983IXCm6BPS4sYdyHGcjD2q09NRWe1ojgcNC9ZEjUWrcuXU/Xww/gbGul5373oU1LwFBez44ih6jHeyirVGra/9x6K203aLbeQv2I5/VZ/T+7icElLx/ffA5D+97/jb27Gs3s3huCDBSAsYlnGPEiUGdani4c8X3X3JubujybGncAoGzHL0TEt76GW0esEMSY9/TPi2FHZfPCDf6O0rFjZodAdCorXK2KqERZme9Q8+hiuTZtoXbmSpo8WRe977DGca9e1ifP76+uRU1PQxcez9/rraf0u3E9PeTl1r76Gr7pauMdq2y4F17hwIc2ffkbD/AW4doTdgIrfD8EbnzPihhwSY53JhJyUhK+iktrnnsO7bx8AgeXPi7Vc98P+8cd4CouofuBBKFyCr+hHAHxbvxcVpIJzaf2rhAD63DpaCpvx1LSw95rLaX7jQRrvOhdHlZGWfSboORzG34Rr+F1trmXIysKYk4OnQTxsuqpcxM8SVZUCTjdyWg/VvRmyjEKCBtCw2U/LXgv1axpx9L0Bjr9bCAOQNGdOcIAUTPn5gMgmjkQyGOh5y6WkDAp/552F+2j8dBn2Dz+MypwFwOvFNn4cupgYArFhMTYPGhQW49171O31b/4HX2UlcdOnIcfGUv/aazjWrsVTXoYhOwtvxT71WPeuQgKtrarLmEAA7969op2XRY3kkFcj9frrsY07FkAVnZQrryDlyiujult1/wM0LYr+foaszNDDmWXQYLzl5SiBAO5dhUgWC6YB/XHv2oVj9Wo8RUU41q2j6csvcUXEip0bN1LzzDPClb5vH+aBA0GWVes8Z948Mu68M+qBw5CehqTXYwxaywCSJJH58EMk/+UvxE4+jrRbb6E2mJwFoE9Px5SXh7esnIDLpT5M6mw2VZjlJGEZyykp6GLE/GfvnlIMWUIcLcOGoXg82D/4ENv48ZgHDFA/r0i8lRUiqz4CU9889CkpyPHxmHL7kBHMDHesF5nqtokTyH7maZL+8heSLwhXUUu56koyH30EnUXkBBgygg/F+w6vsaOJcScJZVSH6A7LuD36ptlYt6eB815eTVHNobkxf0sE7PZDyhIWcbkdbbY3f/MNtc8+S/Ujjx7S9Xx1dVHTGiLjqqGbhnpsQz2G1DSS51yA4nJR/8Z/1H32Dz+k+sEHcW4USVW+2tooMY/8u/Luu2lcsCDcblUVBK0Ub3m5SGL66Fp0Lod6Y9KnpODcvJmaf4cLQfjLtoqKTq21eEpLRX9dTVAmYnXNn34Ib5+D/+ung20HY927l8PaV/AvFR4Bv1uP/4hLAHDvqaL8vteo/EBYWG67QczXPe52nI6wRRPC2KcfhoGjxI3W4yFgt2PMylKteX1iEsY8Yf2ERNbf1IQ+PR1jn/B6sFX3P8Cef70FKWELKvmSi0m68EJSrroKY25um2sjy+gzMog/5zISpozAMqgfhl691M8AwL1jB3JiIpYRI1R3Y+xUkTAUiA3/Rg0ZGehVMd4ttuX0EsJkMGCbNEm1iPb+7Xq8ZeUYs7LpcffdxIwfp7aTMHs2ADFjx4iSjEEiY5c6q5WUSy9Rhaj122UY++aReu21pF57DVlPP6W+38aFC8U4R7z/uKnTwO9XH1rNQ1aA40IAACAASURBVIageDy4Nm/G/v77WEcdhTk/H3dRkTrm7l278BSGXdEAe849j9qnnqbqgQfwVVZiyOyJIStT9FWSMBf0I/Hss5AOISs7dsoUTHl56KxWkufMQTGbSb/tVpIuuABJksQDWSCAe/t2/A0NyCkpJMyerbatD1rU+pRUdDHhuH/s8cdjHTmStJtuFBv8fjWGK0fEma2jRyOZzTjXb8C9333B1C+68lfi2WdhHjwYxelEMhjQZ2RgmzCB9JtvUh/6AEy5ucRNnx7VjmSxUPfKy92SkNsRmhh3kpCrOkR3JHC1R980G25fgOW7ann9u92H5ZpdRSjhxR9MbjkQje+8Q8kpp9K6alX0Dr8foMNYEUDAGZ6PrQSLv4emSkRW59k/Numvb0BOSiLliiuwHn10lPUdsoRDcTrF6STQGi7CEohM6PL78TeE32PIDWnNNuOrqiKw/Gn48XUMLXXqjUmOi1OtLLXNFFGpSSlfR9HUaZRedhn8+Aa+oLUW8IDiduGvCMZA64PW49f3wMd/w+8RP3O/W4enrBx9UgxI0TcZT5MeZeCpKH4/9g/3q/MLGLKzMfbqha+mBk/JbtHXlBTVopGTkzD07IFkseAuEgLvb7Ijx8URP/PkNu0pfj+eoiLkpCT0SUmk33IzqddcjT4xETklel6yZfBgcfM0WjFc8SG93/2QuBNn4N4WLo7iWLsW68iR9P7vm8ROPR6dzYZt0kQxPjYhxrqghayLjxZj2ziRMW0bOxY5Lk4Vc39TE57ycgzZWcROmkSvF19En56OnJhIYjC8EXvCCW1EwJgnPARyqngf+uD7CTgc4YQhhLDlfvhB1LkJp80i/lThcYg7Sawy1LpqFcgyliOOAKD86mtQ/H4ybr8dY15ffJWV6ve6deV30Q+XQU+MMS8P+7vv4dmzB0NGD0zBByc5JVnNNP6lJF1wAem33QqAqb9IxKt7XYQ90m64gfRbwl6OkGWsT0lRk9QADNlZ5Lz5H6zDh2PMyQHAcqQYq5BlrE9PJ+e1V9HZbLR8/XVUvoY+LQ1D+n6rZgXbBTAPHIjuEN+nPiWF1GuuofXbZWpY6XCgiXEnidELi8YUXAO0OxK42qNvavhpPzZiIYnDRd3LL1M048RfdG6gtRUCAfyNjcJ1ewC8paJmcNlVV1Ny2unqdl+dELkDZTN72smGDt20nJvCbmJ/o53CqdOoD95A/PX1YXdaXJyaWQthy8e5fn3EtmphqW5+B09p2PUJ4KtvoGXZMgonT8GxTliy1njRhnfLymBHfWIqy87P0dEUlfgC4E8YAJKOQLGIfTnXrmP7Zc/hrAvfXIqX5lK1XlgQXodOraWxd/sIKlaL96L4hQVpGZCPNS36e6oEJLzJ47F/8AGujZtI/z+xilBoio0xOwvzYDFPt+UbkT2tT0lR44z6pCQknQ5TXh6un7ZSNP0EWpZ8hS4+juRLLiF/ZXSFJXehsOZMeXnsTyi+GCL7hefJfPihqG3m/QQQUJNyks4/n7wvv0AOirASfA/WUaMAxIOPwYDicqGLi8M6YjgAsdOEJZ398suk//02FLcbvF6MwXYBcj/5hL5LvsRc0I++y74l4Ywzwok/2dkYsrKwjRsXHJ9UdZxCRIoxRGfK569cQdKcOfS4/z76LvsWU24fDDm9UBwO5Ph4rEePImbsGHw1NaT97W8Ys7PVsQrFofePR/f94nNyF39Cxj/D85sNGenquIf62FWY+vQh/tRTaf70M/VakejVmHG0GMsRoYRQDNcSjOGqYpwcXDO6HWs10msRiSFDTGMzB6cNHipxJ84Aon/n3Y0mxp1kS514cjolT6wfetjEOC0sxtVNhzfrD6D64UfwFBdHLTfmb2qi+tHHorIQFUWh9vkXourAqtajouC32/E3NorzPG3HTk4IConDIeYQBq1dX60QNF9dx3XBIzNOQ7iCYhyZnOEpKcZbWkrV/SL+5auvR58obhq6+DgC9iYCLhfVjz6Kp0jM1Y3MYPXX1IoSie9ehPf76Hmf3tJSyi69DO/evdQ9/zxIEJMmxmfv/J14WmTw+NBZrDDvTOS6cLsJEwZiyOlFwOGEtIEEdoW9A4o3OPc86Nb01IQ/B8WvI3D2RzRnXE3Thui4l6+mBmNuAenD7KQMjo63u4uLaVmxAn3PHiSeew46mw3r8OH0uPde4k85BctQUcaxeclXAOhT09SbqC5GfB+tR4/CuW6danXKcfFIej365GRy5s2jx73/EuO3/kfcRUUY+7YjxkGhiD/1VDIfexQ5IUF144cwtiPi8ScLS1IyGNSsXQBfz55k3HM3PYPxTUmSkBPEg4uckIBt8mTS/3E7cScFE5VsMcSMDc97Ds2vDe0L9cWQloYkSaRcegmZTzxOzwfup+dDD6JPjRZhXUyM+lATcr1GkvPmf8iZNw99cjKSXo8kSRiCrm1ryFWbkIAkSfR8+GF63PsvEs89R4xVxPjtP0YAhh49MOXmRj3g6NMz1POUdn5znSX91lvCXoFgMlQIOfi72t8y1tnC97OUK68g8/HHwu8nIB7Y5RQhxtnPP49lxAhxXlwcGf+8g4zbbmu3L4pb/C4Mwbnlh4ohPR19RoY67e1woIlxJ+mbIL7kx/c+Hjh8MePsJCunDRPJFZVNLqqbXKwsbJtM1N34Iqrw1L/2OnVz56rzBEFYpzVPPMG+W24l4Hbj2r49yu3rr6+n+Zul1M2di7Od4uyhRJgQ3vJy3MUlqqvUV1mpVqzan/0LwcuJiTg3bkLx+6P6EFlIoXX1DyhOp5r1KcfF429qonXlSurmvhRlbcsmcZNoWfQ6SrHIlPWsCL93QE3CAlA8XiwpboxxIl7tbjRQX1WA4g2gcwnRlI3hzPzk6/6OPjkFf3MLZA7HV9h2fMwDBrT73u1r9tC0tf0EP0Nef8wZNpIHtIBOhyk/GOvdVYhz40asRx6JJEkknHUmcSecQMLppyHHxyPHxmLMy1Vdd/rUcBJOIHjTiwtal+oYxcWpf1uHDyP+tNMw5uZSee99BJqaVHdpJCGhsB41krgZM9p9D5HJRrYJE0j/+20djgWSROLs2aqlDKhxYzkhAZ3RSNI550S5MY25udgmTybz3/9WhbEjDJmZxE2fjnXECKzDh6Pfzz0d+lsXE9OuJ8A6ciTW4cPabds8dD9XbXIyCaefrtZ2NmRmEnPssZjy80m79RaMffOIPX4KKVdfrbqOIWyRgrBWQ/H9/b0wXYGckEDPBx4gZty4qAQwAOuI4cSMHSvcxpGWccT3xJiTQ9zUqeFzRo/GOno06bcKwbUMGUzyhXPU/if+6U/tPogAJF14IdYxo4k/bdbPfh+WI49UPWmHA02MO8ncqXNZdOoidc6xN9D92dQAsk7isbOOZNqgdKqaXNzy7iYuen3NYUk4iLRgI8vGSUbhcnMXhy1SNWNVUWicv4CS089Qk01AWKGhrFR3YVtL1t8cbb15SkvZffbZNAdXWIHoKSoqe38UFmsEMcccQ6C1VVj0TXakYPZkyH0MUBrMsox0UytuN44PX2pziZh08eBVt/ArKjbnwIxH8NS1Xzc8MV8k2fUYaUfOCLs9vYEUUCR0O0ShBF2/Y9R9clY/5NhYMUUoc4Qa+43EPLB9Aaq6735aVq5sNx5oysuD9EHo9DrMA/pjPeoo9D160LpqFb59FaorNf2mm0g4PbrGsXVYWDT0SUnEHCP6GzrHPGRIVMKWLjY6p0KSJHr86x41kc0yeFDb93SEsMBD8cf2iBTOrKeeJOn88zs8tj30QfelJMvt7pd0OrKfeZq4aVPb3X/AtoMiHBJlEGNuHTO6w+t1hGU/MW7TT1mm10tzyV30EYmzZ5P38cdkPfUUqVdfRVJExnBU/zIyMOWKzyj+lAPUFe8EtmOOodfcF9t8/ww9e9LrlZeR4+NVbwGAztZ2QYsQss1Gzmuvqn2GCM/IQe53xuxscl59NcpTcqhYhg7Fu3dv+wVFugFNjDtJiiWF3vG9MQTX+zxclnGIHvEWSmpb+WZHDS5vAJc3cPCTDoC3qopt/QfQ+v3qDo8JWaUg6gQHPB6KTz6Zpk/E3D5Pcdh6DCXz6FOScW3bJuZbLlum7vfXN6g1ZNtzK+8/x7Z19Wp1/mrIevVVVlL10MNs6z9A1Lot/ArmTsK3a23UE3PMsUI4HJ+/jT+YDQy0yT6FcHECOV48sbesamuVWo4Juy/t27zsuPolHJ58JLOpzbHpw5rIn1WJacbVSJcupd/3q4gZNw5XqXDZ6/LHwfDzkQdOFidIwu2oi43FtXUrJXctxOdo+3ONFD4Q7r5er70GCNd+7PRoS1VntWIZORLyp0Kf8eS88QZpt96KKS9PnY9pDiYKtUeky1YyGIgZfTT9vl+FLSjKkiTRe+ECNeYcaGfqmXX4cPKXLyPvyy/addtaBg0i/7uVaszwYPySBKTUa64GusdNq2aYp4Yt6szHHiXzkUd+dlvmYGavnHiI6/weADk5lMksXMT9flhN6nXXdbrdX4rOGv5tynEdi3F7hOL4clLSQY785cSffBK5ixer49bdaKs2dRGRCVwOrwOrwXqQM7qG9DgzXn/46bDJ5cViPPDTt+L1Uv34EyRfcrEqOvZFH6OzxahTfupff52Y0UdHn+fxUP3EvzFk9lS3eSur8JaV4d5VqG5zbtmC4vEgGY2q0Ck+vyq2rZFi3FCPt1xkDzfOn48hM1MUFYiPJ/a4SQRampFTUki/7VYq/3EHLd+G15w19umDs74eb2UljmABg8YFC7ClNBCLyHw25uaqSS2WIUMwZKTQ/O4rBOILkC0mdGYDAVdbb4auVSSO6YLuM09z25+KedqlZFZ8g2ngcOrrjqBx/nwCTU3EjB0bNTcZQNKB3hQQS99Zk5CtYOqXT2uwEIRuxFkwcxZycK6pHBeHpNOhs4kblmtnMVX68A05ZsJ44qZNF1WSdDpRSxpRecl69Cjk5GT8dXXYjj1WnV/d4777sE2cIKaZHHsdHHud+jRuysujdcUKdPHxWAaHF1TYn5AlHMn+Vptss6nVnDqK6Yfiqh1xKJZM7uJPosIkPwfLEUeQ9fxzUclZXYUxN5ceD9wf5WoNzWH9uUgGA1lPPdXG3ftL6LNwAa7tO8QqUES7hn8NIqc27e9BORiSXk/2iy+0eRjtSvQpKVGhhu5Gs4y7iJAYf7fvO46edzTrqtYd5IyuISNeXDczQfzY7c6Du8mblyyh/pVXqHnsMXVbzVNPUf/yKyKRiLZzbwFc27ZR/8or1L8hikhIJhO+qso2sVnF4aD2BbH2qFoAoqEBT7BqUGQGdOO770VZxDWPPUbVvfdSG6yW5W9qxtirF/EnnoghKwvvntLwdbxe0OvxVVbhKS8nYfZsDL16Uf+lyID0NXsw9uqlzgPVxcYSOyyH1koT3ooq5PqNSIqIdcrmcFZ3TA8XZt0u2DAPuaHjqQ3Ggv7E3fsVpsv/Q8Y/bg8n6QwfHnWczqgT68YCpIeFLjJeGoqfhW6Q6vSbPeHs7IAvPA/U3K8fCafNQpJl5KBwxZ10Etlz5yJJkureNOXnk3zJJfR44H4STpsVFTuMJJSJbO7f/4C1sHVGI2k33UTajTd0eAxAzJgx2CZPJu3mmw54XGcw5eYSM2bMLz4/duLEdmO4nUWSJBJOPTUqJtoZbMceExUj/6UYevYk9rhJBz/wMKGOjyT9orGyjR/fLQ9TvxaaGHcRBlncwL4tF5bboqJFBzq8yxiZk8TRfZK4drK4sTcdghiHah0rwdWgFJ8P7759eKuqULzCbdeeGIeK3nv3lCInJmLMyQlaxuG5vrbjjiP2+OOpf/VVFEXBE1wpyL1rFwGHI8qKMmRm4tqyhYDdrs5XBWHduXbsIOB0EmhuRhcs2rD/lJe4aVMxpKWJtu12jH36EHfCCTgKa/G5dPianOgTYtBZRZU0OTaW2N6AIuGzu9CZJPxu4UVIKghWCzJBrwsGIf/4AnxwBfKP4RKaSeeKReVNwUQhfWKiWAzeGIOk16sWpWVYtOtVF58MU+6CmU+LJfeCmArC03NCVkLIEg+JcswYkdWbeF542TjJalVL9wHokxKRExPJfORhtdpTzLHHoIuPx9inD2k3XE9CcO5qR4TEO+m8tuVB9yf5or+oiwZ0hM5sJvuZp9udgqShAcHwgiyji41VE9L+yGgj0EWELGO7W1h966sPz/y07CQr8y8bQ0GGuHk3BV2u32yvptHRfjxMcQeXFTOJWJu3sgp8PnyVlWr2cnti7K8PV8zSZ2Sg75GBt6wsqvCGPiUFy4jhotZuye427amlD4E+779HwmxR5jHlisvp+83X4TrFPh87hg3HtXUrcjC5I+XqqwBRbKLf8i9ICryFPjkOxzrhhTBkZ4ll2hSo3xmD4g2g3zIXOdAEej2S2YyJ8DKCsiFchSu+l0i8svQ0w+DThV956J+QpXDMOvWGG+m3dg19Fi6gYP2PbcYnFP80FxSgs9nU6R3mQYOECA+PXoc1Mvt3f8s49H/yxRfRb+1arMGpHAD5y5apc2JD47H/1I3Es8+m71dfoTO3XYKwPSxDBtNvzQ/ETplySMdraHQWKWgRR2a5/5HRYsZdhFk2IyGhIOK3xfZi6px1JFsOT/A/3iIsc7vTS4vbx19eX8PN0/pzxcS2bjjFJYRHZxQPECExVbxevHuDVZ3atYzD8T9DejqWEcOpefQxtfA8BAtBBFcfcqxdA4CpoEAtXZcw+wxqnngCEIKTcccdWIaPIG76NHQWC6a+eWJB9Yi5yrpgcocpN5der7+GvPT/kD+/DvauweArwBmcnmFs+hFTRgExPVzUbRM/cL05gM6oIKMgrX4B2b4TSZ+K4gPZEKDPfRcQiOmFoSCHzPrTiBkxFEbMgaFng86AbnO4iH1k3E/St/3pJF04B/OQwaI+blwcxr55JNx+hiiZ2A6SToehZ0+8+/a1ieOFLGRJp0O2xUTNJ5Vt0dM40m+9tc1Sg6Hzfg7yz4zbaWh0Fp3Vqn7X/+holnEXYZANjM8SZfViDDFYXQpNP3V+UfhDJS5YhavJ6aPR4REVmDaub7O2LRBRE1o8OHjLwzHf0Dzag1vG6eqcUve2bWpGqz41Rc0mDa1zah4i3LdycjL65GQy7rpLzeKUDAYSZp2qCl3i+eeTcsoYEvLC15cjpj3E9I7F7FwDhUIk9YGIJeA2PIr0wwv0GNmIzhgUt5RkZEMAnc4Nn90CAS9yMB4rGwOYJ8zGOu3PkH00cadfgDz7KWGdGywg65HHX9HueLeHPjGRuOPFfPOEM2cTf9JJxE2bekCRy37pJVxDh6olFUPrve6fXGPs1avDNswFBViGdJx0paHxW0WzjMNoYtyFXDPsGkBU4zrphwCuS68/bIXG44KWcZPTS5PTR2ZzNZOe+wfN33zT5tiQqPqDguuJiPmqYuxwtLuaUQhDRgbGXr1U12zcjBlIBgOmfv0iLGMhxqEpKqFkmcSzziTl8suiO1WyHN44hcRZM0mJ+YweR9mJzQ5a8DsWQks1+H2wK3rtU1O8sAj1acnIBj+sehpDnwGknzkKJAXT0SdhjPVhjPUBEuQci5wkMnl1Q6ZDUrAwv6yHGQ9BakFU+9IxwjUed8zPE7uUyy8nfubB53Cacvtgv+Jy9WFEZzRiyMrClBe9YELoYcc6evTP6oeGxm8ZY05Ot2ZE/y9xSG5qSZKmA/8GZOAlRVEe2G9/PPAm0CvY5iOKorzaxX39zVOQVMDG8zfy/b7vWd/8H3C4xIohXZRVeSAMso5/rnmD/PUump97hXSnENxtm4sZNUWJWpElJKoh69dbXo7Oag3GecNzhGsef4K6V16h/0ZRotEXsehBqMxdr9dexVdbi6FnTzLu/Cc6s1lY4zodvn0VyKkp6rH7J2BFseYlKF4Kpd+L1YxGXYqh4icoK4GGUnhlmpjgb4uodSvpiB/Tl5i/3o6uYBzsfA++ewqOvpSEvMnE5dyG7uTbyEiKBXs5DDkNUgvQfSXmwMpj5oRj1B0OrJmCTRt/drGGzpC3+JMo13+I/ps2wmHsh4ZGd5P19FO/dhd+MxxUjCVJkoFngOOBcmCNJEkfKYqyNeKwq4CtiqKcLElSKrBDkqT/KopyeAo1/4bQSTosBguxwUJM/uaWLpvi0BHuXbuoe+VVRu8VbvFyl49El4ijfrhsK/rTGxmRE1GrN+imVsW4shLTwAE4N2yMmnZU96KYnuRYsxbbBx/Qumy5ui9UAF5nNqvFM6RgspCk16NPTcVXVYUxK1vNoG6vDrHokFt1O7NpgVjRIH0w+sxmoAS/Wwf1wcSrhhJIyoP6IiiYgXT2f1En4gw/LypJSnfum6I/U26PulzITd1eLez2ONTVXrqKjopYdHZ1HQ2N3xrt5V78UTkUN/UooFBRlOKguL4NnLLfMQoQKwnzywbUA22DlX8QzLKZGJdw8QZa2tZ+9TU04Nz08+PJHZ1X8+ST2N8PL1DQ3NhMoltcN9bjZFfVfqsABQsxhJb+81VUYOjZM7wqyn6UXnghMZ+J8pPWMaOJPWH6QVdBCRVjiBkzBnP/AmJPmE7spEmoywl9/xwsvglaa8W6u54WkI2w4b+igYReJJw8ldhsJ0n9gxnNUvDrOunv4v+kXH4JGbf/H3EzTlCrRmloaGj82hzKY0kmEFnVoRw4er9jngY+AvYBscBZiqK0qcsoSdKlwKUA6enpLF269Bd0uX1aWlq6tL3OUO2tJiFoGa/99lu19nKIxIcfwVhURNWT/4ZIaycQiFqofH9sC9/B+u23VD/2aPg8v5/U71ZFPVXt/H41SS5RhjDW08q367eT4QhP6UmtrUUHNFVVsfTrr0mrrKTR7cFoNGIAFL0eKTLxKyJ2XO/1Yj/lFArbWdQhkrhjxmL+YQ0/9S+AH36AU06hcMcO+n58AxmVS5EUL3LAw97yckAhQ2eiImMKWXs/AWD1jkpc5hR6/2kyP2ZMwtaym+S6NZhd1WyqjuPI2HxKWpJp+KWf+cyZ7Prhh192bhfzW/ru/h7Rxrf70Ma26zgUMW4vqLZ/VtI0YANwHJAHfClJ0nJFUaIK0yqK8iLwIsDIkSOViRMn/uwOd8TSpUvpyvY6Q1VrFcVBMR6an6+ubxpi21WiLm76tX/FNmUy2U8/TfOSJey94UZ6L1zQYaGEPa+/jsPnY1RKCtZglafW71dT2tpK5pP/5skVpZy+4FF6mky0Bt3UcV4HXnMSEyeOpOyqq9HFWGkKThuy6nQMHDKEXYEAuUePoqWlhdayMsy5uXhKSlC8XpLmzCFpzgV8/9VXpNzzL9J75TAscpw9DmGx7lgMsRmi3COgjB8PPh8DQw8Njnr47kkIim2ITIMdGnZDv+PJGnsNvCL2Hz31NNCbgOPJCR0c8IMSYLxsgMlTiV4Z9n+X39J39/eINr7dhza2XcehuKnLgciaY1kICziSC4H3FEEhUAJ0vOTK7xyz3owtKMYee2Ob/aEVgQBalnzFvr//H+VXX4PidtO0qOPKXaE6z+XX/pXG9z8AwPHjOpAkYsaMoc9QMeRr1+6IcFO3UlzbguLx0Lp8OU2LPxV9iI8n0NoqCn4gsqNDRdd1MTEYMkWlKEPPnhgyMvBnZpI9dy7pt9wc3an/zob/ngHvXAivngA7RbazZC9DKlsBPg+sew2+vgdWPB59ri0d9qyEpr1QcAJkjQrv07ddbAGdDHLHpRo1NDQ0/lc5FDFeA+RLktRHkiQjcDbCJR1JKTAZQJKkdKAAIkod/cEwexT0QSf9PUvCa4q2LFtGwONRY7Uh7O+9h75HD/Tp6TR98UW706H8dru6lJe/tpaK4GLazo0bMeblIsfGcuZ0YS0PK/mRvEax+EKcx0FpnYPWn7aKFWqC7mfz0CMItLbiqxJLIOrT09EHHxJ0Vqtaq1ifEc5eto07Fjk4D5aqrbD2FShbLWK+IbZ+INzaC86D/8yCN06BRX8VxwIMODl8bP+Twn/nThIu+pOegOP+ceAB1tDQ0PidcVAxVhTFB1wNfA5sAxYoivKTJEmXS5J0efCwe4CxkiRtBr4CblEU5fCvdP8bQbKHC1bEuGB3YwktK1dSdullVD/4EIrD0eac3EWLSL70Erx7SvFVVLTZH1pMwZATLv7gb2zEtWGjOtfXEKxvPLpyKzafWAAh1uNA8fmoiFwCUJYxDxyI4nLh3StE25CRgZwYXERA1mHMzlK3t8ubp8HHf4PI9ZtzJ8GuL4VAVwQX5S6NWL1owq1w1puQnA+GGMiLKFofqtk88kIYf2P719TQ0ND4nXJIRT8URVmsKEo/RVHyFEW5N7jteUVRng/+vU9RlKmKogxRFGWwoihvdmenf+v4G8Ou6XOWBqg572JcW8VMMOd6UbM669ln6b9ZZEbrbDZR8jBXZAdHFuEIERLjXi+/TPbcuQA0ffopfrsdywHWn431Ovnoo1upeec9FFmkCBhzctTpRu7CIiSDATkxUXWfK06XukqMWvNY8cP8c2HbIpEF3bzfA4MtQ5SQbK2GT24EUxyc+pzYZ4qHqffCGFFAg4LpkH+8EGUQx2poaGj8gdEmeXUDkWIMYNu1D8caUac5tNygMTsLyWCg9/y30Qetz5Br2FteBkePimrDV1EBOp2wYBMSQZKwfyKSnSKXV2t55jWuf+8n0l12nhtmpPbpp9GhkLivhJcHnchF2z/D1LcvuhhRt9i5ZQvG3jlIOp26vF7A5SLhjDMw5eer686aXTVCiLd1ENNOGyDivnozVG2GI8+BftNFclfueBh7dfjYqf8KDpQPBp0WFmkNDQ2NPyiaGHcD/naStkIFMwLNzSBJqvCGlq6DoEtYlmlduRJTfj6WI45AURRavv4aT2kZ+tRUJL0e2abH0Csb5/oN4fOC9B09jLIvaymLTUeOFZGCVoOZz3KO5t2+E7jluD5YnB/13wAAIABJREFUjjySQLC4h3vbNmJPmA6AnCTmGSsuJzqrlZg+NhHr7TGU5Lp21mfuPU5YuBvmQe9jwRwP/U+ELe/C4NPAmgQn/xsyOrDcZT3M/sMVatPQ0NBogybGnUQJBEBR1HKJiqLQ/NnnePUSBl/7dakN2dntLm0n6fUYevakafGntK78jvwVy3EXF1MenAplHhoWNWNWNt49pQDqwgwAiTFiKtHZR2Vj6Sfiy38feyk7E8XfKVdeiSRJtEbMsQ0tci8nBitTuYIrJj1/rHpMfntvZMqdkDUSxl4bLis57gawJEKfieL18PPbHQMNDQ0NjTDaQhGdpPrBh9hzwQXqa8fqH2j+8ks+mxyvbrvySpmkZZ9SMUWIaVNmfJt2QoTWGPbb7bSu/gHP7j3qPkN62AI2BBOs5MREdKboaUDF983ggdOPwDJkCP23baU8PVyI3bv0YSheirV3grottDxfKI5szu0Fq57p+E3nHCNixGkDg52OmIqePghOfFRYvRoaGhoah4R2x+wkrq1bcW3ajBIIIOl0NH3yCTqrlSWjTJwsKkhSGweVkp1yYws9AE96YoftBVpEJrZkMFB28cXRbuweYTE2qlOP2mY763RhcZQkiXmXjOa6t9ezr86OYdkDkJKPVLMDfWI/fA3NGIOJY/rERHJefgbT4tPg84Xtd3DCrXD0ZcIFraGhoaHRJWiWcSfxVlWheDz4ampoWryYxoULsU2aRIvk4f7ZOsovOQEkiX0t+/A4hdD6rR0X/M987DGSL7+MtFtuAcQ84hD6SMs4S4ixIcJF3RFHZidw8/T+9JEqkBQ/1GwHFHpddASJ55wjsrhXvwjbPsYaW4dsOMCyj5Nu04RYQ0NDo4vRLONOoCgKvkpRNMO9cyd7r78BgPhTT8W1+2vW99Vxw4wLYPGX7GvZxw8FEsd8A5WjOl7gwDp8GNbhwwBw7dqFff58dZ8cF16k3pAl5uVGFuU4EHFmA/2k6ClTzsYfSb3tOaRProP1/wFzAhTMAKNNVLty2TtoTUNDQ0OjK9Es407gb2wUVa0Izx/OuPsubOOOxRsshpFpyyTBlEBZcxnrkps48zY91Rltk7faQxkUvf6vnJKi/m3s1Qv0etVdfTDiLHryddFinNBShPJQnhDiwaeDqxE2zoPMEXD+RzBkNljFNff2nA5/XnBI19LQ0NDQ+HlolnEniKyU5QiKcaimc4h4UzwFiQUsK1+GJyCEu8kTtX5Gx+0PEBb03jQ9Yx5+Geuoo9R9cmwsvd+apxYKORhxZgP50l68shWD30FZ8jG4anZjjO2De/Cf6TfpHNAZYNPbIkGr55Fw+kvw0vGg+NnV7woy+008pGtpaGhoaPw8NDHuBKFFFgB1zq8+RRTJeGXaK3xd+jUGnYGByQNZXblaPfZQxdjZM4l3x0rsGJ7ElP2KgABYDrKmcCTxUivH6LawO/U4NjfH8rnnWD73JKGvkNBVSfw0PoBh1vNiKlLPYeETcydAwqFZ3xoaGhoavwxNjDtBaJEFY58+eEpKANCnCrfuURn/z957x8lN3/n/T02fnZ3ZXtw7btgYbKoJpoaSYLgU4hQcQhKOXCAhjSsk4Qgh5AIPfsml/Y7kjoODkEDoPQQwGEyzjSvuvWwv04s00vcPzUcjTdmdtXeNwXr5j52RPpI+oxl/Xnq968mc3Kor2VmNegpQk7+JOl+dhYwjmQg+pw+PszioKybH+MsiJ2Oq/Yc+SU0DJU1o1W9wSkn+GPgM/7k7nwqlqBqoGru740xrCcLEhdbjz/2h/tfuWWrDhg0bIwbbZ3wYkDs6wOmk6pScanU6jVxdM2bXzwbg9NGnU+OtIZLOk/HCBxfyjb9/o+T5Y3IMAJejwmeml2+DBz9v3bbhEbitBeeKX/GYtohl4eaSh27tiJGSsyy64xWeW1/cqMKGDRs2bIwcbDI+DGR7enDW1+E9LlefKpdrXIixwbF89fivsnTWUoLuoKGMFVVvZ/hOe74aViaboTfVC0A0o/ckdkrOyia0ezlseRaiHRDrgp+NgSev1/cddzG/9FzL5rZoyUO3dkTZ3hljT0+CN3f2GNvTSpZkJlvZ9W3YsGHDxiHBJuPDgNLbh6uu3ignSYk+xKAX3rhh/g1Mr59OyBsySFaQrhn/svxfWPSXRWiaRlzW85IrVsb9enlMtv8dDr4HmRjICZj3JfjCn/FWBchk1ZKHbu+Msa1Tn9eu7nwLyFueep+r//fdyq5vw4YNGzYOCTYZDxHZSAQtqyvFbG8vzvp6vNOmDnJUHiFPyFDG3cnils8v7nkR0H3JwkxdkTLOyvm2httfhIgpjWn8qfqf+qqiwyQJThhXy+q9fWzKqeadXXky3tkVY09PvOg4GzZs2LAxfLDJeAjQZJmtp5xK+623AjoZu+rrcOZaD3pyNZ4HQsgTIqkkkVXZIGOXlFe+XqceXNWZ6DQUtMYAFbEA5CRsfAw0VS/Ysf1l6NH7HzPvSzDrcgBOm9xgOeyHn5jJX689nW8smkxbOMXdr+0E4GA4SUrWHzj6EzLRlDLo57Jhw4YNG4cOm4yHgGxMV6r9f9arYil9fTjr6pEkiSkvPM/E++8f9BwhbwiAcDpMV6ILgIAnYOwPuPXXnYlOYhn9ekklWfpkkTZQVXjnbnj06/q2uVdAOqwHbtVNgst/Cz79mmYyfuX7Z/PVMycxf0I9F8xqZWKDrpo9LgeaBrtzarg/IRPLKKhlTPA2bNiwYePwYZPxEKDG8+ZaTZZRIxGc9XrTB8+ECSUjqQvRXKVHM3ckOgxlXOXKm4+r3dVAjozlAch4/0q4awa89O+w4+X89pOWguTUTdZ1EyyHzBylk/Ki45qY1BhAynVbcjokHvj6aVx/7lRuvlRPw9qVM1X3JTJoGtji2IYNGzZGDnae8RCg5pQxgNKrB1+56ofWNGF0YDQAbbE2upK6Ms6q+WhloYyf2vkUm3s2A5BSUtaTaFo+SvqNX1n3Nc+CmZfC+49DToULOB0S7/zbeQR97qJ5jan1872PTyeeVrjpsQ3s7I6TzGRJK3rA10NbM9RO7uaMKY1Fx9qwYcOGjcODrYyHADMZp9avB8BZN0QyrtbJ+GDsID1JPYXIrHwVTZeg77a/S1SOFu0HoHsbdL6v15MWOOtGOP/fweWFRTfq2xqKfdjNIR9+T/mAsIDXRWvIx86uOH2JjLF92T6FL/zh7bLH2bBhw4aNQ4etjIeArImM91+nK1Nhpq4UIU+IKlcVB+MH2RHWg6ySShJN05AkiYScKDpGVmUUVcmnOG37m/73/FtgwdXQsx1OvFLvtATQMhv+6S2omzjgXFJKis29m5nXPM+yfVJjgEdW7+eR1fvLHGnDhg0bNoYTtjKuEIl33yW+/HUAAovOMrYP1UwtSRKjq0fz4OYH2RXexcTQRBRNMbo8lQvWSmfTsPUFuOcSWPGf0DRTrxk98UyYf1WeiAWaZ4J74DKaP37jx1z53JV0Jjot2yc1BcocYcOGDRs2RgI2GVeI9ttvp++BBwAYdeutjL7jF3inTcU9evSQz9Vc1YyqqYwLjuPT03RTsyDhuByn0d/I1+d8nYsnXcxJzSfp++//NLz5G9jzhk60l/6q7PkrxarOVUCO6E1oCBTXybZhw4YNGyMH20xdIZS2duO1s7qamksvpebSSw/pXEIFf2/+9+hL9wE6GQfcAdLZNF+d/lW+cYJer/rJHU+yunM1yQPvgJKF4y6GL/z5MD8NxjUB5Kxs2T5/wtBM7zZs2LBh4/BgK+NBEHn+efZ85Stk+3TSxOlE8h9GFyXgBwt+wA0n3cC548/F79LP9eSOJ43ymOZUJ5/TB0BKyn1VLbMO69p/WPcH3m7TA7EEGSezVtP42dObWXvzx4uOdTmkw7q2DRs2bNgoDVsZD4IDN3zHusHhMPJzDxUzG2Yys2EmgEHGv37v1zyy9REAqtxVkFXgsWvwN+m5wklBhC2zD/m6qqby+7W/Z/GUxZw66lSjUUVR6hRQ43czozXI5vZ8YwkNjEAzGzZs2LAxfLCV8QCQ20q0EpTl4m2HAUHGAAfjB4GcMn7jl7DhEXxv/haAZxrHoAI058l4TecarvnbNeyL7KvoWn2pPmRVJpW1km9aSZcc//C1p/PS9xYZ77OqRsLu4GTDhg0bww6bjAdAfMWKEb9Glbu4eUPAHYB1eslNv6qXofyTJ8uKM66BpumA3mrxyueu5M22N1lxsLJ5diQ6gGIlXGimFgj63ExpqrZsi6SG92HEhg0bNmzYZDwglK6uEb+GWRkLVO1fDX17YN4XCeYaRwCsahyrt1nC2n6xPdFedI5S6IjnyVgz1Zoup4wFmoP5OWw8ECGclHl+Q2XXtGHDhg0bg8Mm4wGg9PbiCIxszm1JMn7ldsimYcxJTLhuDb8/93fMqJ/Bqo5VxphwOmy8bouXMKfnsCeyh7tW3oWqqYYyTipJo7oXUGS2LsQ7N53P9xfohPy1+1Zywi1/49r7V9EZHfg4GzZs2LBRGWwyHgDZ3j6c9fU0f/97hBYfWhrTYChJxppeD5q6SRBo5MxxH+P00aezvnu9YWIWPZEB2uPlVeoVT13BPRvvoTPRaRT3SCpJ+lP9xpiyXaHMc3IXB23Fct0j1u7rN4g5o6hsbo8UjbVhw4YNG+Vhk/EAyPb24qyvo+FrX2PML34BgG/u3GG9hkhjmoSHmapeRcstTMimcpazG2ajqAq7wrsA6E/rZDq1dmpZMs5kMyQUvbxmQk7kfcbZlJHfDMVFP0rPs5iMRZ/jL9/zDr9fppf2vOmx9Vz0y+V0RQc/pw0bNmzY0GGT8QBQentxmRpBTHtzBRPu/d9hvUaVu4o7zrqD/0l4+Y1nCt8+8duMVXIRy7XjjXGTayYDGGQszNQz6mfQkeiwdH4SMJu1Y3LM4jM2m7lLpTYVz7M0GSczWfoTskG+D6/S61lv7YgWjbdhw4YNG6Vhk/EAyPb24mzIk7Grrg7HYRb8sCDeA8k+Lpp0EY2RNpprxvO1uV/DoD1nvtXhhNAEHJKDXRGrMp5RPwNFVehJ9RSd3qyYY3LMEk1tVsOD+YwBgm64dtEULpjVYmyLpGS6Y/p5+hMySVPak5mMVU3lT5v+VJE53IYNGzaORdhkXAaapqH09Q25EUTFUFX4nwvhPybC3rch2QuhXJ3rb6+Df1xuGe5xehhbPdZQxpF0BK/Ty4SQXhSksNkDYCHoWCZmCeCykHEFyliSJP7l4hl8cu4oY1s0JdMT19ss9iczrNqTN32byXhb3zZuf+d2Xj/w+qDXsWHDho1jEXYFrjJQYzGQ5SH3K64YO16Cnm3660e/rv8NjdH/1k0oecikmkl5M3UmTI2nhmq3ngcck/X2jte9dB1Oycmvzv2VNf0p3k5SSVLlqiKhJCzpTJWQscCFs1u56ZKZ3PbsJu55Yzdet+7n7ovL7OqJAzCxoYqtHfl2k5lsZsjXsWHDho1jCbYyLoNsr05kQ+1XXDHWPQRVjfCx70H/HnB6YMq5Ax4yMTSRPZE9aJpGf6qfkDdkFA0RfZBf3f8qL+97GU3T6En2EPQEAdgZ3glgKOloRleuAXegpJk6k82wuXdz0Xaf28nVZ04CYHN7lLX7dHN5OClzoC+Jx+lg4dRGtrRHUbJ6VLhojCH+2rBhw4YNK2wyLgMlR8bDbqbu3wev3A69O/WmDydeCUhwwhIItg546KjqUaSzafrSfYQzYWq9tXq1LvTWi2bsCu+iJ9XD+KAeBLajX492nhiaCORTo0KeUEnF+uSOJ1ny9BJLoJeA0yFR7bUaVWJphd3dcUbX+lg4tZFYWmH1Xp2oBQlXErVtw4YNG8cibDN1GahRXTk6Q6HhPfGaP8GrPwfJAXOXQP0k+MpzFTWAaK3Sybo93k44HWZCaIKRGrX8wHK29W0zxq7sWElPsodxwXHs6N+RV8Y1ujIWJBvyhEqSZGeik6yWpSfZQ423pmh/0OcillYs2za2hRlfX8WZ0xpxOSRe2tzBO7t68IX0BxthrrZhw4YNG1bYZFwGakpXi4fbLrEIHev1v5oKNTkf8YTTKzq0NaCT8YHYAbqSXZzQdIKhjJ/b9Zxl7IbuDfSmepnXPI+AO0BPqgcJiXHBcYDucwYIeoJs7NnIms41zGueZxwvlHNvqpfJTC6ai2oqpymwrzfJ6ZMbCPncLJhYx/Kt3WztiHLCdL0BRnc8UdHntGHDho1jDbaZugCaorDp+Dn0/d/9ADi83kGOGCLaN+Rfi+jpCtES0NOK7l53N+F0mHPGnYPP5Ss5dndkN/3pfhp8DVR79CCvel89QbfuQw6nw7gdbqrcVYTTYa587krkrMyZfz6T53c9TyStk7FIoQLoTnYbbRcT6dLdm8bW6Ur9uJYgm9sjKKrGugPdAGzv6it5jA0bNmwc67DJuABKby8oCol33wWGWRlveR76duXfh8YO6fB6Xz0uh4vNvZs5vuF4zhp7Fg7JYZiqBZr8TazrWoeqqdT76g31PCY4xiDvSCaCx+nBa2pE0ZXsIpwOc8fKOwxlLCp1RbNRznnoHH6/9vcAxDN5E/Xkpnz97jG1+v0aXesn13AKDZ24+xK2MrZhw4aNUrDJuADZHmvxjGFRxuv/Cu/dDw9+LrchV9ZDmKkrhENyGMr0k1M+iZTr4FTYhnFu01yymk6ALVUtBhlPq51m1MIOp8N4nV5L+pPwI3udXiPaWtSw3p3eDeg9lA/GDhpE+/o/n8O9XznFOMeZ0xoBnYwNSNncueyiHzZs2LBRCjYZF0Dp7ra8l3ylzcAVI6vAI1+FJ76pvz/xSpj5Sf11aGhkbMaisYuM14JsBeY0zjFez2+Zb+QUT62dapBxJBPB7XAbecuQLxzidXoNZfza/tdYvn+5QcYbujdw4SMX4vDvAaA56KOh2gPAhbNbaAnp92tMbf6+STkyjqatUduapvHwyn12HWsbNmwc87DJuABKVwEZH44y3voCPP8v+ffBUbD41zD1fGidA77iKOXB8MnJnyTkCTE2mDdxF5qp5zblm1nU+mqNFovT6qYZZupoJorX6aXR32iM3RvdC4Db4TZ8xmu61vBPL/0Tu9I6aYvGE/94XjVXL5yEx+WgyuPizX89l99/cb5xrlLKOJaxkvGu7jg/+Os6/u+tPUO+DzZs2LDxUUJFZCxJ0kWSJG2RJGm7JEn/UmbM2ZIkrZEkaaMkSa8O7zSPHMzKWPL5DFPwkKFp8Nw/w7t/yG8buwAkCeZfBde+rr8eIm7/2O0sX2ItlWk2U7skFzPrZwLw8QkfB3RfMOjK2OfMK1aP08Pvzvsdl025DIC9EZ2MzcpYYG9mr+X9rFF1/PjSWcb7UTV+HI7852kO+nA5JFpDPlpDeo3ttJK21K9+Z5duIn//4KG3XJRVmbZY+X7ONmzYsPFhwKBkLEmSE/gtcDEwC/i8JEmzCsbUAr8DFmuaNhv47AjM9YhA6e4yXjsOx0S97x1rsJa/DqZecBgzy8MhWb82szKu9dVS7anm8cse5/aP3Q7AbWfexuyG2TT4GyzR1x6nh5ZAC+eO1yt/CWWc1bJFVbnSmtWU7HIMnBXndEi01vgY31DF1R/LdZ+SFHb35IuTvLNbJ+NNbaXJWNM0Hnp3Hym5dOQ2wNM7nmbx44vtJhQ2bNj4UKOSPONTgO2apu0EkCTpz8BlwPumMV8AHtU0bS+ApmnFXQs+JMgWKONDxtbnwOEGUQLy+9vB4TzM2ZWG8Bl7HB6m1k4FYErtFGP/4imLWTxlMWBV0R6H7usVJTOFMi7XH9kMpzT4Z7nmrMnUVnloV/XcasmhsHxbF06HRCytsHK3Hql9oD9JOCFTU+W2HL/xYIQbH1lHyO/mouNLVyfrSnaRyqZIyAnDHz4YFFXh3fZ3OX10ZfndNmzYsDHSqMRMPQbYZ3q/P7fNjOOAOkmSlkmStEqSpKXDNcEjDbPP+LAiqcP79Wjpq56Fr70ETtchmaUrgSDYnyz8CXdfcPeAY90Ot2GqFmlNgoz3x/RexMKsXQhzJS4VddB5LT19IotPGG2Uwwx44W8bO7j4v+7hiv+7m319CeaNqwXg/RLqOJrSI8cjqfI1rUUpz6FU93rjwBtc8+I1luA1GzZs2PggUYkyLsUgheWXXMB84DzAD7wpSdJbmqZttZxIkq4BrgFoaWlh2bJlQ55wOcRiscM/n6bRsHu3cVPi2ewhn3Pe3veBKtbslgEZth/m3AZAb66O9p4te3h17+DuerfmJkWKaH+UZcuW0S13lxx3dePV1Lpquav9LgAaaSSMnv60bsM6fLt1Un8n9g5jPGMY4ykdHb6jT6+L7XOmWbm9j+BM3Y8e3fRzRrvirAFeems16X1WZbymUyfjtRs30xzbUfLc23u3A7D8zeU0u5sH/ewA78b0HPLX3nqNXR6dkAtN/0cSw/LbtVEW9v0dOdj3dvhQCRnvB8aZ3o8FDpYY061pWhyIS5L0GnACYCFjTdPuBu4GWLBggXb22Wcf4rSLsWzZMg73fOFnnuFgV14VhhoamHuo51ybgDHzD3tOlWD9e+tZtm4ZZy440xJJXQ4NjzUQjUQZ1TyKs88+m3A6zC1/vqVo3HmnnMecxjn87oHf4cPHhOYJ7Nink+KU46Zw9nFnA3D9vdfr8/jy+pLXW/nuSngfRjX4GTu5no2mfZ84fQ7P7lpN49jJnL1oiuW46NqDsPo9Ro+fxNlnTy157tffeh22wLwF8ziu7rhBPztA77ZeWAFzT5zL0zuepjPZya/P+XVFx44EhuO3a6M87Ps7crDv7fChEjnwLjBNkqRJkiR5gCXAkwVjngA+JkmSS5KkKuBUYNPwTnXk0fOHP+KdMYPaJXpxjkOqvqVm9UjqyMEhl7s8VAifsTA3Vzpe+IwL85QFWgOtSJJEa6CVRlejJfgro+pm4UoCp4SZWtFkHvz6aZZ9k5sCVHmcdOdyjV/f1s3T6/RnvUSuypdoSJFWskXBXMJMLWcrb88oGmPIWZl90X3sj+6v+FgbNmzYGAkMSsaapinAdcAL6AT7kKZpGyVJulaSpGtzYzYBzwPrgHeAP2qatqHcOY9GZGNx0lu3Ejz3XJzVei3nIfuM29bCz0bD2gchmz6soh5DwajAKHxOnyVneCCIWtUep07G5sjok5pPMl6L831/wfe5pPYSS4CU8NGWarFYCHM/48JUsVE1PhqrvXTFdIL8r9d2cNeLukElnqt/Hc+R8Y8f38jX71tpOV4Q61DaM4q5y6pMRs3Y3aRs2LDxgaOirk2apj0LPFuw7f8veH8HcMfwTe3IIrVhPagq/hPnkVy7DhhiNLWmwdPfASUFz+VSsY+QMv74hI9zcuvJFStj0SzCXJdaYOGYhazuXI3X6TX8qIvGLULbodHrypfOFATblxq8+YMo4VlImF6Xgxq/m8ZqD905Mu6JZYgk9XMXKuNtnVHawwUpV7lzCqVeCcxkLGdlu8+yDRs2PnDYFbhySK5ZC4B/zhwcOfP0kPKMOzbAgVXgqQahFo8QGTsdzopVMZjM1DllbMbE0EQAar21RftKKWNzV6dyEMRdqEBH1ehFVRqrvXRH9X098TT9CRlN00jkCoT0J2S2d0bpiWfoTVjPYZDxENStYaa2lbENGzaOEtj9jHNIb9+Oe/RonLW1SH6dhIekjLc8B0iw9An443n6ttoJwz/RYYBQ0GYyvmbuNXidXoOo63x1RcdZfMbDQMatNfr5moJeVu3pQ9M0emIZFFUnYkHGL2/uZPm2LiQkMlmVZCaL36PnOR9KapNQ0XJWJpPNDElV27Bhw8ZIwCbjHNRUEofwFfv1vF2Hbwg+4y3P6uUuxy6Af2uDyAGobhqJqR42RF6y2Vd8/Yl6RPRr+18DrDnFAuZSmhk1g5yVjbrXAxUBEcFVhYS59PSJADRWe+lNZOiN60QM0J+UDV+xfg4NkVHXm8gwxqOr9ENRxhYztWqbqW3Y+KBwwV8vYFRgFPddfN8HPZUPHDYZ56Cl0oYSFmZqyVdhNHVWgfYNcPo/6e89VdA4bSSmOSwQpCp8uWaMD+qlK/9h6j8U7fO7rWbqq1+4mjVda4BiMt7Us4l90X2cN/48FE2/TkbNoGn5FPVL5owCoDHoRdNgW2fM2BdOyIYyLkRPLM09r+9iySnjDz+AK5tBURWyahbnCFVIs2HDRmm0x9srqvh3LMAm4xy0VMqInnbkzNQVK+PwPr3sZUPpXNijDcI8XUpNTqyZyLtffNdikhYwK2NZlQ0iBp1oBaGpmsoNr9zAwfhBPjf9c5a0I3PNa1VTcUgOmnItGLe0R419/cmMEcBViFV7+vjj67t4Y0cPjE0Z86kUZp+xYUJXM/gdh5DKZsOGDRvDADuAKwc1nVfG0lCVcW+uOtSHhIzdDr3SVTkCK0XEYG1IUYrIRXvFt9re4mBczxV+/cDrluvEMnn1q6gKmqaxJfYKSLKlYUQkKRMvo4z39OjXkTi81KZMNmN5bcOGDRsfFGwyzkFLpQwlnI+mrlAZ9+TIuH7KwOOOEgykjAdCYQBXndca5CVyjp/f9TxBT5Dr5l3HgdgBS63raCavfmVVZn33ev645XaqJvwXz8S+CFIuMCwhl1XGWzv0c1R5nIbSfvS9PSz+zeuW+T214ymLWdy8T1zfJmMbNmwcDbDJOAc1nUbyFviMvRVEU//9FnjuRnC4oLqy2sgfNAwyHmIUsSW1qcSxFz96Ma/tf43VnauZ3zKfk1r0AiJ7InuMMeY+yYqqsC+q9yBx+vUqWJJTV71b+98nJlsjtZ2BLbhCawwy9nucpBVdEW9s62Hd/rBBvv/z3jP82+v/xqu71xbNM63mK3CJz2EHcdmwMXzIqlm6k6Vr3tsoDZuMc9BSKaScEvZMmEDt55cQOKOCFnuv600UUJUR68o03Di+8XgAzh137pCOMytjOSsTzUSwB7prAAAgAElEQVS5bMpl/OzMnxnbn9j+BHsie5jfPJ9ZDbOQCvqMmMlYVmUOxqxlzmsCWVwOeLjtB/SG/j/Lvqrx9+Af82e6YzqBel1S/qFA0lV0e0RXyit26e0gX9m6h0IIFWwu5TkS6U2KqvC9Zd9jc+/mYT+3DRtHM3713q8456Fz6E31Dj7YBmCTsQE1ncaRU8KS282om2/G3Vq6h64BTQNvSH99+e9HeIbDh8k1k1l95Wo+PvHjQzpuWu00lkxfwqSaSUQyERRNYWLNRMZU58t+7o7sBuDElhMJuAM0V+nWAlHNy2ymVlSFA7EDlmuMqpOortJJUnPr5m2H9yBz5j1fNJ+UybQs5ch4W4fuk/Z6dKXbESsu1ynIWPi4zdtK4dLHLuXejfeW3V8O3clu/rbnb6xsX1l2TDqbZmV8ZUlzeimomspdq+4qum82bBxNeHnvy0Bl5XJt6LDJOActlUIaai3qWAekI3DxHTDvCyMzsRGCCOIa0jFONzeddhMtVS3GE2/IE7I0mtjRr/vPp9dNB6DeVw9gkPLO8E5jrKzKRaTSXAORbCcAmqanGvnGPMju9LKi+URTeTIVyliYsGUtDkB3ojwZx+W4sW0gM/XuyG7uXHln2f3lIAqSmCPIC3HXyru4t/teVnaUJ2wzOhOd3LPhHl7dN3irTBs2PihkVT34cqD6AzassMkY0DQNLZ0eWpEPgK4t+t+jOKd4JOBxeuhJ9gB6NS9RRAQgq2UJuoOGSVuQ8YlNJ+J2uFlxYIUxVlGVIjN1fVDlSwv14ita1s8lc1oZW1tYgEQFIJ4xkZxDQXKFue35lazY3k1c0Um5JxmhEKXIuJwyVjW19E2oAMIMPhDRC3+6IO7BINLEzBHqP3rjR1z/0vWHOk0bNoYdh/P/5liFnWcMaBl9Ia4oYMuM7ly75qbpwzyjoxseh8dQe9XuakvKE0CDv8F4Lci42lPN3Ka5rOpYZezLZDMcjB/EJbmMwiA1gSxjGrJwQCfji44fxaNtNbSZOjVKzjhVE/6L/tTH8tskheppt6MqQf66+jgSqm6u7k1GeOjdffTEM3zjbD3aXZBjQi42U2uaRlu8jdHVoy3bh4qHtjxkqH4RZFYKYi6l6oSXgvBtm+f1+PbHD2mONmyMFNTcA3OpwkI2SsNWxugmaqgwlSkdg/sug/b10LcbXD4IjhrZCR5lcDvzJu6gJ0i9r56vHv9Vg3jNZCxqXLscLha0LLCcZ03nGhRVYX7LfGPb5uhr/Od7/wnAxEYfi08YTbW72nLcjPFxHN5u0p6NxjaHR4/cdLii7O1JkMrqZJxUEtz4yDr+4/l8ENVAZuo3297kokcuMnocVxLYtaV3S1E/5Xs23MMDmx4ABjZTD0b2sipbolIPpUuVDRtHGqqqk/FQivEc67DJGFBT+gJXkTJuXw87l8GfPgeJHgg0fWiiqIcLHkdexQU9QSRJ4ob5Nxi9kM0dpARBK6rC1FprUZSndz6N2+Hmn0/5Z46v1SPXN/bllXNcjiNn5SLVeOVZeoqV6sqX0ROpUQC7evtJ5ZSx5EgBWUAx2jQKIosreTKOy3G++dI3eWbnM2hoRok+M1nG5TgPbn7QYoILp8MseXoJT+x4wjLHuByvqG72YGMe3/44lz52aVF9bzsVy4ZAUkly0+s3Fbl8PkhkNd1nLCxeNgaHTcaAltaVi1SJMk7mQvUjByDWCVX1IzizoxNmcjT3UBbNJUqRcSwTM0y/Amu61rBw9EKm1U3jwcvutgSCAfSmejnp/pN4cc+Llu0iYtvhLvYHA/RlDpDO6kQrOdN4W5/AP+4+1uztR9M0g8jCqXw1sA3dG3ht/2s8ueNJIJ+CFU3n7eM/euNH/Oztn7Gua51ljoqm+76zataIHjWr7lQ2RVusjYe2PFQ018GUbmeik5gcI5nV5yGURqESL7fNxkcfj29/nCd3PMn/bvzfD3oqBsQD60Bm6kozCI4VHLNkrGka4aeeQk0kUA0zdQXKOGoqar7jZaiqvI/wRwVmMjabkEUP5AZfsc84IkeKyBhgUu2konN9aeaX+PZJ3y57/V3hXZb3mqpHbNZ79S5ZruAG0ppOppIjhcPThcPTzdfuW8nX71tpkFZPIlL2nIKM9/blU7HEQ4FZxYpUrd5UL3euvJMz/3wmkUzEQq5pJc3TO5/m1rdutZC0+VzllHFhi8iBxoczdhrJsYgtvXogqTnFcDBk1SxXPHUFy/YtG5E5Gcp4ADIWY8AO+IJjmIxT77/PwR/cSNvN/46WFmbqCpRxrMP0RoOqhrJDP6oQZmqX5LJU5RJkbFbGQi1HM1ELSQv4ncX1v1uqWopUshlFZJzVx57cchoA3qaXwaGTmNOVRnImjcpef9/UljedOfKEtqVvi+WckbROxvv7ignOnCstXvekenhk2yMARV1oUtkUMVlX4eagMRjcTG2OyJazcp6cSyhpc0EVG4NjW9+2D0WVqMEUpCBjc0vUwZBQEmzq3cTWvq2HNbdyEHOulIztQK9jmIxR9C8/+d57+QCuSsg42g5mH2bg2FPGIoBrbHAskslfLojXHMAlzNjRTNQyVsBc1UsQVXNVc1HQFuRzFjsSHZbtaka/3tS6Sfzo1Fst+zxuOUfGaW5ZPBOk/AIgSflFrrBSkFCZB8MxCmEmPbMyFsTaEbfOL51NG5/NXGgETEpXzUdzP7z1YXb26/nYgnzT2TTXvHgNt79zu/G+aF7pI0PG0Uz0I0H8n3ryU1zy6CUf9DQGxdz75vKDV39Qcp+maQahHmqzlJFARcpYzf9ftAO9jmEyVuO6uVDu6MgHcFVipo51QuNxeUI+Bn3GLkl/Ahe1pwXGh8YjIRk9kQFGBfRI8y/O+GLJc5nJWARUlSLjy6dezlOXP2W8b/VPQIlNZUzih0gu/biJNeP47PTLUGPHG+McrnQuiEvj8vkNeNyVmcMEsbVFisnYrIwFKfUkewxTW+HDQlpJGyRsLsEJxcr46Z1P85M3f8Kv3/s1kI/EzmQz7AzvNM5dyj98pAjy5hU382/L/+2IXGukUfh9HG0QCvP53cUV6ECfv1FffYAUukKYm6WMBCrxGZuvbcc7HMN5xtlYbpGVZfoe0FNQKjNTt0N1i97DOJs5Js3UO8J6la0Tmk6wbJ/fMp8XP/MiLYEWY1uVu4r1X15vvK/z1tGX7jPem3ski//ALVUtaFhNc3W+Ol2JI6Gh0eqfwLZ9l7DgtPH0ROOk1LxSr8rOJMUGADQpgeTUF6lIJkLQp1GJFohkIvxu2XaeWLuHqgn57RISUbmYjM3KuoiMs2nDV1y4+BfmDd+z8R5Az8uG/AKbUlIW5ftBmqk74h0feiXzYQkeEu6NcjCnzQ1JGasjS8ZD9RnbUdfHsjKO5n/ksWXLgEoDuDog2ArCV3oMBnAdV3ccAKeNOq1on5mIS+HRyx7ltjNvM9773cU+46aqpiJlXOWqQpIko8b18c3j+clls7npklmkVJ3oRADLWNc5JA9+himei8ho+e85kolQNcDz1uSayZaxv3h+CzjyC4ZDclDtri7pMzaTbCkztUHGcn6ceSE0cp8zVtIWUdR9qT7LglVq4TXPayQRl+MD5k5/GHC0K2KBvlTfgPvNangoZCzGjpQiFQ87slb+/BYzta2Mj2Eyjhc/cQ6aZ6xmId6pK2N3buwxqIyvPv5qXvrsSyWjowdDo7/RUuTDHMD1leO/Auima6EMBURAl3iavnjyxSw9fSJ+j5NZDbOAfABZc9CPEl5A0GPtt9yX6kMNLtffaMX+6zNGn2G87ojpi+DYurzx6MszryHoCZYkYzMKlXEqmyrpMxbECyaFLHKgc+QtfMadiU7LOUuaqY+QzzihJCou33m0otB3f7RCWFzEQ2ghDlUZlyqrOpwYqjI+Gi0t+6P7eePAG0fsQeGYJWNhph7/P/9tbBu0Alf7Or1VYvPMvDL2FgcafdThcriMxg+HdLyUJzizz/i7879rmLQLo6kL389tmmu8vvuCu3ny8ieNALG6Kt2fH3LXWo75xt+/QcSrd5ORtGJFfs64cwA96KwrrvdSvmaR7v+O77yeS8cvJegJ0hHvMCKmS5JxoTJW0nTG9YCwmImAzccWBtQUknEhwZvLd4o2lUfKTG0uaPJhRWGK2dEKQcblsgvMD0WHpIxHiASFm2kgMjbvOxqjqV/a+xLX/v3aI2YFOmbJWI3FkXw+Amfk1dCgAVw7c51yJi2Cj/9UV8X1U0Zwlh9NmFMwzGRsRr2vnt+e91ujqpdoRvHsp57l5c++bBlb461hUo0pX9mrn7/OU6zcmzmHVPtiAtk5RftOGXUKvz37j7Q4TqMv1c8ZUxoMs7am+omkZIKeIG+3v83nn/k8mqYRyUSKFspSyjiaI+HeZN4iY/Y9FwbUGGScWwi6kl2Wc6ZVfTHNallj4RsOMq4k3zOhJA7LzHvvxnvZ1rftkI8fDhxNZNyT7ClrjhbbS2UXgJWACyOjZVXmgr9ewLM7ny06zvAZj7Dq+zArY/EbKay9P1I4hsk4hqPa+gMfMLVpw6Pw95uhaSYEW2Da+XDjzmNSGR8uzLWtzQFchThr7FlG0RCh/sYFx9FU1TTg+QM5MvZoxePGeOYj952B11H6P9iWPU1s3K+nQ911xQn5BU5105+QjVSt7mQ3e6N7iWaiRYFshUE3mWyGjKqTVziVJwHR+QqKA7kqNVObF7HC/GaBBzY9wKv7XuWeDfcMmFf73+v/m1MfOHVAP6WclVFUhXQ2fUhBUEklyZ0r7+Sq56/6QNOjjiYy/ufl/8yPV/y45L5BlbFJtaWyKXb07+DhrQ8DutuiPd5eMpd4pKOpBT7MqU1xOY7f5cfpODJtII9pMnYG9B941YJcAwPXAMHlr+SCjuZ8eoRn9tGH2UxtLhpSCqLv8lD+swplrMqhon2tPl1Be0254vGdNzBD/jkAL2/upKmqFiSVUJVmyv910peQLYvixu6NRDNRQp4QP1hQOg8UdAUg+iuH03kSMBNjJpshq2YNtVCojAvJuNRiKnpFRzIR7tt4n6Fyf/7Oz7nu5eu4a9VdZfNV90f388vVv9RLd8bbjO1JJcnlj19udNsS81I19ZAW0O5EtzHHhQ8u5M2Dbw75HMMB4cMv54s9ktgb2Ws0JimEIONSOfqQD+BySk4y2QwPbn6QW9+8FVmVDTdIqYeeo4GMzQGJR6OZOi7Hj5gqhmOYjLPxvDIe94e7mfK3F8r+4FGz0L8XzvgWnFV+0bVRGQTBwuBkvHDMQgCLGXowzBylk/C0lmIybvLrvm6vS5+Dpro4sXUmG/Y4eXVrF+/u7mVWSyugL2KGMtZctIeTFtW4oWcDkUyEkCfE0tlLeX3J64wLjgPKL/LRtE4CD215iDcOvgFAvbM+p57zZkahrssp48Jo2NZAK52JTqKZKDe/cTN3rLyD9d3rKUQ5Zbw3sjc/R5MvuyvRxY7wDqMetznw6VBM1YXm9rVda4d8juGAeKjwOofYw3yYoWoqXckui5XEDJEGWM6cLB7WQp4Q6Wya/dH9aGj0JHsGJOOR9BmbLSbDpYx3hXfxozd+VFTBDnSLkPmaclYels+VkBMDVgIcbhyzZKzG4gYZO/x+POPHlx8cOajnFDfY/uHhQCU+Y4HLpl7GK1e8wuyG2RWf/8xpjTz7rY/xhVOKv9Og3527bk4Za06uPG0CSTnLl//nHUJ+N2dN1Y8Lp8MGGY+tC7KpPcr2Xp20qt3VvNfxHv3pfqPiWI23xvDtNfpKp7zF5QSyKnPrW7fywu4XqPHW4HP4dDLOXavOW4eiKqSUlLFoFi6ohXmiM+pmAPqiJVRsqeCycsEoETl/fvO1BOGKhxDzYjhYwFCphbgzaX2oOFQy1DRNb9JxiIpKFJiptI/0SKE/3Y+iKvSl+0oSSG+uMU25SlniOwh5Q6SVNPtjusLuSnQZ36NoXmLGSCpj8zkHyh+u1GeckBMsfnwxj29/nO392y37NvZs5IK/XsCj2x41tn3rlW9x+9u3H8rULYgrcZuMjwRK+YzLolcvckH95IHH2agIZgvEYGQM1lrXlWLW6BCSJLF4ymIA/vjxP/KXT/6FoE9/EPC5dVIeFxrLGVPy6Wmv3XgO05v0XGnR8MEluZjZWssz69rYtfUCsqkWzmg9jw09G1A1lT1tIdrDOsmJlKzJtfpvxS1ZySYuxw1TLUCTvwmX5CKjZowFSfSALizRaUahz3h6/XQAtvdvN9RUV6KrKCArraSJZCIGYQvEMqbAMhOJCzIWczG3nRwovWl3eDcn/t+JvLT3Jct282eH0mTcm+od1B/98NaHWfSXRVz9wtUDjisH8VDhdQyPMm6Ptxs1oocCs8WjlK9elGUVD19mNQn576DGU0NSSXIgdkA/b7KzIjP1SJTDND+kDUSylUZTm2vRFz4A7g7vBuDt9reNbQdiB8qa/YeCuGyT8RGBGovhrK7gRm98DO67TH9tk/Gww2yyHgn8dOFPWbd0HaeOOpVZDbMMf3Ja082C15zwZZpDPq46YyJ/XLqAaq+LkFc3b0fSupna4/QYpu9s/DgSu76DV8lbSR5akeWBt/cAefPnmWPOJOgJEnBZy6Um5KQl2togY5MyFvnS5UyXgBFAJRa7iTUTcTlcvLL3FWNMT6qniDDT2TTXv3Q9Vz1/lWVfOTIWSlqQsVkZp7Ip9kX3ceoDpxY177h/0/0AvNfxnmV7oTIuRH+qn/MfPp+/7fnbgOME6Wzu3QzA79b8jk09mwY8xgzxPZmDCQ8Hn3riU3zmqc8YDxGbezcXEWcpmMm4lAtBfC+ZbIatfVs5+YGT2RfdZ+wX5BT0Btkb3WuQmlkZi/zznmQP73Xq38dIVuAyE2bF0dQDRHWbAyILyVgEV5nvdSabGZZ0JJuMjxDUWAxHoAJl/Lcf5V8Hh17kwsYHC0mSLEpcKOOFzZ/mqtlXcemUSwH498WzOX+WrohDnhwZZyKks2k8Tg+TGvVAjtMnN9AQ8NDTq4/VVDeaXE8srS867/e8D+jVye658B5OqLYG/CUVKxk7HU7ckttCxiKCfKDIZw2Npc8tNUyQPqePGk+NJXK2K9FVshb26s7VxucTiMpRpNy/SCZCVs2yL7rPWMgNM7XJZ5xSUjyz8xkSSoKnduTrhifkhFFLWTQPEShUxoWLa0+qB1mVWdm+sugzP7j5QYP0xYNEUkkSTof5/drf88LuF0rfrBIwB6INB0Sa2v7ofnb07+CzT32W3675bcmxZjXalcj70Es9fAkiymQz7ArvQlZli3/fMFN7QhZi7Ux05sk49/eBTQ/wjy/+o+W4ETFTm4i1Yp+xqVLXe53v8V9r/8t4b458L6y/LYJBzcSeyWaGpcJaXI4bKZVHAsckGYefeIJsODy4mTqT0BtDAEw+BxzH5O36SCHo05XQ5JrJfG/B90q2nTOTsazKeBwezpnezFnHNfHzT89hwcQ6Nu3z4ndVoaZbAQcdEZ0chFl8VNUkJgSnoin5IDJNdZHOpixqKJqJGmZqoVY27tMXlu6UlbgK08DWdK0xTKNuh5uQN0R7Ip/e1JnoLEqzMi9akXSEBzY9wLJ9y4hlYlR7qgl5Q0TSEX70xo+45NFL+OlbPwXygURmZXzNi9cYCtbtcLO6YzUPbXmI3675rfGQUFjpqlAZFy6a4v2mXqvKTWfT/OztnxlmafNxeyK6VWKwOs5miAV+OKJ4zYS+pmuNQX4rDq4oGtuT7GH+/fO5d+O9gPV+iIev3lQvD2x6AE3TDCtFRs0Yry2WCyWFhGT8ZkGPrO5OdlvGq5pKOB3WG0tkM3k3xwjkGVeqjM3+ZPM8lj63lN+s+Y1B1mYyNivePZE9xme0KGM1Y+l2du3fr+XSxy4dskn+SCvjY7JRRMd//AIA/7x5Aw/cuwKyafjiIzD1vCMwMxsjjeNagpw+uYF542rLjgm4AzglJ+F02FDGtVUe7rv6FABOnljPCxs7+OKCz/OXN/XFQPiMf3LGT/jhaT/kS39YybTmalJy/gFOk2tJu5KWCl3zmuaxJr6GeDZuLEi7uyS8jcVKqSXQYhCPgCB2t8NNyBMyiGFyzWT+vvfv/H3v38t+zqgc5efv6Cldl06+lKA7iENyEJWjbOjWG20IEi5lpo7LcaN4h8vh4svPf9m4f5+Y/AmW719uWUg1TaMtlk+bgmK/szj/lt4tZNWsYYbsT/VbxpuP2xvda8ynUojrDAcZm/O713atNfLgSwXQibF3rryTz8/4PF2JLqpcVSSUBD0p/ft+Zucz/OLdX3D66NORVRm/y09SSdKf1u9BJBOhO9lNQk6QzqbxuXwW3/vk2sl0JjuNB00NndTFA4w4Do68mXpn/07+6aV/4v5L7rcQaKnvoS/dR6O/0fKbMxPqJx/7ZP54E7GblfHy/ct544CetRDJRIYUf5KQEwRctpl6RKGl09QtvZLguecMPLBNT+dg3ClQLu3JxocKNX43D15zGmPrypufJEki6AkaqU2FEbfzJ+gBVlr/eSiRkxhb56cjkubNHT18/b7VOPGyuS3Cuv1hlHQNmuoi03sG2dRoZE1XxuOC43hs8WN8+6Rv48ZNWknTlSvRqin63IrIuKq4CYcwebudbos6mhCaUDS2EOZa1lE5SrWnmqAnSH+qnwPRA5axSSVJUklaArjMMFsY4nKchaMXEnAHSMgJ4nKcTT2beOPgG+yN7uX4hnyLy0LlLBbRVDbF49sf57a3bkPTNIOIRN6naKAB+bSsISljZWjKeE9kjyVi1wwRROSUnGzo3mDUHC9FxuY5rutaR1eii3HBcVS5qgxlLB6wRBCScFuI30M0E+WXq37JDctuIKkk8Tq9BhmHPCHGVI+xmKlBJyJxr+NKfER9xmbCLLy/2/u3cyB2gL2RvWWjqUVaoPi8lqDBbIqN3Rt5cc+LlvOarRNmn7G5Q9xQ6qkrqkIqm7J9xiMNNZ3G4Rs4vxWA3p0QaAJfcb6qjY82Qh7dXCtn5SIynj26Bp/bwQsbdJUzZ0wNHZEUL23q4OXNnbyypZN4JsuenjipVA2xLT8h3bEYTfMgayk6Eh20VLUwtW4qbqcbl+SiLRLjq/e9BYCW1d0nhWU1ByRjh9vwz/pdfg7GDg76Gc0+6c5EJ0FPkJAnxNa+rSVTUvpSfSXzPEFfvFoDrcb7E5tPNBTfg5sf5EvPfol7N95La6CVL8/+sjFOLJD7ovvYF9lnIed/f/Pf+fOWP5NRM3kyzvnwUkqKsdVjgbyZ+pCUsaYgqzJ/2fyXIuLQNI1Htz1KOB1m6XNLuXnFzZYxr+x9haSSZFdE92NfOPFCtvdvNyKgS5KxKVCuL91HZ7KTpqomZtTPYMXBFWiaZnynIlCrzqs//JnJuDfVS3eim3Q2bSHjel89k0KT2BXeZXmYi2QiFmVspDaNtJm64HdkdCNTkmXzjMUDl/h9FvqMlzyzhO8u+67lvFk1S3u8nRUHV5DVskZ3NHNa11Bqd4vfoe0zHkFosgyKgjRYUwiAvt12BPUxihpvDX3pPpJKsoiMPS4H88bV0hPXF7Q5Y2tQVI0NB/X/+A++kzObZrLs6Ioj/ptpcg2yFmZneKel45UDp65UJH1xUuU6nGoNyw/oHaZEkMqrm4p9XoVmatAfJMyEVw4iJxX0wLOgO0jIGzIKc5jJFXRTdbluRwklgceh36dGfyNjqscYyrg93k5G1aOBZ9bPNPKyIb84X/LoJVzy2CUlA29imVhJMh5VPQqH5DBIy0x0g8Fspl3ZvpKfvv1TVnestozZFdnFzStuZulzS/OpXTliePPgm3zrlW/xh3V/4GDsIH6XnzNGn0E6m2Zj90ZAJ6LCFC1zPfL+dD9diS6aq5q5fOrl7ArvYm3X2mJl7M8p41SejBNKgmgmSlqxmqlDnhCzGmehqArvdb5nFNWJpCPGA0hcjo9onrG5eE0h2ZvdDOUqcInvWJBxQk4Yn6NclLSiKfzf+//HdS9dZ8whq2Yt1oGhRFiLe1WuJvhI4JgjYzWt/yesTBnvgrrKKz/Z+Ogg5AnxVttbvN3+tkEyZiyYoC+QAY+TyY26KWv1Xp0wXt2aj5DtjuWfxr3qWJB0k6tXHW/4mcMpJ5KkIEm5BUl14UrPNhYoQYqdvfpv1kn+QVL4n12OfEpWjbeGS6dcyi/O+sWAn9GcIgMYZmqBmfUzLft7U73sj+4vWSIwqSQJZ8K4HW7+ce4/IkkSfrefhJIwIrF7U73Uemst5vTCBVIsgmYze0JOGD5jce1UNkXAFaDOW3dIPmNBxoqqlM3HFcpSlBk1X0NUJMtkM3Qlu2j0Nxq53iJaHfIEKmB+YOhJ9tCT6qHJ38SFEy/UU9P2vWJEWJc1U8tREnICRdOLhXidXuOBMegJGgVyslrW6PEdzoSNB6mEMrI+Y7P1pNDaIL7vhJIYkjIOeoK4JFdZdSu+R/N50tm0xRVTGIk9EMT3ZJupRxBaSv8xDKiMNQ3e+QNE9tvKeIRw70X38tdL//pBT6MszFGuhaUoARZM1E2HzSEfo2t1kswoKi6HRGG9isZqfaFs9edzk//3lSyn3f4Sm9oirOmUdFXsyC1cmgs5qi/sn5j8CcaH9IpgE2p1Na2q+dxYscCafcbib43HmlYkEPKEqHJVFRVGqHZXGwuhz+ljYs1Ey/43D77JK/te4YrpVxSdM5aJEUlHuPr4q1kyYwmgL6pxOW7x29X6ammqajJ8zEklafHlCWUsunWB7mcVythQSEoKn8tHo7/RMEUOxWds9msKEi40K5cquiKuIQi6qaqJ7mQ3Tf4mJtdMxiW5LFWiCpt3CGXscXjY2b8TVVNprmqmyl3F1NqpbOndYvzexMOSQcYFyhh0wvI588o46Aka5nuAaXXTAJ0gzWZqQVqyKhPLxHh5r7UT2uFAfOZ6X31ZM3Uqmxq0ApeZjAPuAF6Xt+e6aicAACAASURBVKzft1QnsaSSPGRlLPzUtpl6BKGmcsrYO0Dlp44N8Oz39dd1E0d+UscgTmo5yVASRyPMC5pQXmacNKEOSYKmoJfprUEjvu/8mbpf1+2UjG2fnj+Wn/3DHM4YPw2yPhw4UFOjAHjg7T2oqgvJoeCp183SmuYi0juNO8+6k5+c8RND9YwJ6gpZ1YrN1WYztfAdC6VciJNbTybkDRlmakFwQU+QsUH9c191/FVF53tk2yM4JSfXzL2m6JxdyS40NEtesTBTmytL1XnrqPfV89ynnuOM0WeQUlIW/3ZCSeCUnJzcerKxLS7HDTIW7SKTStIgY/M4gXQ2bZCroin86I0flSyWARjnLiTjUhWxhGIS+c6ZbIauRBcN/gY8To9x/wQKCT2WieF3+an317OtX49Eb/Lr0dfT66bzbvu7BmmI70eQsXme4rN2JbvwuryGaTjk0SvPXTP3Gj417VN8d/53jesKxWpWxqqm8uMVP+bbr3zbCEQ7XLTF2/A5fTT5m4pI1sgPl5MW1Ww2Z4sHDXMAV8AVwOv0lk1PMj9sCAgyFpatoShj4bKwlfEIQkvpX5jDPwAZ95jqn446ofw4Gx9Z/PTMn/LU5U9x2qjT+PZJ3y7aH/K5OW1SA7NGhfC6nLQE9d/TeTObaQp6GVXj59pFuhJuDvr4wqnjGd8QQEmNZXRgEmj6AvH0ujbEf0OnP6dUNRea5uC0lnPxOD2G6hmdM1eLAC8zCn3GUFoZL56ymB+e9kNCnpChKC+YcAGgL/KfPe6zPPupZ/nmvG8aC1G9rx6Pw0NSSdLgb7CYsgVEpyczGYsALjMhiepirYFWqlxVJJWkxXctOuV8YvInuO3M24xthQ0TUtkUPqfP4n9OZ9PG/jvfvZMrn70SgE65k8e3P87bbW9bxgqUU9YlyViOoWpqvvhINmUoY9DTzyDf8tMcQNQeb6ct3kbQHaTWW2so6OYqvXnJjPoZFn+rmKMgY4FoJmoQazgdxuv0Gg8S4ru5/sTrueWMW4yHlbgSN8jK7DOG/IOFIPtKoGmaUQWtEG3xNloDrbgcrrJm6kJlbFbQYp4iz140bPA6veVjFkqQcUpJEUlHjFSzSgO4doV3cdequwBo8DUMMnr4cMyRsVDGkm8AMu7N+Yh+sBOaZxyBWdk42hDyhJhYM5E/fPwPfG3O10qOeeBrp3LzpbMAGFunq8v6gIdvnj2Fz508jhsvnM69V5/C507WOzlNbKgidfAznBb4jnGO/oSMI3qileA0Pbc2nNSJxSW50TSJ0cF6Qu5a5P6TSey7inHOC41DRNEPKK+M/S4/t515G43+Rsv1Lp96OaATpMfpMTpPieAVv8tv1MtuznW9KoQwTZofAKrcVcQyMQshCTIW500qSYu5vDvZjd/lxyE5mNM4B7AqY0FWKSWF3+W3kLEYC3rxjR3hHfQke4ir+jbzYp3JZgzCLKeMC/29oCvMpJI0SCWcDhOTY8aCLyLeha/WUPSaxgV/vYAX97xItafach8EGZstRebvp5CMw+mwhZR8Tp+h/kYFRlnGuhwuvE4v8Uy8ZDQ15GvFm7+nezfey76INabAjGd3PctFj1zEttS2on3t8XZGBUaVJmNT5TSLz9ikjEUk9Pa+7ciqbFTC8jq9ZavSxeV4UaR/KpsikokY97dSM7VwE3xn/neKXDUjiWOOjLV0zmfsHcBn3LsTqlsgcOSeimx8+OBw5Ett3vYPczh5Yh0LJtZz1cJJfPOcqUiSxKLjmox62BMaqtCUWrbst/72go4Grpt3nfH+vOm5hTypL5hOqtCUappDfh6+9C98a8HVjPOexLaD+abnpXzGhZGg5sIQWUV/eHBIDua3zOeJy59g6eyllvFCGftdfoMQBOkUQiz0FmXsrtKjWk0KSJA66E1CCpVxZ6LT8NOJ68fkGO0xnewz2QyqphrFLgq7Y8XkGLIqs6Nfb+6ysWejQcaFHafE/REkVMpMXVgkIibHLKQu1KEYJ8i4uaoZCck4t9lEHvQEDTJ2Sk7j3p7QdAJLpi/hjkV3ML95vn4PXVVFzVQKHxK8Li9fmPkFbjr1Jj5z3GcoRMAd0Jtv5Ez8hcrYkaMBQXThdJg7V97JV//21aJzCWzs0SPG96aLXTht8TZGV48uScbiIcIcTe2QHJZUK0VTmNs4l750H6/vf93wGftcPmOOPzz1h3xp5peM82bUjCVSHfTgxnA6bFgtKjVTi9/BwtELKxo/XDjmyFhN6mTs8JeJpn7sWnjvfjtwy8aQML01yMPXnkGNv3zjgbF1VUgSvLNLN9uK5hNBj2RRSlefORXIK+MLRn+e5P4v01jtZXRwNNedM4sFE+vR5PwxboebRn8jLsllRF+L6lUC5qjwjj79IaLaXY1DcjC5ZnJR0w6hznwun0EYQmXcuehObl14a9FnNKvxUlHXZrL2u/yklJSlqlhnotPwYQuyvPWtW9kR1sk1nU0b6qrQZww60ewO7zZ8leu71xPPWpWxqqnIqmyQfjkzdW+q15KCJs4vlBvkI57Fgi/uvazKhLwhQxmb+zZXe6qN+zC7cbbxPXmcHm467SYumniRsb/aU2353sR1zPC7/HicHpbMWFL0nQOWgiLiPqTVPDEZRTZyJC+CnoTroRREaVZzTWnQv5/uZHd5M7VJGYt91e7qvGrPkfUFEy6g3lfPH9f/kb5Un2GmFp+jqaqpyPJTWCTnhmU3EJNjQ1bG4vObo/6PBI45Mh5UGa99UP/7Afc5tfHRg8/tZHSNH0XVqK1yMyYXhV3tkSwk1Vilk8TDK/dz4k/+RluvBzU1lsbq/G/W73aiynmV6Xa4qfPV8Zuz/sQvHvGzu7s4zcecL+3O6GlLpdrrCQhlWuWqypupcwvbhRMv5BOTP8HlUy/nhKZ8XIXZTF0q+EUUsACdTFPZFDv6dxgPI2Yy9rv8him53lfPya0nI6uysaj6nMVkHJNjbOnbYhz/fs/7RWZqocLE/AYK4GqparFUFzNHMkM+yErMQ5BxTI5R6601iF6kQgmIB59TWk8pukeQf6g5tfVUy/cmzN9mTAgOXG0t4A4YueOgP1CYzcLiIUQQ3UC/CQFhZSkkY5FqNxgZJ5SEYTEJeoKWYiBi27+e+q+s615HQklQ5a7C5/QZ31XQEyx62CsXTS9+s8Jn/Ni2xyztLmVVtsQ1FPrfjxQqImNJki6SJGmLJEnbJUn6lwHGnSxJUlaSpGJbyVECNTWAMs6YfA6TFx2hGdk4ljApl5PcEPDQFMzlhrqtyrgp6EOS4Mm1B+lLyDyfq/TVGMyT8bxxtWgmMnZKuiJ65j2Ffb1pnlxbXIHLvKjLEd0f65CKlZSAUKY+p88gY7Myczvc3LrwViMfWUIqq4zrffU4JIdlgfO7/KiayoHYAYPQs1oWv1v/vylJkqFeL550MaMCo/RSh7kF3ewzFp9fKGMJibmNc+lP9ReRsViUBRmXMlNv6tnEjvAO6rx1xsOBOL/ZTC3IRlgOhJk6nolT460xyOP93veNY3qTvcb24xvzpUHN+Mrsr3DX2Xdx68JbLW0eT2o5qWjscfXHlTyHQMAdsChjEU0tCFUQkUHGptzcu1bdVTKdqBwZi4eTMdVjcEvuotQmI4BLSRk+YzMZiwcdv8vPRRMv4orjrjDmZP79Bj1By/cyEGRVxiE5SCkp5KzMj1f8mC8++0Vj/9XPX805D51jfJfhdBiH5DiiaU1QARlLkuQEfgtcDMwCPi9J0qwy4/4DqLyP2QcAkWfsKKWMRXu382+BhTccwVnZOFbwxVP1nOEdXXGaqkVuqNW8WxfwcP2504z3b2zXf5ciXxngUyeN4c0bFxvv32/TF9At7TqhpJUsctbaHtDsM+6JycS238hdZ/yp7FwFWZUyU5shyHNCaILF1G1ezI5vPJ6QJ2Qxo5oXU7O6NpO4qGDVWqUHl5nJ2GymFn+jmSgxOUa1W+9AlVASxLK6YhILvVDG5czUWTVrdIdqrmq2zDMmx0pG9FZ7qi33R0OjxlNjnHt/dD/zmvTGNL2pXq478Tq+cvxXOGvsWUXnAt0Me8GEC3A6nBYz9emjTy8aO71u4BTBgDtgiZSOy3GLz1wEvQkzr1kZ37PhHpbtX1Z0TqFqi8g4Z7YfFxw3uJlas5qps2qW5fv19D5xz6878TrGB8dz/oTzLb5zMxkPRsoLxyzE6/SSzqbZF9N99+KB7I0Db7Cmaw2qphq9saOZKEFP0DDfHylUcrVTgO2apu3UNC0D/Bm4rMS464FHgIG7h3/AUI2iHyWiqeM5Mm48Dkr4XmzYOFxcdHwrVywYy08um20o3UKfMcB3zp/GS99bxOTGAIqqUeN343Xlf5OSJNFak1+ErrlvFe8fjLBmn77o7u5O8In/XA57buFX5/wKwFBYSlalN5FBk+txq6UDsvR56SrW7/IbKR6lyFiQ55TaKZbtgvwvnHghV82+im/O+6Zlv3kRndc8r+R2QXytgVY8Dg8ZNWMoOJ/TR8gTwuVwMaV2Ci7JxZa+LcQyMQKeAH6Xn4ScIKHq5yhSxrmOPIIURA7x+z3vE5NjXDLpEpbMWGJ5ODgQO2CYYsX9cUpOw4da663l63O+zu/O/51hpk4qSXpTvZw66lRA94eOqR7Dd+d/t8hPXwpmRTi1dmrR/sKI8kKY3QUuh4tIOkJfqq8o8rrQZywgUp/MEA80mYKc9/2x/bgdbpqrmkuSsVH0I6eMJSQC7gBJJckj2x7hzpV3AvkHpTpfHc986hnOHHOm5WHSbKYeHSjfZ37d0nXMbpiNz+mjK9nFqo5VQD6q39yq8622t4zPH3QfWRM1VNZCcQxgjnHfD5xqHiBJ0hjgH4BzgZM5iqGJoh8DkXGg8jZbNmwMBZIk8YvP6CrwufV6gEy1RyqKfJYkiSlN1dQHPOzsjvPxWcVNIsw40J/kkv9cTlPQS0PAwzPrRfCNl7e26irGMEsmMkaVsN54+R6vHqeHmfUzmV4/nfPGn4eqqSXJQHTMGR8cb9l+cuvJ3HLGLVwy6RJ8Lp+lkAfoBNzsb0ZFZVZD3thWSumItKtoJmpE+fpcPiRJYmz1WMYFx5FSUrzd9jajA6OpdlcTcAeIK3G8ufKh5XzGAtFMFE3TjEX5xpNvpMZbY5nPqo5VxoJe560jmokScAeMqHpJkvjWSd8CMMzUom3kpJpJvL7k9SEXkjCTUGGaU+H7UjBfr8HXwO7IbjQ0xgbHsqFng7GvM9GJrMoWMzXA5p7NRec0zM2q1YS9P7qfMdVjcEgOXA5XcdGPrCm1SdNbZIoUN7PfttRvQNwHCf3/ixjTWt1qBPgVQnwvXpeX53Y9x3O7ngPy9y0hJ4wAxnfa3uFrc75GNBMtWzBnJFEJGZfqHVhQ8O//tXfe4VGVaf//PFOTTHpvQOiBEBApCigEUBDFjoq6Ft5V13VXXV3LYtnFXXfX17bv2rDsa9tVkbW8PwuLghAQQaT3XtMgvWcmycz5/XHmnJlJJmGAhITJ87kuLjKnPvMEzvfc93MX/gd4VFEUp2in1aAQ4i7gLoCkpCRyc3MDHOaJqa2tDeh6tt27CAdWrl0LBl/HQNKxVQwBftx+APsB/8nlPZVA51cSOAWVqkiGuBysWLFC3+49z70tjawHxtrK/M7/yLCRbKrfRFacgUYn3JplYEVeA96Pz/d+yMOSATWVNTy3YCnxoZ7/o2u37CS6qnWuqMY9EfdAHqzLW0cccawoXNHqmPWl6wGoL6pvNcZYYvmx4Mc2r/9EwhO4cLHuh3VqZTJclBW1/q6Hth6iqMY3unfX1l049jqYEzGHkPoQqh3VLK5aTE1NDSZhorSwlDpHnR6Adbz8OLm5ueQ1qrZFZbHHdSsQNCvNLFm+hG9LviXVnMq2tdsAcNT5T4kxONTnh8lp8vu7qaisoL65ni9Xf6nef+9xNh3d1OZctIV35PPKFSv1n59MfZIYU8wJ/1+Wl3tEzua0cbxRteyd5Z6Us77WvhxyHOLVxa9yxHEEI0Ze6P0C75e+z+bCza3ucaBcFb+aphqffbuLdhNhiCA3N5fS0lLq7HU++6sbVKEvrynn0JFDCEVQWVJJpb2Sw4cO68ft2LSDcotv9bLSctVYshlsrFyxkoN2tR6EUtVSjjxo93Y5fJdsauvU59m+8n1YsJDcnMz6Y+tZtnwZ+aX5mIX5jD/vAhHjfKCX1+d0oGV0yGhggVuI44FLhRDNiqL8n/dBiqK8CbwJMHr0aCUnJ+cUh92a3NxcArle8YaNlJnN5EyZ0nrnD1thN5w/+TLZNrEFgc6vJHAmKQpp/Y9hKdmtzu176nbveb7gQhdz65tIiPAf/T9JmUTLF+DCxbtZevQAKVEhFFXZaWw2YQHMobG8utpBvwSPpRSd3ItR5/fnspdW8dys4ZzX7+Rz680FZtYtXccdU+5o1enpZHC9pz4wp4+cTk5GjrrRPSeXT7mc4m3FfLPZE5Iyfux4MmM9RXlii2L5z7f/Ia8xjwlpE8hMzGTJpiXUKeqaqDHUyPDzhxNdHQ1FMDBjIN9vV9cok2xJHKs7xvbI7Yg6wYCwAfrvYcGSBRws9DSL0MhIzOBw/mHiIuP8/t84tvsYX6/9GkOKAUrg8omXt5mn3R6KonDxiou5duC1TEiboM/J9InTfaLw22LH5h0s37IcgPH9xnNwl/pdJmRNYMkatS/wDSNu4O1tb7PHsofkmGSiGqOYOnkqh7cdZuPGjYwcN9LnXivWrIAacAgHqSNSqXZUMzp5NI999BgT+kwg5/wcVqxZwb6j+3zmxvkv9QWgpLmE76q/w2a20a9XP3Yc3EFUShS4645MHD+xVeT4xg0bWbF9BSOSR5CTk0NKeQp/+/JvDOw9kNU7V7f63uNSxun3fuWLVyiu8KygNpuaycnJIXd1LpH5kVw8/GJWrV5F/1H9EcsEfaL7nPHnXSBrxuuAgUKIvkIICzAb+ML7AEVR+iqKkqEoSgbwCXBPSyHuLrjsdv8uaoC6EjBawXrm1wskPQ8hBDOyUzAa2vYmmYyGNoVYu0ZLbhzTm6vOSeXxy9xdlxT1nbvW7VE8WOJJe5qfe4CrXv2Bo+X1vLBk7yl8CzVAZttt205LiEFdW75+0PVM7+OpLKZVAzMZTK1aWRpbRIJ7VwfzbnphV9QvfqjqEJM+nqSXofR2387sN5ORiSN5Z8c7VDdW6+vJ4HGZ3pl9J8PiPNHPei5wG232tDXlveV7sRgsJ1zbbQshBC/mvKgKsReBuru18RmEwSeVyruOdpQ1ipn9Z/Jj4Y/k1eTpObbaskTLdWOtgEa9q55rv7iWOd/MYdnRZdQ01ujNKUzC5BNNrRVr8abZ1ezXTe1vTl0u9WVNi0DXfi9WU+v/H6tmr+LVqa/qnzUXd1ZcFuNTx1PtqEZRFOqb1LQpbZlkZ9lO1U19hnOMIQAxVhSlGfg1apT0LmChoig7hBB3CyHu7uwBdjSK3d52x6b6MnW9uB1Xu0TSWczsN5Oc9JzTvk7vuDD+Z/ZI+sS6H9ZuMa6qB4ux9X/5A25x9o7W7gqen/Q8T4570ucF46PLPuKba1Vr2FuMrxl4DX2jfNubRod4guDCzeE+0dzCa7VNW/P1FrPeEb31Gt0l9SU+52oP/czYTKb09njUtPNPJMb7K/eTZEvqsOjcGwbfAOCT/9we2nfpF9WPATGeNX/vamgR5ggu7nMxzUoza4vW6mum2hy3FGO9FaLLs5z37LpnyYzN5OoBVwNqwKB3AJcWSa315wY1mC7UFEqzq5ni+mKy4rL46LKP/Fr8+6vUlyjNG6L9XrTgOQ2TMBFljfJJCdPGO7PfTM5LOY9mpZmG5gbqm9Veyf2j+2MxWNhRukMN4DrDOcYQmJsaRVEWAYtabHu9jWNvP/1hdR4uh73tXsZ1pTJ4S9Jl/PXCv3bo9WLd4qq4xbii1sXVI9O4dXwffthfysL1+ewv9hRKMHiJYGOzC4NQLfOuJMoapT+YvSOPnxr/VOtjLVEIBAoKNrPNJwo6ISxBrzm8q0yNoPXe7y1Mja5Gv2IcYgrxEQkt1eZEYlxUV+QToHa6PH7e48wdOzfg47X0sAHRA3wijzMiM/SftT7IqbZUCusKdcswNTwVs8HM4erDPtfULFzvaOrShlKm9Zmmi2CkJVJPo7Iarbogtsw91uY3vzaf7PjsNnOvx6WM44eCH/TI+0hrJNHWaHpF9sIgDHogobcIa2g55PGh8XoKW3VjtS7GZoOZ/tH92VG2A4fT0SVi3PMqcDXYMfizjOvLIW8txPRtvU8iOQuJs7ktSZcqxo4mA9npUWSlRnHXxP58cvc4XrpxpH58SY36gP1kQz7Z877hujfW0NwiV1ljW34VeeWBBznOzz3A7e/8dIrfRKWlm7olRoNRf4i2tIy9RUiLvPW2jGNDYn0+e//sbYH5iLHbItPyrFuiibRTcXZodK4Qwm/Zy7bQLOixKWN9zrMYLSy/fjn3jbyPYfHDEEJw05CbAPQKVSaDiT6RfVq7qf10QHI4HT5zoZUS1ZqItNWLWDunuL643ejwW4feytqb1uo55VajleXXL2d6n+n678JsMPtEn2toaWvxofH6i0Z1Y7XupgZ1SUSruX0muzVp9BgxLnntNXZlDsHlsCP89TJe/hdwVMOkR8784CSSTiDEbCTMYgQMKIoAxaRXAAOIDrMweXAC2pK1JsZvrzqEo9nFpqOV/O6zbcyav1qvp61x+SuruPDZ5QGPZd3hcta5r7HmQFm7KVVt4V38os1j3IIdbgn3sXxb5tTCCcTYe83Y7LGMvfPBtbXKtnKFva2rrliD1Lii/xX8T87/MGugWhhxyawlLLtuGaCK053D79TnbXbmbMDjCgfVgva3ZuzPTe4959oL0EsbX+Lbw9/qywP3jryXm4d4KmB5pzG1J8beFdk0TAYTQgj9GtHWaL8vbVoTCe+OZdWOahqaG/Qxp0Wk6elvfSLbLzHaGfQYMS596WUAnKVlrQO4ju+A9f8Lo38OSVldMDqJpHOItVkAQVP5BJprM+kd6/swiwgx8+6csVw2PIWSGgeltQ52FlXz0LRB3Hxebz7ZkM/6IxU8u7h1rqlGk9PFwvV5/L/Nvv1tD5bUkvX7xew5VsOxKjt1jU7qHM3c+NaP3PDGmpP+Lo4md99n0fbqmiaMLS3jlPCTE+O23NTeYqytfba1dttdxNhkMDG1z1R9LT7ZltxmVLfVaGXzLZv5xYhf6NsyojLIr8n3Wf91OB1+Xe/ec6hZxt8e+ZbfrvgtT615iqy4LG7KvInfjfVUVQ5UjNvjtqzbAHU5wp9lrOHdYEKrM679rtNsnujtM9k6UaPHiLHBXXy/8cgRDBEt1gNWPgeWCJj8WBeMTCLpPDRXtaN4Js76fqREtfYKTRyUQFZqJDWOZpbuVHNQLxyYwJ+vzuaLX0/g/qkDWX+kgh2Fat6Jy+Wb1/nGigM88slW7l+wmVqH54G94UgFdY1ONhyp4Hi16qLU1qj3Ffsv6t8eFXWqy7y5uW0x1qyiVmvG7pra3sFDLcXXe+3Xrxi3cFNra5+mNl4OwkxhetBWV4rxydLSBZ4Wnkaz0qyvuYMqxlodbm+857xltTaH08H1g6/XS4dqeIvxqUac3551O1tu3UJcSJxfD4q2Ph5mCmvtpvayjEH9d9EVburAwvGCAIPNhqu+HlddHaYEr7fCqgLY+QWMuwfCTu2tTCLprozJiMVsNLD+SAXQdkBWYoQq0p9tLCA6zMywNFV0hqdHEx1q4e/f7WNbfhVZqVHU2L0spGYn76/xtEB8dvFu+sTZ+PkFffUo7d3Hqilzu6X3HvftjNSSL7cUcm6fGL2jlTe17v4MimLC3uQkxGxkkbvS2KXZquXrzzI2YNB7+Q6MGaiXQGyZGuQtEt5u6qy4LIbEDmllTWqWYluWsRBqqceaxpqA8oG7K1q+b0FtgW7tOpyOVlHM4PsS429evF9KHhnzCOX2ch8B7xd1aq1rhRAIBFcMuILyhvJW+9+95F2O1x9HCOErxu4ALvB8zz6RffymDHY2Pc4yBjDFe0VM7/0PKE4YNacLRiWRdC5PzBzK/J+NOuFxWi7zT4fLmdA/3if3OS0mFIvJwEF3W8Zqu6fE4WcbCyiucXD3JLUu9ftrjvDPNYcBOFCiWr9rDnj6zHpbxN5WNKg1s+/9aBNXvLzK7xir6t0WuWJi6a7jKIrCH7/cyV//46kvrIlxmDlMf8iHG8N1C7WtGtjQ2lLWGJ4wnIWXLyTUFOojQCcSY/CIz9lkGbckPVzNR9aaQIAajGU1WUk0+Vq//npYe+M9D7cMvYX7z73f5/dwuu7hSzIu0YPQvIkLjdPd6poHpNxeTrOrWf9day8aXbFeDD1IjIXNS4wTvMS48qjau1hGUUuClOiwEzcjGJ7msdwuGOib3mc0CPrF2zjgFtKqBo8Yv/PDIaJCzcwe4ynSd7S8nuJqu24Fewuwt2W8v4WrWrO4y1oEd32w9gj/+vEIVfXu1BVhYX7uAfYcr+FYtZ288gaK3W5wLa3Fpbj0h7zNYOO6QdfxwKgHeHDUg/p1WwZeeQtJWwU1NIspMSwxIDHWHvxdUeu4o0gOT8YgDBTUemICtHSlB5Mf5IFRD+jbWwZYzRs3z+ezv3nwnvdAGmecLkaDkQhzhN7wQ7u/1WjlpsybuKzvZZ0+Bn/0HDe1l2Vs9LaMK/MgKr1VnWqJJFgwGw3ck9OfKZmtOy5pxNgsPH/dCJ76YgeTB7c+rl+CjZ2F1dQ6mn0Ede/xWmaNSictJhSDAJei/hn7l+/c9xY0OT1rzPuOewR4+e5izunlCYjydn9787+rDmEyCNIS3b2Dw8LYsa+a91Yf1o9Zf6SCS7NTeGj0Qzy+6nEGxw7GaFC7KdkMNixGC/817L98rttSRL2bFrTXy3bR1YuItEaiKApHa45yY+aN0youhgAAIABJREFUbR6rub7PZsvYbDCTFJZEQW0B9mY7Mz6bQW1TLVajFZvRxqgkj+elpWV87aBrqW2q1bsx+ZuHtlLDOpMISwTH6tWUK+/f9dzzAs/f7mh6jAJ5V+DxcVNXucVYIgliHrkkk9EZ7cdEzBqVztZ500j2E+TVPyGcvIoGLn5xBQ8u3OKz79pz0zEbDfq6sze/nOTbVrGgsgGLycBl2Sm8lrufI2V1OF0Kjc0uH/d3faMqvI5mJ0fK6skrb6CkTnWTh1vVh/fXW4tIjwnFajKwwb0mPjJxJIuuWaRbtmHmMGxG/1ZuVb1aJznG6in4oVmy7blbe0X2IsoaRXRINM9Per7d9WCtFd/ZvGYM6npqQW0B20q3+bSwBF93v7+XGG8B9ifG2u/qyv7+OvN2DpHWyFaWcVfTYyxjl8OTpO4bwJUP/ad2wYgkku5HW4ErmcmROF0KRVW+hRvSokMZ11+NPE2NDuFYtWf/nqcvwWoy0jvOxoq9JazaV0JFfRPxNgsPXDyIr7cVse5wBY98spX8igaev26Efu7BkjqKa+z8ZsFmnC6FBpeTw8csiD5wWd/L2foTVNubGdUnhtSoUD1ArSUXpF2AtcI31UVrNP+7T3fwxwv+6NPa0Wa2UdJQctJtDttCS286my1jUNdRlx5dqucKgyfP2luM/bU+1L67URj9zmuoKZRvrv3Gb6/sziLSEsnByoP6/bsDPcYyVuyeh4Qpzh223twINcekZSyRnIDpWUmtSrZ/ds94lj44Sf+cEh3qLjICUaFmrCb151mj0nn5xpHEhasP7/gIKxlxYViMBj7bmM/aQ+UUVDZQWe9ZK96SX8mzi/dQ7eW6rq23cX38h/xX9k16gFmfOBujM2LYUVBFQ6OnJaDGny/4MxMjJvps+3bWt8SUP8nR8nquHni1T8ME3aLuIGspGNzUAEPjhlLlqOKrg1/p27R8Xu+58jdv2gtJhCWizZe91PDUgGttdwSRlkgaXeq/t/aWJM4kPUaMXY2qZWwID8cQ6n4Tqi4AFIju1faJEokEk9HAkgcmcfkIT1nJc9KjCbV4clLvvLAff7k6m5UPT2b5QzmtrpEUqT68TQaByWigX4KN1X4irW0WI68tP0D/hNY1n89Jj8NsMpIcqbpI+8SFMTojhmaXwvoj5by18iD2ptai7E1sSCw1NdGtAsVAdVOHGENOqtxke/SN6kuyLbnbWF+nihaJfKTak8amlcTUvpvZYPZbF1oL2upOLyTeBVm6y++mx4ixYnf/wxme7dlYeVT9O0qKsURyIgYkhvPUFZ4KdYYWrR/P6RXNVSPT6B0X5q785cvcGUNIirSS4w4QG5Coiq0W7b37mNp4/smZQymobGDF3hIARvXxrOlOHKQuMaXFqA/QPnFhnNtb3f/E/23nz4t28ebK1r2HvXG6FCobmqisb6KpRe1tm9nWoZbSDYNvYNHVi7okb7Uj0doigmdtt6BGja7WGma0NW/d0VXvPZYz6R5vjx4kxnZibrqJ3m+/7dlY4i7xlzC4awYlkZxlxASQJtUWw9KiWPvYRdw3VX2wu9zdhK4frb4M7y5So7TP6a1GWNc6mhnbN5ZPfzlev0ZUqHr/dHdRkN6xNqLDLMTaLORXqFVBdhVVtzuOyvpG3LduVSO7b1Rfn25Gp4tBGPxai2cbVqNV7x9978h7sRgsXDXgKkD9jqGmUJ9CKd7oudbdKL1LG4vJYNKrs3U1PSqAS7SqSb0dwuIgvHVZN4lE0pqOtPDuyRlAcbWDOy/sx1vfH+RgaR0Wk4GUKI/bMNotvu/OGeMT5d0/MZxQs5FeseqxSZEhurD+Z/sxJjyzDKvZQEyYhVv6uiiqauDhf2/lueuGU+dVbKSkxkFSpOe69597v16tSyOvvJ5esd1jXbEld/9zAzmDE5g9tnen3+vbWd9iMpiIDYllwy1qIFcuuYDq6m3LMraZbQhEt7KMNWs92hrdbbwWPUKMFUVBsdsR1haus+M7IXEorSJTJBJJm7x4/QiaW9SnPhWGpUXxidvqTYpQI7EjQ0xEhpiwGA00Ol26CzunRe7zf03oy6XZKXqQWHKklV1Fnv0FlQ2M7hPD9sIq/t1koCLiGKv2l/J/mwp93N4t1421sooay3cXM+fddbx162guHtq9XtqbnC4W7zjG4h3HzogYt+fODTWFthn0ZhAGEkIT2mxO0RVoLwbejT+6mh7hplYa1f9wBu/WiS4XFO+CJP+NrCUSiX+uOTdddy13FOnuNeCIEDNCCOLC1Rfn6DD/bRNDLUafdpDeVnOszcLyh3L45JfjmTgwgf2VTr0k57c7j/m4pktrPCmPiqKws7Ca5bs9DRH2uAucLNt9XD/mjRUH+GF/6Ul/x/sXbNJLhbakztFMY7P/3tFtUeB2y3cHQk2h7RbveGv6W/xi+C/a3H+m0aLmY0JiTnDkmaNniLE7rUmEeOUbFu+EpjrZMlEi6QYMT1ctFKtJfSRpYqytEZ8IzdU8cVACG5+8WBfqEb2iOV6vsGTXcSwmA5uOVrLnmKeCWFmdKsbf7DhG37mLuPSl75nz7jp9f7W79Of+4loOlNTy+aYC/vqf3cz9bJvfcfzin+v5bz/tJo9V2fl/mwt5dfkBnH68Cll/+IZb314b0HfVOFxWd1LHdyaX9buMi3tf3Ob+flH9ulXhk0an+kLWXdaLoYeIscsdSe3Tx3jzB2Aww+AZXTQqiUSicV4/tTrYIXczijib+uIcqBhrqU6JEb4FPoanqwKgKOjNLBauzwPUFKvSWvWh/PXWIp/zFHeEV57b+lx3uIKpL6xgfu4BQG1q8eKSvRRUNvic882O48zPPcDhUl+hXOW2pI9V2/l+X4nfe/14sHW3ofY4UlYPqCVHu5o7su/ghswbunoYATMxfSLXDryWh8c83NVD0ekRYqw43Jax5qZ2uWDLAhgyE2zx7ZwpkUjOBGPdpTodbletx00doGXsdlNrucwa2e4GGCaD4P6pA8mIC6OgsoFQs5G0mFCOugUt1OybV1zdoAZ55ZXX+2zXcqELq+y89N0+pr24Qk+P8u5C9dG6oz7nrdpXQqzNQpjFSO4eXzH2LmyiCbM/FEXxiRTXxNggY15OmhBTCPPGzyM+tPs8/3uEGLvcbmqD5qZuqICGcuh1fheOSiKRaMTYLIzuE8Mjl6hphvHual3Rof7XjFuSEqVZxr4ZE9FhFn5zrpXVv5uC0SCY4e573Cs2lFF9Ylh7qAyXS6Gwynf9taTWwaajFeworOKG0b1YdN+F+r6cwR7XZl2jk9eWH+CO99brVjPAhsO+5Tn3FdcyPD2K7LQoNuVV+uyr8FrD9i4n6o29ycnyPcXM+Pv3/PNHtfDGEbeb2tHs8lt97ETYm5y4OiAQryP4YkshpbWOEx8YxPQIMVbcdal1y7jeHXwhrWKJpNvwyS/Hc0/OAADibCe3ZjwwMYJf5vTnkmHJrfadk2gi0e3Gvn/qQP599zj++fPzmNA/nor6Jib89zK+31fKtKFJ/H222u9409EKrn5tNU1OhaSoEAYmhevu4Evdgq7x5soDLN11nNfcYpyZHMHWgioczR6BrKxvItZm4Zze0ewqrPbZ5x3R7b2erdHkdJH55GJ+8U81neiFb/fgdCl6XjVAuVcpUUVRcLkULnx2GX9bstfvfDmanYz763d8siGfrfmV3PvRplYFUM4UFXWN3PfRJj5el9cl9+8u9Cgx1i3jOrcYh8V10YgkEkl79I23YTYKvx2k/GE0CB69JNMnZ9gfIWYjYzJiSYoMYcIA9WVca37RJy6Mwclq/qm3lTuydzRmo4EBiREIAdPcKU5XuEuD1rWwSmcMS6Gx2cX2gip9W3ldIzFhFkb2iqbR6WJHoepufuDjzVw7f7V+nHd7So3KejWITGtFWVnfxP7iWo7X2HW3vLd1/YcvdnDpS9+TV97A37/b53ceiqsdVNQ3sTm/kite+YEvtxTqbu8zjWYRl9RIyzjo0QK49KIf0jKWSLo1Fw9N4odHp5DQIiCrI0mOCmH+zecSYVXLLaRGh+qBYwdL65g8OIG9T8/Q+zuPyYhhaEok0WEW1j1+ES9eP4J499q297Ktlo+8JU8VY3uTk4YmJ7E2C8Pca9gLfjrKtfNX8/mmAp8x7Sz0rR626WgFPx701O/WGmRsya+ksr6JzGQ1X7bCbRm7XApfbS1itx8L25tit/D9n9f9j1X5d5F3NloQXYl0Uwc/egCXpaVlLMVYIumOCCF013JnMiM7RXdth5iNPjW1J2cmYjF5HpFPXDaUf989DoCECCsmo6da2AUDPM+SwckRxNkseq1tzbKNDjOTEhWK2ShYuD5f78GsccGAeLYVVLFybwkvLtlLVUMTV7+2mns/2qQfMyUzEaNB8P0+9Rk2JEUVYy13eltBlU8edZjFf8OLkhr1mVjvZdUXVXVN3rJmGZdKyzj4aRXAVe9+05Ruaomkx/PIJZlcNjyFS7KSdcsTYGzfWJ/jLCYDYRbfooVa4NgELzE2GgRDUiLZVVTDT4fKefiTLQDEhlkwGgTpMf4rVY3OiOFASR23vv0TL323j3lf7Gh1zKCkcPrF21jpbqIxJEV1q6/eX0aT06WnUGnR4Y5ml9+8Zm+XsFZvvGWv6o5AURSf9XF/lGliLC3j4Edp6aauKwFrFJgCi9SUSCTBS0KElVdvOpeYFp2mBiVGtHGGh1R3w4qW7R6HpESw53gN17+xRrditWpiWp3rcf3iyPVqNZmV6lsU44stha3u1z8hnMHJEVS5i5EMSopgZO9oPl6fx8vL9nO0rJ7ECCt3XNiXXrGhOF2KX5Er9hJj1T1v6XAxrrY3MeWFFdz8VvvFTLQANs1d3VPpEWLcVFgIBgOmePfba10p2KRVLJFIWnPtuelcmp3cqkWkP1Kj1Rf89JhQJg9O0Ps9D0mJbFXeMsamWqC93KU/BySGk+FV0nNMRgyDksKZf/O5AK0s2pduHMnlI1IZ2dtTwjE1KpTPfjmey7JTeGvlQXYWVZMUGcJvpw3m9zPV6oL+1oKLqz1iHBFiIiU6hGMd5Kbed7yGC59dxl8X7eZQaR3rj1RwvI2ULfCIcFVD00mXBA0meoQYOw4cwNwr3VOBq75UrhdLJBK/vHD9CF67eVRAx140JIlZo9LpnxDOO3PG8vKNIwGYmpnEsDTfLkWxbsu4t9sy1oR4elYSs8f0IjrMwrcPTGJGdoq+dt0nzuPSnjEsGbPRwGVeqVWRoSaEENw+IYOGJifbCqr0CGutKtmxajtNThcHS2r180pqHfpxv548kOTI0A6zjOfnHiCvvIGPfvIUPtG8A/4o87LctfKkPZGeIcb792HtP8Czoa5MRlJLJJLTpl9COM9fN8In0AsgKszMl7++gC1/mKZvi24hxn3j1b/fuGU0z1w73Od8rbb2uH6qB89mMWI2qvfwTvfS2v/187KwtcC3pChVbI9V2Zmfe4ApL6xgv7uCWHGNnaEpkRx+5jIuGBhPSlQIBZUNbVYA21+srn97szW/klc22VvlJ3tXIhueHkV8uKXdxhrebvTSmp7rqg56MVaammg8fARr//7uDQpU58sexhKJpFMRQvgULdEEe9LgBH5z0UDG92/bIMiIc4txf1WMW3av+unxqSz77ST9c6zNQrg7RSvJXYUs3mYlIcLK2kNles7zCnfgV3G1wydtLCs1khp7sy7WLZn58iquf2ONT6T21a+tZv1xZ6uSoXWNHjFOCLcyKCmCoy2O2VVUrVvOZXWNel/qktquSa/qDgS9GDcePQrNzVgHui3jsgNgr4LUkV07MIlE0iMJs5j4zUWDCDH7TzsC1XU9Y1iynroU2aISWWJECP28gsaEELrFrbmfDQbBJVnJLN9dgt29Frts93GanC5Kah0kR3laHmrR4Fo0dmOzi5e/20deeT1ltQ7sTer5//j+IKA2ytDWtFsGXpV5fY4PV18IWhb0eOv7gzz++TaOlNVRUNHAGHdt8mNVrd3Uh0rr2l1LLq11MPezbT4lQesczTzxf9v09LKzgaAX46aiYwCY09PVDfnu9mi9xnbRiCQSSU/i+0cm8+WvLzipc6ZlJTP/Z6O8yoKaTnCGZ33ZuwrZjOxkGpqceirUmgNlfLW1EEWBVC93d6/YMPrEhenu5GW7j/PCkr1Mem45T325E1BLlH6zQ32ebvaqr32krI5F24rYVVSNoig+rR3jwi0khKtirCgKDY1O3lt9mK35VbgU+OOXO1GA30wdhNEgKKisp7HZRZU7N/vLLYVMfj6XD9Ye8fmuX2wpJHeP2nd6xZ4SPvrpKJuOevK271+wmX/9eJT/t7l1RHp35cS/4bMcxa5GCBpCQqC6EFY8A9ZIiB/cxSOTSCQ9gV6xYfSKPfFx/ogOs2AQgdXo7uN2bSd6da4akxFLqNlIQ5OT60al89Phch74WM17TokO9Tl/bEYsy3YXoygK3+48rh4TFcoXWwoZ2zeWSYMSeO6bPZTVOsir8LidH/5kq3rfCCsL7jpft6IB4sKtNDtdNDQ5qWt08vnGfP7glT/93e5ipg1NondcGMmRIRRUNPDGigMsWJfHkzOH8MDHmwFaubmf+2Y38eFWcgYnUuhuY6mNyeVSWO4WanvTyTfQ6CqCXox9SmEunQcVh2HwZWAIeqeApINpamoiPz8fu71j17WioqLYtWtXh16zJxASEkJ6ejpmc2DNJM5GjAZBcmRIq25U/jinVzThVpNPURGz0cDApHC25leRmRJJSlQILy3bD/haxqCuG/97Qz6FVXaW7S7mmpFpvHjDOTQ2u7CYDKw/rAZw3fH+es71Sq8CiLCaKK5x8OziPT7b48MtHnd2jQN/4WFacZW0mFAKK+24FCiobODZxXvonxDOkfI6Hxd0k9NFYaWd8tpGXC5F7ymtNc4oq2vU7+m9xt0e+4trqG90Mjw9OqDjO4OgF2O9SYTVqlrGUb3hune7dlCSs5L8/HwiIiLIyMjQo1g7gpqaGiIiTlxgQuJBURTKysrIz8+nb9++XT2cTuXd/xrrU6azLaZnJZEz+KJWa9Fp0aFsza8i3GokNcqTbtXSMh7qLjry/urDVNY3MS1LDXLVAs+y09X9m45WsuloJSaDwGaGKofCzef34authWxwu4r7J9g4UFJHfLgVlztCu6TWQUVdU6txn9MrWh/nT4fK9Spoh8rquGlsb4TwFdXCygacLoW6RicFlQ26GH/0U566/uxVOc37vPyKeuxNTgb4KeZy0YsrATj8zGX+pvaMEPTmoUurSx0SArXFkDpCVt6SnBJ2u524uLgOFWLJqSGEIC4ursO9FN2RQUkRen/n9hBC+A0K++20wWSlRjIlM4nMFI8Ya9HXGpnu0ppvrDyIxWTgwoEJPvutJiMf3nGe/jnGZqHR3UlqQGI40WFmPVBLq0gWF27Rx15S49DziJ+6Iku/jlZ5LC06lGPVdr1GtqKoAWAxYRa9EQb4uqwXrs/T16hLax18tqmAuZ9tA1SrvKy2Uc9jvuC/l3PRiytxuhQe/vcWn65a3YGgF2O9FKbVqpbBtCV28YgkZzNSiLsP8ncRGAMSw/n6vgtJiLDSJ9Z/XWyAyBAz6TGexhc2a2vH6fgB8WS7O0/F2Sw0NHvuER3qMXKGpUVhcrvYtRQqVYwb6Zdg47bxGcwYlkxqVAih7mYWaTFq+c7DXq0c48MtxNosPj2fvVs9vrxsP3nl/iuHDUmJZGdRNaOeXsquIk9U9fFqO//ekM+Nb/4I4OMC96ayvpGCygZcfmp7dwbBL8Zuy9hgMkBDOdgSTnCGRNJ9CQ8PP/FBEkkbGAwCm8XIwET//46enDmUuyb247FLM9u8hhat7e06759gIzrMs3Y/Z0IGi+6/kOgwCzHuBhklNQ7Kah3Eu9tUzv/ZKFbPnaqf4+9FIT7cSozN7NOvOa+8HovJwM/O793q+FvH9dF/HpzkcUd7R39ra8s17uIk3tHfzU4X2/KrsDc5+WxjAROeWabXAe9sgn7N2GV3gNGIaHS7JMKlGEskkp7LhicvxtCGV2F6VjLTs5LbPV/rVBVrs/C7sSFUhaUREWLWxdhiMhARYiYiRP1sNAiSIqwUVDZQXtdIv3j/LwJDUyNbbYuPsBIbZqGivonb3v6Jv91wDvuLa+kVE8rTV2Vz0ZAkbn9nHXNnZFJjb+bS7BTeX6OmQXmneG3N97ikdxZ6flYUhYMlHjHee7yWy19ZxY1jexMdZsZsFAFFsncEwW8Z2+1q8FadGuou3dSSYEBRFB5++GGGDRtGdnY2H3/8MQBFRUVMnDiRc845h2HDhvH999/jdDq5/fbb9WP/9re/dfHoJV1JiNnYqnznyaCV44yzWciMNfLoJaoVrbmp/YlX/8RwDpbUUlbbSFy4/5idllXGtHto3bRW7C3hs435rD5QplcvyxmcyOrfTeGuif14aPpg+id6yoJ6W+5rD5bpP+8o9Lis8ysaOFTqqTq2+oCaZ705r5KSGgfx4daAGoZ0BMFvGTvsavBWnZr0TrgUY8np89SXO9hZ2DHVfZxOJ0ajkaGpkfzh8qwTnwB89tlnbN68mS1btlBaWsqYMWOYOHEiH374IdOnT+fxxx/H6XRSX1/P5s2bKSgoYPv27QBUVlae4OoSSdt43NS+QWWaZexXjBPCWbg+j4Ymp17IxB8GAd5LtPERVh9RffprNQXwoqGecsapXlHhVpMngM27atnBUo/1u9Nr/XhnUbXPmrNW9CQixERprW/J0M6mB1jGDkSIFWrdYizXjCVBwKpVq7jxxhsxGo0kJSUxadIk1q1bx5gxY3jnnXeYN28e27ZtIyIign79+nHw4EHuvfdeFi9eTGRka3egRBIoWjeolhauJsL+xdhGfaMTRVELgbTFpEHq8zk+3KK6u62mVhZzRIiJ8/u1XUVl6YOTWPXoZGxW38jywUkRGA3CR4wPl9ZRXOOJyP/hgGpBl9Y6dMv4TNEjLGODNcTLTS3FWHL6BGrBBsKp5Bm31V1n4sSJrFy5kq+//ppbbrmFhx9+mFtvvZUtW7bwzTff8Oqrr7Jw4ULefvvtjhi6pAcyMCkcq8nA4OQI6g57tseEteOm9qqj3Z61+fJN57LucDn/+P4gh0rqEEIQGaLK1K3j+jAmI5YR6dE+FnBLBriD09KiQ/nf20bz4MItVDU0kRQVQq2jmYLKBkLNRkItRg6X1XG82kFadCgFlQ16Dez8igaiQs0Mc6ddnQmCXowVRyPCaoGdX0BkGlhlcQXJ2c/EiRN54403uO222ygvL2flypU899xzHDlyhLS0NO68807q6urYuHEjl156KRaLhWuvvZb+/ftz++23d/XwJWcxSZEhbH9qOmajgdzDnu2amzrajxgP8Irenjy47aXCcKuJyYMTySuv14t5jOwdw4d3nsd5feP0giCBIIRg6pAk4sItVDU0keAuzVlQ2UBMmJnkqBAOldZRXONgWFqkfr+MuDAOl9VTUnNm3dTBL8Z2OwbFAQXr4YpXQOYmSoKAq6++mjVr1jBixAiEEDz77LMkJyfz3nvv8dxzz2E2mwkPD+f999+noKCAOXPm4HKpb/1//etfu3j0krMdrbeyN5oYt+wwBWqP5WdnDWfCgHg9r7g9bh2X4fO5vXaTJ0Kz1BMirESHmVl9oIzIUDMZ8Ta+31dKWZ2DAQnh5O5RlzJzBify7urDgOouP1MEJMZCiEuAvwNG4B+KojzTYv/NwKPuj7XALxVF2dKRAz1VXA4HAndbrmHXdu1gJJLTpLZWjfwUQvDcc8/x3HPP+ey/7bbbuO2221qdt3HjxjMyPknPJaqdaGqA60f3OpPD0Yn2EmNtbFUNTWTE2fhsYwEAGfGeKOwrz0nVxTghgJrgHcUJA7iEEEbgVWAGMBS4UQgxtMVhh4BJiqIMB/4EvNnRAz1VFLsdIRrVtWJL29VnJBKJRHLqxNosXJKVzAUDT92K7Qy8LeOh7nKgRVV2+noJsHdO8oj0aN0ijgk7c01IAommHgvsVxTloKIojcAC4ErvAxRFWa0oitZM8kcgvWOHeeq4HG43dXTrai0SiUQi6RiMBsHrt4xiTMYp9ovsJHQxDrfqa9eJEVYu9HppSPJqO2kwCBbcdT4XDoxnWHr3CuBKA/K8PucD57VxLMDPgf/42yGEuAu4CyApKYnc3NzARhkAtbW1fq8XV1mJOaqW4sYEdnbg/Xoabc1vTyIqKoqampoOv67T6eyU6/YE7Hb7Cf9dyn+7ncfZMLeVxWopzUO7tuDIM3D/uVbSwg1s/mk1ViM4nHBg+0am9jYRbhb69/l5f9j44w9nbJyBiLG/iCe/eRVCiMmoYnyBv/2KoryJ24U9evRoJScnJ7BRBkBubi7+rrcXgVnYSRw4isQOvF9Po6357Uns2rWrU1odyhaKp05ISAgjR45s9xj5b7fzOBvmtiwinyV527ni4omEW03keO37ZlgdX24p5KopA7i6i4N7AxHjfMB75T0dKGx5kBBiOPAPYIaiKGUt93cVir0BYWiWbmqJRCLpgVw1Mo1JgxNatYwENXDr3qkDu2BUrQlkzXgdMFAI0VcIYQFmA194HyCE6A18BtyiKMrejh/mqaM4HBgMihRjiUQi6YEYDeKMVtI6VU5oGSuK0iyE+DXwDWpq09uKouwQQtzt3v868HsgDnjN3WO0WVGU0Z037MBQXC6UpmaEEUjquIpJEolEIpF0JAHlGSuKsghY1GLb614/3wHc0bFDO30Uh5pfLCJiIKrbBHhLJN2e5uZmTKagrwkkkXQbgrpRhKteLZBgSOoeawISSUdw1VVXMWrUKLKysnjzTTWlf/HixZx77rmMGDGCqVPVhu21tbXMmTOH7Oxshg8fzqeffgpAeLinNOEnn3yil8e8/fbbefDBB5k8eTKPPvooP/30E+PHj2fkyJGMHz+5+cN1AAAQ2ElEQVSePXv2AGr090MPPaRf9+WXX+a7777j6quv1q+7ZMkSrrnmmjMxHRJJUBDUr77KMXX5WiT17+KRSIKO//wOjm3rkEuFOpvBaILkbJjxzAmPf/vtt4mNjaWhoYExY8Zw5ZVXcuedd7Jy5Ur69u1LeXk5AH/605+Iiopi2zZ1nBUVFe1dFoC9e/eydOlSjEYj1dXVrFy5EpPJxNKlS3nsscf49NNPefPNNzl06BCbNm3CZDJRXl5OTEwMv/rVrygpKSEhIYF33nmHOXPmnN7ESCQ9iKAVY8Xlom7NGgBMialdPBqJpON46aWX+PzzzwHIy8vjzTffZOLEifTt2xeA2Fi16MLSpUtZsGCBfl5MTMwJr33ddddhNKq1g6uqqrjtttvYt28fQgiampr069599926G1u73y233MK//vUv5syZw5o1a3j//fc76BtLJMFP0IrxkZ/dQsPGjYTGNRI+aWJXD0cSbARgwQZKw0nkGefm5rJ06VLWrFlDWFgYOTk5jBgxQnche6MoCsJP7qT3Nrvd7rPPZvOUCHzyySeZPHkyn3/+OYcPH9bzSdu67pw5c7j88ssJCQnhuuuuk2vOEslJEJRrxq6GBho2bsQcH0X6heWI8O5VK1UiOVWqqqqIiYkhLCyM3bt38+OPP+JwOFixYgWHDh0C0N3U06ZN45VXXtHP1dzUSUlJ7Nq1C5fLpVvYbd0rLS0NgHfffVffPm3aNF5//XWam5t97peamkpqaipPP/20bNMokZwkQSnGje6HUuJ1YzGFuCCse9VKlUhOlUsuuYTm5maGDx/Ok08+yfnnn09CQgJvvvkm11xzDSNGjOCGG24A4IknnqCiooJhw4YxYsQIli9fDsAzzzzDzJkzmTJlCikpKW3e65FHHmHu3LlMmDABp9Opb7/jjjvo3bs3w4cPZ8SIEXz44Yf6vptvvplevXoxdGjLXjISiaQ9gtKP5DhwAABrrBGqTGAJP8EZEsnZgdVq5T//8Vv6nRkzZvh8Dg8P57333mt13KxZs5g1a1ar7d7WL8C4cePYu9dTw+dPf/oTACaTiRdffJEXX3yx1TVWrVrFnXfeecLvIZFIfAlOMd5/AEwmLLYmCI2FLq45KpH0BEaNGoXNZuOFF17o6qFIJGcdwSnGB/ZjyeiDaKqULmqJ5AyxYcOGrh6CRHLWEpRrxs3HizGnpEJ9hWoZSyQSiUTSjQlKMXbV1mKMCIeGCgg9cW6lRCKRSCRdSdCKsSEsDBrKIUyKsUQikUi6N8G1ZlxfDgUbcFZXYNj5AQwvl5axRCKRSLo9wSXGy/6E8tPbKI5UDIkZ0Gsg9Mvp4kFJJBKJRNI+weWmLtmLq1lNYzKedyv8/FsYcFEXD0oi6Rq8uzO15PDhwwwbNuwMjkYikbRH8IixosDx7bgyrwfAEBnZxQOSSCQSiSQwgsZNbXWUgr0SZ3hf4HsMNll1S9J5/PdP/83u8t0dci2n04nRaCQzNpNHxz7a5nGPPvooffr04Z577gFg3rx5CCFYuXIlFRUVNDU18fTTT3PllVee1P3tdju//OUvWb9+vV5da/LkyezYsYM5c+bQ2NiIy+Xi008/JTU1leuvv578/HycTidPPvmkXn5TIpGcOkEjxuG1hwFwhaYDYGjHRSeRnI3Mnj2b3/zmN7oYL1y4kMWLF/PAAw8QGRlJaWkp559/PldccYXfrkpt8eqrrwKwbds2du/ezbRp09i7dy+vv/46999/PzfffDONjY04nU4WLVpEamoqX3/9NaA2k5BIJKdP0IhxWH0eAC6z2qHJGG5r73CJ5LRoz4I9WWoCbKE4cuRIiouLKSwspKSkhJiYGFJSUnjggQdYuXIlBoOBgoICjh8/TnJycsD3X7VqFffeey8AmZmZ9OnTh7179zJu3Dj+/Oc/k5+fzzXXXMPAgQPJzs7moYce4tFHH2XmzJlceOGFp/y9JRKJh6BZMw5tOAahsbjU/ucYAuwPK5GcTcyaNYtPPvmEjz/+mNmzZ/PBBx9QUlLChg0b2Lx5M0lJSa16FJ8IRVH8br/pppv44osvCA0NZfr06SxbtoxBgwaxYcMGsrOzmTt3Ln/84x874mtJJD2eoLGMQxuKILYvzpoaQLqpJcHJ7NmzufPOOyktLWXFihUsXLiQxMREzGYzy5cv58iRIyd9zYkTJ/LBBx8wZcoU9u7dy9GjRxk8eDAHDx6kX79+3HfffRw8eJCtW7eSmZlJbGwsP/vZzwgPD2/V6UkikZwaQSPGIfZjkD4RV3UdgAzgkgQlWVlZ1NTUkJaWRkpKCjfffDOXX345o0eP5pxzziEzM/Okr3nPPfdw9913k52djclk4t1338VqtfLxxx/zr3/9C7PZTHJyMr///e9Zt24dDz/8MAaDAbPZzPz58zvhW0okPY/gEOPmRkLspRDbD1dhLQiBISy0q0clkXQK27Zt03+Oj49nzZo1fo+rra1t8xoZGRls374dgJCQEL8W7ty5c5k7d67PtunTpzN9+vRTGLVEImmP4FgzrjyKwAWxfXHV1WKw2RCG4PhqEolEIgl+gsMyrjik/h3Tl+ayTRhsMpJaIgHVir7lllt8tlmtVtauXdtFI5JIJP4IDjHuO4m1Wc/T74t1VC9eTNTll3f1iCSSbkF2djabN2/u6mFIJJITEBxibLLA1jxK//1vzGlpJD7ycFePSCKRSCSSgAkOMQZMRUUYo6Lov3TJSVUfkkgkEomkqwmaKCdTURHWgQOlEEskEonkrCMoxFhRFExFRVgG9O/qoUgkEolEctIEhRg3l5RgqK/H2n9AVw9FIuk2tNfPWCKRdC+CQowbDxwAwCotY4mk29Hc3NzVQ5BIuj1BEcBlzcyk8q47GZSV1dVDkfQQjv3lLzh2dUw/42ank3KjEeuQTJIfe6zN4zqyn3FtbS1XXnml3/Pef/99nn/+eYQQDB8+nH/+858cP36cu+++m4MHDwIwf/58UlNTmTlzpl7J6/nnn6e2tpZ58+aRk5PD+PHj+eGHH7jiiisYNGgQTz/9NI2NjcTFxfHBBx+QlJREbW0t9957L+vXr0cIwR/+8AcqKyvZvn07f/vb3wB466232LVrFy+++OJpzbNE0p0JCjE2xcTgOPdcjJGRXT0UiaTT6Mh+xiEhIXz++eetztu5cyd//vOf+eGHH4iPj6e8vByA++67j0mTJvH555/jdDqpra2loqKi3XtUVlayYsUKACoqKvjxxx8RQvCPf/yDZ599lhdeeIE//elPREVF6SU+KyoqsFgsDB8+nGeffRaz2cw777zDG2+8cbrTJ5F0a4JCjCWSM017FuzJ0hX9jBVF4bHHHmt13rJly5g1axbx8Wpf8NjYWACWLVvG+++/D4DRaCQqKuqEYnzDDTfoP+fn53PDDTdQVFREY2Mjffv2BWDp0qUsWLBAPy4mJgaAKVOm8NVXXzFkyBCamprIzs4+4fxIJGczUowlkrMIrZ/xsWPHWvUzNpvNZGRkBNTPuK3zFEUJOD3QZDLhcrn0zy3va/MqS3vvvffy4IMPcsUVV5Cbm8u8efMA2rzfHXfcwV/+8hcyMzOZM2dOQOORSM5mgiKASyLpKcyePZsFCxbwySefMGvWLKqqqk6pn3Fb502dOpWFCxdSVlYGoLupp06dqrdLdDqdVFdXk5SURHFxMWVlZTgcDr766qt275eWlgbAe++9p2+fNm0ar7zyiv5Zs7bPO+888vLy+PDDD7nxxhsDnR6J5KxFirFEchbhr5/x+vXrGT16NB988EHA/YzbOi8rK4vHH3+cSZMmMWLECB588EEA/v73v7N8+XKys7MZNWoUO3bswGw28/vf/57zzjuPmTNntnvvefPmcd1113HhhRfqLnCAJ554goqKCoYNG8aIESNYvny5vu/6669nwoQJuutaIglmhKIoXXLj0aNHK+vXr++w6+Xm5pKTk9Nh15P4IucXdu3axZAhQzr8uoGuGfc0Zs6cyQMPPMDUqVPbPCaQ34n8t9t5yLk9eYQQGxRFGd1yu7SMJRJJt6KyspJBgwYRGhrarhBLJMGEDOCSSIKYs7GfcXR0NHv37u3qYUgkZxQpxhJJECP7GUskZwfSTS2RnARdFWMhaY38XUiCCSnGEkmAhISEUFZWJkWgG6AoCmVlZYSEhHT1UCSSDkG6qSWSAElPTyc/P5+SkpIOva7dbpeicgqEhISQnp7e1cOQSDqEgMRYCHEJ8HfACPxDUZRnWuwX7v2XAvXA7YqibOzgsUokXYrZbNbLOHYkubm5jBw5ssOvK5FIzh5O6KYWQhiBV4EZwFDgRiHE0BaHzQAGuv/cBczv4HFKJBKJRBK0BLJmPBbYryjKQUVRGoEFQMsebVcC7ysqPwLRQoiUDh6rRCKRSCRBSSBinAbkeX3Od2872WMkEolEIpH4IZA1Y38tXFqGkwZyDEKIu1Dd2AC1Qog9Adw/UOKB0g68nsQXOb+dh5zbzkXOb+ch5/bk6eNvYyBinA/08vqcDhSewjEoivIm8GYA9zxphBDr/dX7lHQMcn47Dzm3nYuc385Dzm3HEYibeh0wUAjRVwhhAWYDX7Q45gvgVqFyPlClKEpRB49VIpFIJJKg5ISWsaIozUKIXwPfoKY2va0oyg4hxN3u/a8Di1DTmvajpjbJbuASiUQikQRIQHnGiqIsQhVc722ve/2sAL/q2KGdNJ3i/pboyPntPOTcdi5yfjsPObcdRJf1M5ZIJBKJRKIia1NLJBKJRNLFBIUYCyEuEULsEULsF0L8rqvHczYihHhbCFEshNjutS1WCLFECLHP/XeM17657vneI4SY3jWjPjsQQvQSQiwXQuwSQuwQQtzv3i7n9zQRQoQIIX4SQmxxz+1T7u1ybjsIIYRRCLFJCPGV+7Oc207grBfjAMt1Sk7Mu8AlLbb9DvhOUZSBwHfuz7jndzaQ5T7nNffvQeKfZuC3iqIMAc4HfuWeQzm/p48DmKIoygjgHOASd0aHnNuO435gl9dnObedwFkvxgRWrlNyAhRFWQmUt9h8JfCe++f3gKu8ti9QFMWhKMoh1Cj6sWdkoGchiqIUaY1TFEWpQX2wpSHn97Rxl+CtdX80u/8oyLntEIQQ6cBlwD+8Nsu57QSCQYxlKc7OI0nLF3f/nejeLuf8FBFCZAAjgbXI+e0Q3G7UzUAxsERRFDm3Hcf/AI8ALq9tcm47gWAQ44BKcUo6FDnnp4AQIhz4FPiNoijV7R3qZ5uc3zZQFMWpKMo5qJX/xgohhrVzuJzbABFCzASKFUXZEOgpfrbJuQ2QYBDjgEpxSk6J41r3Lfffxe7tcs5PEiGEGVWIP1AU5TP3Zjm/HYiiKJVALup6pZzb02cCcIUQ4jDq8t8UIcS/kHPbKQSDGAdSrlNyanwB3Ob++Tbg/3ltny2EsAoh+qL2sf6pC8Z3ViCEEMD/ArsURXnRa5ec39NECJEghIh2/xwKXATsRs7taaMoylxFUdIVRclAfa4uUxTlZ8i57RQCqsDVnWmrXGcXD+usQwjxEZADxAsh8oE/AM8AC4UQPweOAtcBuMuhLgR2okYK/0pRFGeXDPzsYAJwC7DNvbYJ8BhyfjuCFOA9d9SuAVioKMpXQog1yLntLOS/205AVuCSSCQSiaSLCQY3tUQikUgkZzVSjCUSiUQi6WKkGEskEolE0sVIMZZIJBKJpIuRYiyRSCQSSRcjxVgikUgkki5GirFEIpFIJF2MFGOJRCKRSLqY/w8QwBIxULkwvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQCvPGZks9v"
   },
   "outputs": [],
   "source": [
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'data/cvision/model_9-6.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3158 - accuracy: 0.8878\n",
      "loss= 0.3157714605331421\n",
      "accuracy= 0.8878048658370972\n"
     ]
    }
   ],
   "source": [
    "#모델 불러와서 정확도 확인 및 예측\n",
    "hdf5_file = './data/cvision/model_9-6/model_9-6ep455-vl0.3158.hdf5'\n",
    "model.load_weights(hdf5_file)\n",
    "\n",
    "score = model.evaluate(valid_X, valid_y)\n",
    "print('loss=', score[0])        # loss\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8nb5jokyny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-dbea146f3fce>:6: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "#예측 진행\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255.\n",
    "\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('data/cvision/my_subm_9-5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_653 (Conv2D)          (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_654 (Conv2D)          (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_655 (Conv2D)          (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_656 (Conv2D)          (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_657 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_658 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_659 (Conv2D)          (None, 14, 14, 48)        13872     \n",
      "_________________________________________________________________\n",
      "conv2d_660 (Conv2D)          (None, 7, 7, 48)          20784     \n",
      "_________________________________________________________________\n",
      "conv2d_661 (Conv2D)          (None, 4, 4, 48)          20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 50)                9650      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 93,536\n",
      "Trainable params: 93,536\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model_9-7: \n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), Adam(0.001)\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1598347122783,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "tPFielyJ5yMH"
   },
   "outputs": [],
   "source": [
    "#earlystopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=150)\n",
    "\n",
    "#modelcheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "model_num = 'model_9-7'\n",
    "MODEL_SAVE_FOLDER_PATH = './data/cvision/' + model_num + '/'\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + model_num +'ep{epoch:02d}-vl{val_loss:.4f}.hdf5'\n",
    "\n",
    "cp = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537462,
     "status": "error",
     "timestamp": 1598347660504,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "yeNCbX2vko4s",
    "outputId": "012ce4a9-2ca8-41d3-d1de-9331032469e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.3482 - accuracy: 0.0956\n",
      "Epoch 00001: val_loss improved from inf to 2.30263, saving model to ./data/cvision/model_9-7\\model_9-7ep01-vl2.3026.hdf5\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.3482 - accuracy: 0.0956 - val_loss: 2.3026 - val_accuracy: 0.0927\n",
      "Epoch 2/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2993 - accuracy: 0.1191\n",
      "Epoch 00002: val_loss improved from 2.30263 to 2.30030, saving model to ./data/cvision/model_9-7\\model_9-7ep02-vl2.3003.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2988 - accuracy: 0.1207 - val_loss: 2.3003 - val_accuracy: 0.0976\n",
      "Epoch 3/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2962 - accuracy: 0.1090\n",
      "Epoch 00003: val_loss improved from 2.30030 to 2.29465, saving model to ./data/cvision/model_9-7\\model_9-7ep03-vl2.2946.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2961 - accuracy: 0.1108 - val_loss: 2.2946 - val_accuracy: 0.1024\n",
      "Epoch 4/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2913 - accuracy: 0.1238\n",
      "Epoch 00004: val_loss improved from 2.29465 to 2.28493, saving model to ./data/cvision/model_9-7\\model_9-7ep04-vl2.2849.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2913 - accuracy: 0.1248 - val_loss: 2.2849 - val_accuracy: 0.1659\n",
      "Epoch 5/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2781 - accuracy: 0.1358\n",
      "Epoch 00005: val_loss improved from 2.28493 to 2.25866, saving model to ./data/cvision/model_9-7\\model_9-7ep05-vl2.2587.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2798 - accuracy: 0.1339 - val_loss: 2.2587 - val_accuracy: 0.1756\n",
      "Epoch 6/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2581 - accuracy: 0.1638\n",
      "Epoch 00006: val_loss improved from 2.25866 to 2.25343, saving model to ./data/cvision/model_9-7\\model_9-7ep06-vl2.2534.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2581 - accuracy: 0.1638 - val_loss: 2.2534 - val_accuracy: 0.1415\n",
      "Epoch 7/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2502 - accuracy: 0.1569\n",
      "Epoch 00007: val_loss improved from 2.25343 to 2.18490, saving model to ./data/cvision/model_9-7\\model_9-7ep07-vl2.1849.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2502 - accuracy: 0.1569 - val_loss: 2.1849 - val_accuracy: 0.1951\n",
      "Epoch 8/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2232 - accuracy: 0.1767\n",
      "Epoch 00008: val_loss improved from 2.18490 to 2.15575, saving model to ./data/cvision/model_9-7\\model_9-7ep08-vl2.1557.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2232 - accuracy: 0.1767 - val_loss: 2.1557 - val_accuracy: 0.2585\n",
      "Epoch 9/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.1450 - accuracy: 0.1983\n",
      "Epoch 00009: val_loss improved from 2.15575 to 1.97571, saving model to ./data/cvision/model_9-7\\model_9-7ep09-vl1.9757.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.1450 - accuracy: 0.1983 - val_loss: 1.9757 - val_accuracy: 0.3024\n",
      "Epoch 10/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0983 - accuracy: 0.2128\n",
      "Epoch 00010: val_loss improved from 1.97571 to 1.90587, saving model to ./data/cvision/model_9-7\\model_9-7ep10-vl1.9059.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.0983 - accuracy: 0.2128 - val_loss: 1.9059 - val_accuracy: 0.3512\n",
      "Epoch 11/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0622 - accuracy: 0.2490\n",
      "Epoch 00011: val_loss did not improve from 1.90587\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 2.0622 - accuracy: 0.2490 - val_loss: 1.9175 - val_accuracy: 0.3561\n",
      "Epoch 12/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0252 - accuracy: 0.2590\n",
      "Epoch 00012: val_loss improved from 1.90587 to 1.77472, saving model to ./data/cvision/model_9-7\\model_9-7ep12-vl1.7747.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.0256 - accuracy: 0.2566 - val_loss: 1.7747 - val_accuracy: 0.3317\n",
      "Epoch 13/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9885 - accuracy: 0.2851\n",
      "Epoch 00013: val_loss did not improve from 1.77472\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.9885 - accuracy: 0.2851 - val_loss: 1.7977 - val_accuracy: 0.3756\n",
      "Epoch 14/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.9583 - accuracy: 0.2770\n",
      "Epoch 00014: val_loss improved from 1.77472 to 1.74924, saving model to ./data/cvision/model_9-7\\model_9-7ep14-vl1.7492.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.9576 - accuracy: 0.2801 - val_loss: 1.7492 - val_accuracy: 0.4146\n",
      "Epoch 15/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.8965 - accuracy: 0.3314\n",
      "Epoch 00015: val_loss improved from 1.74924 to 1.61741, saving model to ./data/cvision/model_9-7\\model_9-7ep15-vl1.6174.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.8972 - accuracy: 0.3289 - val_loss: 1.6174 - val_accuracy: 0.4098\n",
      "Epoch 16/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.8494 - accuracy: 0.3296\n",
      "Epoch 00016: val_loss improved from 1.61741 to 1.58385, saving model to ./data/cvision/model_9-7\\model_9-7ep16-vl1.5838.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.8511 - accuracy: 0.3306 - val_loss: 1.5838 - val_accuracy: 0.5171\n",
      "Epoch 17/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.8374 - accuracy: 0.3371\n",
      "Epoch 00017: val_loss improved from 1.58385 to 1.53292, saving model to ./data/cvision/model_9-7\\model_9-7ep17-vl1.5329.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.8387 - accuracy: 0.3376 - val_loss: 1.5329 - val_accuracy: 0.5707\n",
      "Epoch 18/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7852 - accuracy: 0.3796\n",
      "Epoch 00018: val_loss improved from 1.53292 to 1.39348, saving model to ./data/cvision/model_9-7\\model_9-7ep18-vl1.3935.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.7852 - accuracy: 0.3796 - val_loss: 1.3935 - val_accuracy: 0.6000\n",
      "Epoch 19/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7485 - accuracy: 0.3907 ETA: 0s - loss: 1.7565 - accuracy: \n",
      "Epoch 00019: val_loss improved from 1.39348 to 1.34655, saving model to ./data/cvision/model_9-7\\model_9-7ep19-vl1.3466.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.7485 - accuracy: 0.3907 - val_loss: 1.3466 - val_accuracy: 0.6000\n",
      "Epoch 20/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7034 - accuracy: 0.4035\n",
      "Epoch 00020: val_loss improved from 1.34655 to 1.33531, saving model to ./data/cvision/model_9-7\\model_9-7ep20-vl1.3353.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.7034 - accuracy: 0.4035 - val_loss: 1.3353 - val_accuracy: 0.6244\n",
      "Epoch 21/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.6350 - accuracy: 0.4297\n",
      "Epoch 00021: val_loss improved from 1.33531 to 1.23018, saving model to ./data/cvision/model_9-7\\model_9-7ep21-vl1.2302.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.6356 - accuracy: 0.4274 - val_loss: 1.2302 - val_accuracy: 0.6537\n",
      "Epoch 22/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6317 - accuracy: 0.4227\n",
      "Epoch 00022: val_loss did not improve from 1.23018\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.6317 - accuracy: 0.4227 - val_loss: 1.2750 - val_accuracy: 0.6049\n",
      "Epoch 23/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.5708 - accuracy: 0.4579\n",
      "Epoch 00023: val_loss did not improve from 1.23018\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5718 - accuracy: 0.4577 - val_loss: 1.2566 - val_accuracy: 0.6098\n",
      "Epoch 24/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.4595\n",
      "Epoch 00024: val_loss improved from 1.23018 to 1.14172, saving model to ./data/cvision/model_9-7\\model_9-7ep24-vl1.1417.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.5291 - accuracy: 0.4595 - val_loss: 1.1417 - val_accuracy: 0.6195\n",
      "Epoch 25/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 1.4983 - accuracy: 0.4816\n",
      "Epoch 00025: val_loss improved from 1.14172 to 1.06923, saving model to ./data/cvision/model_9-7\\model_9-7ep25-vl1.0692.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4983 - accuracy: 0.4816 - val_loss: 1.0692 - val_accuracy: 0.6390\n",
      "Epoch 26/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.4196 - accuracy: 0.5173\n",
      "Epoch 00026: val_loss did not improve from 1.06923\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4194 - accuracy: 0.5184 - val_loss: 1.1408 - val_accuracy: 0.6488\n",
      "Epoch 27/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4692 - accuracy: 0.4939\n",
      "Epoch 00027: val_loss improved from 1.06923 to 1.05894, saving model to ./data/cvision/model_9-7\\model_9-7ep27-vl1.0589.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.4692 - accuracy: 0.4939 - val_loss: 1.0589 - val_accuracy: 0.6732\n",
      "Epoch 28/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.3785 - accuracy: 0.5246\n",
      "Epoch 00028: val_loss improved from 1.05894 to 1.02115, saving model to ./data/cvision/model_9-7\\model_9-7ep28-vl1.0211.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.3634 - accuracy: 0.5329 - val_loss: 1.0211 - val_accuracy: 0.6634\n",
      "Epoch 29/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4319 - accuracy: 0.5015\n",
      "Epoch 00029: val_loss did not improve from 1.02115\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.4319 - accuracy: 0.5015 - val_loss: 1.0262 - val_accuracy: 0.7073\n",
      "Epoch 30/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3888 - accuracy: 0.5219\n",
      "Epoch 00030: val_loss did not improve from 1.02115\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3888 - accuracy: 0.5219 - val_loss: 1.0409 - val_accuracy: 0.6732\n",
      "Epoch 31/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3536 - accuracy: 0.5294\n",
      "Epoch 00031: val_loss did not improve from 1.02115\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3536 - accuracy: 0.5294 - val_loss: 1.0504 - val_accuracy: 0.6585\n",
      "Epoch 32/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3077 - accuracy: 0.5417\n",
      "Epoch 00032: val_loss did not improve from 1.02115\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3077 - accuracy: 0.5417 - val_loss: 1.0479 - val_accuracy: 0.6829\n",
      "Epoch 33/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2960 - accuracy: 0.5609\n",
      "Epoch 00033: val_loss improved from 1.02115 to 0.96027, saving model to ./data/cvision/model_9-7\\model_9-7ep33-vl0.9603.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2960 - accuracy: 0.5609 - val_loss: 0.9603 - val_accuracy: 0.6927\n",
      "Epoch 34/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3102 - accuracy: 0.5499\n",
      "Epoch 00034: val_loss did not improve from 0.96027\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3102 - accuracy: 0.5499 - val_loss: 1.0590 - val_accuracy: 0.6585\n",
      "Epoch 35/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2735 - accuracy: 0.5661\n",
      "Epoch 00035: val_loss did not improve from 0.96027\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.2743 - accuracy: 0.5650 - val_loss: 0.9757 - val_accuracy: 0.6829\n",
      "Epoch 36/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2770 - accuracy: 0.5607\n",
      "Epoch 00036: val_loss did not improve from 0.96027\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2723 - accuracy: 0.5653 - val_loss: 0.9613 - val_accuracy: 0.6829\n",
      "Epoch 37/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2203 - accuracy: 0.5901\n",
      "Epoch 00037: val_loss improved from 0.96027 to 0.96013, saving model to ./data/cvision/model_9-7\\model_9-7ep37-vl0.9601.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2203 - accuracy: 0.5901 - val_loss: 0.9601 - val_accuracy: 0.6927\n",
      "Epoch 38/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2347 - accuracy: 0.5767\n",
      "Epoch 00038: val_loss improved from 0.96013 to 0.95777, saving model to ./data/cvision/model_9-7\\model_9-7ep38-vl0.9578.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2347 - accuracy: 0.5767 - val_loss: 0.9578 - val_accuracy: 0.6976\n",
      "Epoch 39/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2390 - accuracy: 0.5778\n",
      "Epoch 00039: val_loss did not improve from 0.95777\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2390 - accuracy: 0.5778 - val_loss: 1.0377 - val_accuracy: 0.6439\n",
      "Epoch 40/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2428 - accuracy: 0.5738\n",
      "Epoch 00040: val_loss did not improve from 0.95777\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2428 - accuracy: 0.5738 - val_loss: 0.9912 - val_accuracy: 0.6537\n",
      "Epoch 41/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1827 - accuracy: 0.5936\n",
      "Epoch 00041: val_loss improved from 0.95777 to 0.84483, saving model to ./data/cvision/model_9-7\\model_9-7ep41-vl0.8448.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1827 - accuracy: 0.5936 - val_loss: 0.8448 - val_accuracy: 0.7073\n",
      "Epoch 42/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1849 - accuracy: 0.5825\n",
      "Epoch 00042: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1849 - accuracy: 0.5825 - val_loss: 0.9422 - val_accuracy: 0.6488\n",
      "Epoch 43/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1880 - accuracy: 0.5883\n",
      "Epoch 00043: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1880 - accuracy: 0.5883 - val_loss: 0.8878 - val_accuracy: 0.7024\n",
      "Epoch 44/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1214 - accuracy: 0.6064\n",
      "Epoch 00044: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1214 - accuracy: 0.6064 - val_loss: 0.8473 - val_accuracy: 0.7122\n",
      "Epoch 45/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1676 - accuracy: 0.5994\n",
      "Epoch 00045: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1676 - accuracy: 0.5994 - val_loss: 0.8496 - val_accuracy: 0.7171\n",
      "Epoch 46/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1382 - accuracy: 0.6172\n",
      "Epoch 00046: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1360 - accuracy: 0.6155 - val_loss: 0.8686 - val_accuracy: 0.7122\n",
      "Epoch 47/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1175 - accuracy: 0.6131\n",
      "Epoch 00047: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1223 - accuracy: 0.6093 - val_loss: 1.0069 - val_accuracy: 0.6634\n",
      "Epoch 48/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1266 - accuracy: 0.6163\n",
      "Epoch 00048: val_loss did not improve from 0.84483\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.1266 - accuracy: 0.6163 - val_loss: 0.8889 - val_accuracy: 0.7220\n",
      "Epoch 49/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1188 - accuracy: 0.6208\n",
      "Epoch 00049: val_loss improved from 0.84483 to 0.82285, saving model to ./data/cvision/model_9-7\\model_9-7ep49-vl0.8229.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.1135 - accuracy: 0.6172 - val_loss: 0.8229 - val_accuracy: 0.7561\n",
      "Epoch 50/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0742 - accuracy: 0.6356\n",
      "Epoch 00050: val_loss did not improve from 0.82285\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0742 - accuracy: 0.6356 - val_loss: 0.8346 - val_accuracy: 0.7317\n",
      "Epoch 51/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0971 - accuracy: 0.6303\n",
      "Epoch 00051: val_loss did not improve from 0.82285\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0971 - accuracy: 0.6303 - val_loss: 0.8254 - val_accuracy: 0.7268\n",
      "Epoch 52/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0500 - accuracy: 0.6362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00052: val_loss improved from 0.82285 to 0.81973, saving model to ./data/cvision/model_9-7\\model_9-7ep52-vl0.8197.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0500 - accuracy: 0.6362 - val_loss: 0.8197 - val_accuracy: 0.7268\n",
      "Epoch 53/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0870 - accuracy: 0.6321\n",
      "Epoch 00053: val_loss improved from 0.81973 to 0.79758, saving model to ./data/cvision/model_9-7\\model_9-7ep53-vl0.7976.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0870 - accuracy: 0.6321 - val_loss: 0.7976 - val_accuracy: 0.7512\n",
      "Epoch 54/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0832 - accuracy: 0.6298\n",
      "Epoch 00054: val_loss did not improve from 0.79758\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.0816 - accuracy: 0.6280 - val_loss: 0.8508 - val_accuracy: 0.7073\n",
      "Epoch 55/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0406 - accuracy: 0.6519\n",
      "Epoch 00055: val_loss did not improve from 0.79758\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0406 - accuracy: 0.6519 - val_loss: 0.8132 - val_accuracy: 0.7317\n",
      "Epoch 56/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0168 - accuracy: 0.6490\n",
      "Epoch 00056: val_loss did not improve from 0.79758\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0338 - accuracy: 0.6437 - val_loss: 0.8201 - val_accuracy: 0.7268\n",
      "Epoch 57/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.6443\n",
      "Epoch 00057: val_loss did not improve from 0.79758\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0333 - accuracy: 0.6443 - val_loss: 0.7986 - val_accuracy: 0.7512\n",
      "Epoch 58/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9952 - accuracy: 0.6659 ETA: 0s - loss: 0.9986 - accuracy\n",
      "Epoch 00058: val_loss improved from 0.79758 to 0.74678, saving model to ./data/cvision/model_9-7\\model_9-7ep58-vl0.7468.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9952 - accuracy: 0.6659 - val_loss: 0.7468 - val_accuracy: 0.7805\n",
      "Epoch 59/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0195 - accuracy: 0.6583\n",
      "Epoch 00059: val_loss improved from 0.74678 to 0.73652, saving model to ./data/cvision/model_9-7\\model_9-7ep59-vl0.7365.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.0195 - accuracy: 0.6583 - val_loss: 0.7365 - val_accuracy: 0.7610\n",
      "Epoch 60/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0055 - accuracy: 0.6612\n",
      "Epoch 00060: val_loss did not improve from 0.73652\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0055 - accuracy: 0.6612 - val_loss: 0.7759 - val_accuracy: 0.7512\n",
      "Epoch 61/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9543 - accuracy: 0.6623\n",
      "Epoch 00061: val_loss improved from 0.73652 to 0.71273, saving model to ./data/cvision/model_9-7\\model_9-7ep61-vl0.7127.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9570 - accuracy: 0.6606 - val_loss: 0.7127 - val_accuracy: 0.7659\n",
      "Epoch 62/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9821 - accuracy: 0.6706\n",
      "Epoch 00062: val_loss did not improve from 0.71273\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9821 - accuracy: 0.6706 - val_loss: 0.8252 - val_accuracy: 0.7317\n",
      "Epoch 63/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0139 - accuracy: 0.6397\n",
      "Epoch 00063: val_loss did not improve from 0.71273\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0139 - accuracy: 0.6397 - val_loss: 0.8098 - val_accuracy: 0.7659\n",
      "Epoch 64/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9848 - accuracy: 0.6647\n",
      "Epoch 00064: val_loss did not improve from 0.71273\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9848 - accuracy: 0.6647 - val_loss: 0.7418 - val_accuracy: 0.7707\n",
      "Epoch 65/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.6560\n",
      "Epoch 00065: val_loss improved from 0.71273 to 0.70288, saving model to ./data/cvision/model_9-7\\model_9-7ep65-vl0.7029.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9753 - accuracy: 0.6560 - val_loss: 0.7029 - val_accuracy: 0.7951\n",
      "Epoch 66/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9945 - accuracy: 0.6700\n",
      "Epoch 00066: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9945 - accuracy: 0.6700 - val_loss: 0.7886 - val_accuracy: 0.7610\n",
      "Epoch 67/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0007 - accuracy: 0.6449\n",
      "Epoch 00067: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0007 - accuracy: 0.6449 - val_loss: 0.7535 - val_accuracy: 0.7707\n",
      "Epoch 68/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9697 - accuracy: 0.6676\n",
      "Epoch 00068: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9697 - accuracy: 0.6676 - val_loss: 0.7182 - val_accuracy: 0.7854\n",
      "Epoch 69/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9992 - accuracy: 0.6589\n",
      "Epoch 00069: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9992 - accuracy: 0.6589 - val_loss: 0.7409 - val_accuracy: 0.7659\n",
      "Epoch 70/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9216 - accuracy: 0.6863\n",
      "Epoch 00070: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9207 - accuracy: 0.6863 - val_loss: 0.7983 - val_accuracy: 0.7317\n",
      "Epoch 71/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.6711\n",
      "Epoch 00071: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9483 - accuracy: 0.6711 - val_loss: 0.7601 - val_accuracy: 0.7512\n",
      "Epoch 72/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.6892\n",
      "Epoch 00072: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9567 - accuracy: 0.6892 - val_loss: 0.7714 - val_accuracy: 0.7317\n",
      "Epoch 73/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9276 - accuracy: 0.6736\n",
      "Epoch 00073: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9128 - accuracy: 0.6781 - val_loss: 0.8691 - val_accuracy: 0.6927\n",
      "Epoch 74/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9320 - accuracy: 0.6950\n",
      "Epoch 00074: val_loss did not improve from 0.70288\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9320 - accuracy: 0.6950 - val_loss: 0.7304 - val_accuracy: 0.7659\n",
      "Epoch 75/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.6892\n",
      "Epoch 00075: val_loss improved from 0.70288 to 0.69406, saving model to ./data/cvision/model_9-7\\model_9-7ep75-vl0.6941.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9121 - accuracy: 0.6892 - val_loss: 0.6941 - val_accuracy: 0.7756\n",
      "Epoch 76/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9240 - accuracy: 0.6974\n",
      "Epoch 00076: val_loss improved from 0.69406 to 0.68797, saving model to ./data/cvision/model_9-7\\model_9-7ep76-vl0.6880.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9240 - accuracy: 0.6974 - val_loss: 0.6880 - val_accuracy: 0.8000\n",
      "Epoch 77/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9098 - accuracy: 0.6816\n",
      "Epoch 00077: val_loss did not improve from 0.68797\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9098 - accuracy: 0.6816 - val_loss: 0.7254 - val_accuracy: 0.7512\n",
      "Epoch 78/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8808 - accuracy: 0.6857\n",
      "Epoch 00078: val_loss improved from 0.68797 to 0.64190, saving model to ./data/cvision/model_9-7\\model_9-7ep78-vl0.6419.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8808 - accuracy: 0.6857 - val_loss: 0.6419 - val_accuracy: 0.7902\n",
      "Epoch 79/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9659 - accuracy: 0.6673\n",
      "Epoch 00079: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9572 - accuracy: 0.6717 - val_loss: 0.7352 - val_accuracy: 0.7756\n",
      "Epoch 80/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9021 - accuracy: 0.6931\n",
      "Epoch 00080: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9076 - accuracy: 0.6886 - val_loss: 0.7379 - val_accuracy: 0.7707\n",
      "Epoch 81/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6927\n",
      "Epoch 00081: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9124 - accuracy: 0.6927 - val_loss: 0.6954 - val_accuracy: 0.7951\n",
      "Epoch 82/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9261 - accuracy: 0.6939\n",
      "Epoch 00082: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9261 - accuracy: 0.6939 - val_loss: 0.7923 - val_accuracy: 0.7610\n",
      "Epoch 83/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9126 - accuracy: 0.6915\n",
      "Epoch 00083: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9126 - accuracy: 0.6915 - val_loss: 0.6808 - val_accuracy: 0.7707\n",
      "Epoch 84/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.6904\n",
      "Epoch 00084: val_loss did not improve from 0.64190\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9120 - accuracy: 0.6904 - val_loss: 0.7343 - val_accuracy: 0.7805\n",
      "Epoch 85/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8865 - accuracy: 0.6933\n",
      "Epoch 00085: val_loss improved from 0.64190 to 0.63819, saving model to ./data/cvision/model_9-7\\model_9-7ep85-vl0.6382.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8865 - accuracy: 0.6933 - val_loss: 0.6382 - val_accuracy: 0.8098\n",
      "Epoch 86/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8395 - accuracy: 0.7143\n",
      "Epoch 00086: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8395 - accuracy: 0.7143 - val_loss: 0.6456 - val_accuracy: 0.7854\n",
      "Epoch 87/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8908 - accuracy: 0.6933\n",
      "Epoch 00087: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8908 - accuracy: 0.6933 - val_loss: 0.6728 - val_accuracy: 0.7854\n",
      "Epoch 88/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8786 - accuracy: 0.6933\n",
      "Epoch 00088: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8786 - accuracy: 0.6933 - val_loss: 0.6581 - val_accuracy: 0.7951\n",
      "Epoch 89/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8597 - accuracy: 0.6997\n",
      "Epoch 00089: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8597 - accuracy: 0.6997 - val_loss: 0.6834 - val_accuracy: 0.7756\n",
      "Epoch 90/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8500 - accuracy: 0.7224\n",
      "Epoch 00090: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8500 - accuracy: 0.7224 - val_loss: 0.6660 - val_accuracy: 0.7805\n",
      "Epoch 91/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8794 - accuracy: 0.6910\n",
      "Epoch 00091: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8794 - accuracy: 0.6910 - val_loss: 0.6903 - val_accuracy: 0.8000\n",
      "Epoch 92/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8513 - accuracy: 0.7026\n",
      "Epoch 00092: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8513 - accuracy: 0.7026 - val_loss: 0.6782 - val_accuracy: 0.7756\n",
      "Epoch 93/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8518 - accuracy: 0.7067\n",
      "Epoch 00093: val_loss did not improve from 0.63819\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8518 - accuracy: 0.7067 - val_loss: 0.7552 - val_accuracy: 0.7512\n",
      "Epoch 94/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8730 - accuracy: 0.7076\n",
      "Epoch 00094: val_loss improved from 0.63819 to 0.61783, saving model to ./data/cvision/model_9-7\\model_9-7ep94-vl0.6178.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8737 - accuracy: 0.7050 - val_loss: 0.6178 - val_accuracy: 0.8000\n",
      "Epoch 95/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8345 - accuracy: 0.7131\n",
      "Epoch 00095: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8345 - accuracy: 0.7131 - val_loss: 0.6498 - val_accuracy: 0.7805\n",
      "Epoch 96/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8426 - accuracy: 0.7125\n",
      "Epoch 00096: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8426 - accuracy: 0.7125 - val_loss: 0.6854 - val_accuracy: 0.7902\n",
      "Epoch 97/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8016 - accuracy: 0.7184\n",
      "Epoch 00097: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8016 - accuracy: 0.7184 - val_loss: 0.6880 - val_accuracy: 0.7756\n",
      "Epoch 98/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.7120\n",
      "Epoch 00098: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8390 - accuracy: 0.7120 - val_loss: 0.6848 - val_accuracy: 0.7756\n",
      "Epoch 99/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8236 - accuracy: 0.7259\n",
      "Epoch 00099: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8236 - accuracy: 0.7259 - val_loss: 0.6390 - val_accuracy: 0.8000\n",
      "Epoch 100/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8579 - accuracy: 0.6953\n",
      "Epoch 00100: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8514 - accuracy: 0.6980 - val_loss: 0.6485 - val_accuracy: 0.7951\n",
      "Epoch 101/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.7271\n",
      "Epoch 00101: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8140 - accuracy: 0.7271 - val_loss: 0.7056 - val_accuracy: 0.7707\n",
      "Epoch 102/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8149 - accuracy: 0.7160\n",
      "Epoch 00102: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8149 - accuracy: 0.7160 - val_loss: 0.6578 - val_accuracy: 0.7707\n",
      "Epoch 103/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7817 - accuracy: 0.7347\n",
      "Epoch 00103: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7817 - accuracy: 0.7347 - val_loss: 0.6826 - val_accuracy: 0.7902\n",
      "Epoch 104/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8267 - accuracy: 0.7227\n",
      "Epoch 00104: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8220 - accuracy: 0.7236 - val_loss: 0.6421 - val_accuracy: 0.8000\n",
      "Epoch 105/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8002 - accuracy: 0.7218\n",
      "Epoch 00105: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7979 - accuracy: 0.7236 - val_loss: 0.7001 - val_accuracy: 0.7707\n",
      "Epoch 106/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.7242\n",
      "Epoch 00106: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7952 - accuracy: 0.7242 - val_loss: 0.6724 - val_accuracy: 0.7805\n",
      "Epoch 107/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8187 - accuracy: 0.7201\n",
      "Epoch 00107: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8187 - accuracy: 0.7201 - val_loss: 0.7014 - val_accuracy: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8095 - accuracy: 0.7219\n",
      "Epoch 00108: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8095 - accuracy: 0.7219 - val_loss: 0.7730 - val_accuracy: 0.7220\n",
      "Epoch 109/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8274 - accuracy: 0.7171\n",
      "Epoch 00109: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8327 - accuracy: 0.7143 - val_loss: 0.6536 - val_accuracy: 0.7902\n",
      "Epoch 110/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7718 - accuracy: 0.7271\n",
      "Epoch 00110: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7718 - accuracy: 0.7271 - val_loss: 0.6809 - val_accuracy: 0.7756\n",
      "Epoch 111/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.7283\n",
      "Epoch 00111: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7804 - accuracy: 0.7283 - val_loss: 0.6800 - val_accuracy: 0.7756\n",
      "Epoch 112/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.7283\n",
      "Epoch 00112: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8232 - accuracy: 0.7283 - val_loss: 0.6308 - val_accuracy: 0.7854\n",
      "Epoch 113/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.7172\n",
      "Epoch 00113: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7981 - accuracy: 0.7172 - val_loss: 0.6344 - val_accuracy: 0.7951\n",
      "Epoch 114/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7868 - accuracy: 0.7277\n",
      "Epoch 00114: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7868 - accuracy: 0.7277 - val_loss: 0.6797 - val_accuracy: 0.7756\n",
      "Epoch 115/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7357 - accuracy: 0.7499\n",
      "Epoch 00115: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7357 - accuracy: 0.7499 - val_loss: 0.6623 - val_accuracy: 0.7561\n",
      "Epoch 116/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7754 - accuracy: 0.7364\n",
      "Epoch 00116: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7754 - accuracy: 0.7364 - val_loss: 0.6547 - val_accuracy: 0.7854\n",
      "Epoch 117/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8156 - accuracy: 0.7236\n",
      "Epoch 00117: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8156 - accuracy: 0.7236 - val_loss: 0.6628 - val_accuracy: 0.7756\n",
      "Epoch 118/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7797 - accuracy: 0.7353\n",
      "Epoch 00118: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7797 - accuracy: 0.7353 - val_loss: 0.6609 - val_accuracy: 0.7756\n",
      "Epoch 119/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.7434\n",
      "Epoch 00119: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7669 - accuracy: 0.7434 - val_loss: 0.6181 - val_accuracy: 0.8049\n",
      "Epoch 120/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7619 - accuracy: 0.7423\n",
      "Epoch 00120: val_loss did not improve from 0.61783\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7619 - accuracy: 0.7423 - val_loss: 0.6369 - val_accuracy: 0.7902\n",
      "Epoch 121/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8134 - accuracy: 0.7259\n",
      "Epoch 00121: val_loss improved from 0.61783 to 0.61634, saving model to ./data/cvision/model_9-7\\model_9-7ep121-vl0.6163.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.8028 - accuracy: 0.7312 - val_loss: 0.6163 - val_accuracy: 0.7902\n",
      "Epoch 122/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7719 - accuracy: 0.7394\n",
      "Epoch 00122: val_loss improved from 0.61634 to 0.61604, saving model to ./data/cvision/model_9-7\\model_9-7ep122-vl0.6160.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7719 - accuracy: 0.7394 - val_loss: 0.6160 - val_accuracy: 0.8049\n",
      "Epoch 123/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7731 - accuracy: 0.7364\n",
      "Epoch 00123: val_loss improved from 0.61604 to 0.61333, saving model to ./data/cvision/model_9-7\\model_9-7ep123-vl0.6133.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7731 - accuracy: 0.7364 - val_loss: 0.6133 - val_accuracy: 0.8146\n",
      "Epoch 124/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7492 - accuracy: 0.7417\n",
      "Epoch 00124: val_loss improved from 0.61333 to 0.60856, saving model to ./data/cvision/model_9-7\\model_9-7ep124-vl0.6086.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7492 - accuracy: 0.7417 - val_loss: 0.6086 - val_accuracy: 0.8098\n",
      "Epoch 125/3000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.7431 - accuracy: 0.7337\n",
      "Epoch 00125: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.7452 - accuracy: 0.7335 - val_loss: 0.6436 - val_accuracy: 0.8000\n",
      "Epoch 126/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7465 - accuracy: 0.7470\n",
      "Epoch 00126: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7493 - accuracy: 0.7452 - val_loss: 0.6112 - val_accuracy: 0.8098\n",
      "Epoch 127/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7384 - accuracy: 0.7528\n",
      "Epoch 00127: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7384 - accuracy: 0.7528 - val_loss: 0.6486 - val_accuracy: 0.7951\n",
      "Epoch 128/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7851 - accuracy: 0.7388\n",
      "Epoch 00128: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7851 - accuracy: 0.7388 - val_loss: 0.6415 - val_accuracy: 0.8146\n",
      "Epoch 129/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7531 - accuracy: 0.7410\n",
      "Epoch 00129: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7527 - accuracy: 0.7394 - val_loss: 0.6243 - val_accuracy: 0.7951\n",
      "Epoch 130/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7594 - accuracy: 0.7440\n",
      "Epoch 00130: val_loss did not improve from 0.60856\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7594 - accuracy: 0.7440 - val_loss: 0.6744 - val_accuracy: 0.7854\n",
      "Epoch 131/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7634 - accuracy: 0.7376\n",
      "Epoch 00131: val_loss improved from 0.60856 to 0.60107, saving model to ./data/cvision/model_9-7\\model_9-7ep131-vl0.6011.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7634 - accuracy: 0.7376 - val_loss: 0.6011 - val_accuracy: 0.8098\n",
      "Epoch 132/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7658 - accuracy: 0.7332\n",
      "Epoch 00132: val_loss did not improve from 0.60107\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7672 - accuracy: 0.7312 - val_loss: 0.6235 - val_accuracy: 0.8049\n",
      "Epoch 133/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7979 - accuracy: 0.7382\n",
      "Epoch 00133: val_loss did not improve from 0.60107\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7979 - accuracy: 0.7382 - val_loss: 0.6426 - val_accuracy: 0.8049\n",
      "Epoch 134/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7404 - accuracy: 0.7548\n",
      "Epoch 00134: val_loss did not improve from 0.60107\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7374 - accuracy: 0.7556 - val_loss: 0.6107 - val_accuracy: 0.8098\n",
      "Epoch 135/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.7603\n",
      "Epoch 00135: val_loss did not improve from 0.60107\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7082 - accuracy: 0.7603 - val_loss: 0.6186 - val_accuracy: 0.8146\n",
      "Epoch 136/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.7153 - accuracy: 0.7551\n",
      "Epoch 00136: val_loss did not improve from 0.60107\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7153 - accuracy: 0.7551 - val_loss: 0.6401 - val_accuracy: 0.8146\n",
      "Epoch 137/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7250 - accuracy: 0.7608\n",
      "Epoch 00137: val_loss improved from 0.60107 to 0.59434, saving model to ./data/cvision/model_9-7\\model_9-7ep137-vl0.5943.hdf5\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7251 - accuracy: 0.7586 - val_loss: 0.5943 - val_accuracy: 0.7951\n",
      "Epoch 138/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7345 - accuracy: 0.7410\n",
      "Epoch 00138: val_loss did not improve from 0.59434\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7318 - accuracy: 0.7440 - val_loss: 0.6232 - val_accuracy: 0.8049\n",
      "Epoch 139/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.7452\n",
      "Epoch 00139: val_loss did not improve from 0.59434\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7255 - accuracy: 0.7452 - val_loss: 0.6101 - val_accuracy: 0.7854\n",
      "Epoch 140/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.7656\n",
      "Epoch 00140: val_loss did not improve from 0.59434\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7091 - accuracy: 0.7656 - val_loss: 0.6378 - val_accuracy: 0.7756\n",
      "Epoch 141/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7339 - accuracy: 0.7376\n",
      "Epoch 00141: val_loss improved from 0.59434 to 0.58046, saving model to ./data/cvision/model_9-7\\model_9-7ep141-vl0.5805.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7339 - accuracy: 0.7376 - val_loss: 0.5805 - val_accuracy: 0.8098\n",
      "Epoch 142/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.7510\n",
      "Epoch 00142: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7082 - accuracy: 0.7510 - val_loss: 0.6345 - val_accuracy: 0.7805\n",
      "Epoch 143/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7221 - accuracy: 0.7435\n",
      "Epoch 00143: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7227 - accuracy: 0.7429 - val_loss: 0.6320 - val_accuracy: 0.7902\n",
      "Epoch 144/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7563 - accuracy: 0.7417\n",
      "Epoch 00144: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7487 - accuracy: 0.7440 - val_loss: 0.5936 - val_accuracy: 0.7902\n",
      "Epoch 145/3000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.6949 - accuracy: 0.7800\n",
      "Epoch 00145: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6970 - accuracy: 0.7778 - val_loss: 0.6352 - val_accuracy: 0.7854\n",
      "Epoch 146/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.7522\n",
      "Epoch 00146: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7113 - accuracy: 0.7522 - val_loss: 0.6206 - val_accuracy: 0.7951\n",
      "Epoch 147/3000\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.7428 - accuracy: 0.7327\n",
      "Epoch 00147: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7410 - accuracy: 0.7382 - val_loss: 0.6056 - val_accuracy: 0.7902\n",
      "Epoch 148/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7306 - accuracy: 0.7448\n",
      "Epoch 00148: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7271 - accuracy: 0.7475 - val_loss: 0.5960 - val_accuracy: 0.8098\n",
      "Epoch 149/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7182 - accuracy: 0.7534\n",
      "Epoch 00149: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7182 - accuracy: 0.7534 - val_loss: 0.6175 - val_accuracy: 0.7951\n",
      "Epoch 150/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.7621\n",
      "Epoch 00150: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6730 - accuracy: 0.7621 - val_loss: 0.6445 - val_accuracy: 0.7951\n",
      "Epoch 151/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.7673\n",
      "Epoch 00151: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6814 - accuracy: 0.7673 - val_loss: 0.6216 - val_accuracy: 0.7902\n",
      "Epoch 152/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.7504\n",
      "Epoch 00152: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7341 - accuracy: 0.7504 - val_loss: 0.6014 - val_accuracy: 0.8195\n",
      "Epoch 153/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.7615\n",
      "Epoch 00153: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7235 - accuracy: 0.7615 - val_loss: 0.6333 - val_accuracy: 0.8049\n",
      "Epoch 154/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.7569\n",
      "Epoch 00154: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6854 - accuracy: 0.7569 - val_loss: 0.6110 - val_accuracy: 0.8049\n",
      "Epoch 155/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7336 - accuracy: 0.7475\n",
      "Epoch 00155: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7336 - accuracy: 0.7475 - val_loss: 0.5996 - val_accuracy: 0.7951\n",
      "Epoch 156/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7227 - accuracy: 0.7572\n",
      "Epoch 00156: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7225 - accuracy: 0.7580 - val_loss: 0.6341 - val_accuracy: 0.7756\n",
      "Epoch 157/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.7633 ETA: 0s - loss: 0.6888 - accuracy: \n",
      "Epoch 00157: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6995 - accuracy: 0.7633 - val_loss: 0.5969 - val_accuracy: 0.8098\n",
      "Epoch 158/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6906 - accuracy: 0.7572\n",
      "Epoch 00158: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6904 - accuracy: 0.7574 - val_loss: 0.6196 - val_accuracy: 0.8244\n",
      "Epoch 159/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.7714\n",
      "Epoch 00159: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7243 - accuracy: 0.7714 - val_loss: 0.6080 - val_accuracy: 0.8195\n",
      "Epoch 160/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6825 - accuracy: 0.7697\n",
      "Epoch 00160: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6825 - accuracy: 0.7697 - val_loss: 0.6187 - val_accuracy: 0.8146\n",
      "Epoch 161/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.7580\n",
      "Epoch 00161: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7004 - accuracy: 0.7580 - val_loss: 0.6242 - val_accuracy: 0.8098\n",
      "Epoch 162/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.7504\n",
      "Epoch 00162: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6985 - accuracy: 0.7504 - val_loss: 0.6698 - val_accuracy: 0.7805\n",
      "Epoch 163/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7199 - accuracy: 0.7528 ETA: 0s - loss: 0.7368 - accuracy: 0.\n",
      "Epoch 00163: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7199 - accuracy: 0.7528 - val_loss: 0.6092 - val_accuracy: 0.8098\n",
      "Epoch 164/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.7679\n",
      "Epoch 00164: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6789 - accuracy: 0.7679 - val_loss: 0.6113 - val_accuracy: 0.8195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6743 - accuracy: 0.7722\n",
      "Epoch 00165: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6762 - accuracy: 0.7703 - val_loss: 0.5918 - val_accuracy: 0.8244\n",
      "Epoch 166/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6850 - accuracy: 0.7698\n",
      "Epoch 00166: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6803 - accuracy: 0.7708 - val_loss: 0.6202 - val_accuracy: 0.8146\n",
      "Epoch 167/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6746 - accuracy: 0.7755 ETA: 0s - loss: 0.6916 - accuracy: 0.77\n",
      "Epoch 00167: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6746 - accuracy: 0.7755 - val_loss: 0.6169 - val_accuracy: 0.8146\n",
      "Epoch 168/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6843 - accuracy: 0.7732\n",
      "Epoch 00168: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6776 - accuracy: 0.7755 - val_loss: 0.5990 - val_accuracy: 0.8049\n",
      "Epoch 169/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.7603\n",
      "Epoch 00169: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6957 - accuracy: 0.7603 - val_loss: 0.6127 - val_accuracy: 0.8098\n",
      "Epoch 170/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6955 - accuracy: 0.7599\n",
      "Epoch 00170: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6947 - accuracy: 0.7598 - val_loss: 0.6021 - val_accuracy: 0.8195\n",
      "Epoch 171/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6772 - accuracy: 0.7679\n",
      "Epoch 00171: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6772 - accuracy: 0.7679 - val_loss: 0.6028 - val_accuracy: 0.8146\n",
      "Epoch 172/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6722 - accuracy: 0.7767\n",
      "Epoch 00172: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6722 - accuracy: 0.7767 - val_loss: 0.6070 - val_accuracy: 0.7951\n",
      "Epoch 173/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.7778\n",
      "Epoch 00173: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6792 - accuracy: 0.7778 - val_loss: 0.6275 - val_accuracy: 0.8000\n",
      "Epoch 174/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.7621\n",
      "Epoch 00174: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6702 - accuracy: 0.7621 - val_loss: 0.6281 - val_accuracy: 0.7951\n",
      "Epoch 175/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.7819\n",
      "Epoch 00175: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6217 - accuracy: 0.7819 - val_loss: 0.6199 - val_accuracy: 0.7805\n",
      "Epoch 176/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6617 - accuracy: 0.7698\n",
      "Epoch 00176: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6581 - accuracy: 0.7723 - val_loss: 0.6321 - val_accuracy: 0.8098\n",
      "Epoch 177/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.7668\n",
      "Epoch 00177: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7004 - accuracy: 0.7668 - val_loss: 0.5993 - val_accuracy: 0.8049\n",
      "Epoch 178/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7878\n",
      "Epoch 00178: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6531 - accuracy: 0.7878 - val_loss: 0.6444 - val_accuracy: 0.7951\n",
      "Epoch 179/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.7708\n",
      "Epoch 00179: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6886 - accuracy: 0.7708 - val_loss: 0.5931 - val_accuracy: 0.8098\n",
      "Epoch 180/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6899 - accuracy: 0.7675\n",
      "Epoch 00180: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6907 - accuracy: 0.7662 - val_loss: 0.6374 - val_accuracy: 0.8000\n",
      "Epoch 181/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6539 - accuracy: 0.7714\n",
      "Epoch 00181: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6539 - accuracy: 0.7714 - val_loss: 0.6091 - val_accuracy: 0.8000\n",
      "Epoch 182/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.7767\n",
      "Epoch 00182: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6797 - accuracy: 0.7767 - val_loss: 0.6271 - val_accuracy: 0.7902\n",
      "Epoch 183/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.7848\n",
      "Epoch 00183: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6371 - accuracy: 0.7848 - val_loss: 0.6392 - val_accuracy: 0.7902\n",
      "Epoch 184/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6703 - accuracy: 0.7749\n",
      "Epoch 00184: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6703 - accuracy: 0.7749 - val_loss: 0.6376 - val_accuracy: 0.7854\n",
      "Epoch 185/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.7644\n",
      "Epoch 00185: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6970 - accuracy: 0.7644 - val_loss: 0.6238 - val_accuracy: 0.8000\n",
      "Epoch 186/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6400 - accuracy: 0.7812\n",
      "Epoch 00186: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6435 - accuracy: 0.7829 - val_loss: 0.6260 - val_accuracy: 0.7902\n",
      "Epoch 187/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7697\n",
      "Epoch 00187: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6431 - accuracy: 0.7697 - val_loss: 0.6088 - val_accuracy: 0.8049\n",
      "Epoch 188/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6606 - accuracy: 0.7650\n",
      "Epoch 00188: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6613 - accuracy: 0.7650 - val_loss: 0.6206 - val_accuracy: 0.8000\n",
      "Epoch 189/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6699 - accuracy: 0.7703\n",
      "Epoch 00189: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6699 - accuracy: 0.7703 - val_loss: 0.6291 - val_accuracy: 0.7854\n",
      "Epoch 190/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6550 - accuracy: 0.7720\n",
      "Epoch 00190: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6550 - accuracy: 0.7720 - val_loss: 0.6126 - val_accuracy: 0.8049\n",
      "Epoch 191/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6481 - accuracy: 0.7813\n",
      "Epoch 00191: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6418 - accuracy: 0.7854 - val_loss: 0.6509 - val_accuracy: 0.7805\n",
      "Epoch 192/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6648 - accuracy: 0.7808\n",
      "Epoch 00192: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6648 - accuracy: 0.7808 - val_loss: 0.6292 - val_accuracy: 0.8049\n",
      "Epoch 193/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.7790 ETA: 0s - loss: 0.6411 - accuracy: 0.78\n",
      "Epoch 00193: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6537 - accuracy: 0.7790 - val_loss: 0.6478 - val_accuracy: 0.7902\n",
      "Epoch 194/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6848 - accuracy: 0.7669\n",
      "Epoch 00194: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6737 - accuracy: 0.7708 - val_loss: 0.6302 - val_accuracy: 0.7854\n",
      "Epoch 195/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.7673\n",
      "Epoch 00195: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6537 - accuracy: 0.7673 - val_loss: 0.6375 - val_accuracy: 0.7951\n",
      "Epoch 196/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6209 - accuracy: 0.7965\n",
      "Epoch 00196: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6209 - accuracy: 0.7965 - val_loss: 0.6208 - val_accuracy: 0.7805\n",
      "Epoch 197/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.7784\n",
      "Epoch 00197: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6538 - accuracy: 0.7784 - val_loss: 0.5902 - val_accuracy: 0.8000\n",
      "Epoch 198/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6632 - accuracy: 0.7662\n",
      "Epoch 00198: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6726 - accuracy: 0.7638 - val_loss: 0.6270 - val_accuracy: 0.8098\n",
      "Epoch 199/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6691 - accuracy: 0.7738\n",
      "Epoch 00199: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6691 - accuracy: 0.7738 - val_loss: 0.6281 - val_accuracy: 0.8000\n",
      "Epoch 200/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6566 - accuracy: 0.7710\n",
      "Epoch 00200: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6500 - accuracy: 0.7734 - val_loss: 0.5992 - val_accuracy: 0.8049\n",
      "Epoch 201/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.7796\n",
      "Epoch 00201: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6318 - accuracy: 0.7796 - val_loss: 0.6341 - val_accuracy: 0.8000\n",
      "Epoch 202/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6578 - accuracy: 0.7819\n",
      "Epoch 00202: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6578 - accuracy: 0.7819 - val_loss: 0.5990 - val_accuracy: 0.8049\n",
      "Epoch 203/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6394 - accuracy: 0.7776\n",
      "Epoch 00203: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6399 - accuracy: 0.7773 - val_loss: 0.6130 - val_accuracy: 0.7951\n",
      "Epoch 204/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.7708\n",
      "Epoch 00204: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6476 - accuracy: 0.7708 - val_loss: 0.6062 - val_accuracy: 0.8049\n",
      "Epoch 205/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.7784\n",
      "Epoch 00205: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6702 - accuracy: 0.7784 - val_loss: 0.6074 - val_accuracy: 0.8195\n",
      "Epoch 206/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6540 - accuracy: 0.7776\n",
      "Epoch 00206: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6560 - accuracy: 0.7743 - val_loss: 0.6243 - val_accuracy: 0.8098\n",
      "Epoch 207/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6589 - accuracy: 0.7732\n",
      "Epoch 00207: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6589 - accuracy: 0.7732 - val_loss: 0.6088 - val_accuracy: 0.8000\n",
      "Epoch 208/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6591 - accuracy: 0.7681\n",
      "Epoch 00208: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6644 - accuracy: 0.7656 - val_loss: 0.5835 - val_accuracy: 0.8146\n",
      "Epoch 209/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6448 - accuracy: 0.7750\n",
      "Epoch 00209: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6421 - accuracy: 0.7784 - val_loss: 0.6074 - val_accuracy: 0.7951\n",
      "Epoch 210/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6619 - accuracy: 0.7851\n",
      "Epoch 00210: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6646 - accuracy: 0.7837 - val_loss: 0.6000 - val_accuracy: 0.8146\n",
      "Epoch 211/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6655 - accuracy: 0.7757\n",
      "Epoch 00211: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6662 - accuracy: 0.7761 - val_loss: 0.6153 - val_accuracy: 0.8146\n",
      "Epoch 212/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.7825\n",
      "Epoch 00212: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6433 - accuracy: 0.7825 - val_loss: 0.6019 - val_accuracy: 0.8146\n",
      "Epoch 213/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6444 - accuracy: 0.7782\n",
      "Epoch 00213: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6352 - accuracy: 0.7812 - val_loss: 0.6014 - val_accuracy: 0.8146\n",
      "Epoch 214/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.7808\n",
      "Epoch 00214: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6096 - accuracy: 0.7808 - val_loss: 0.6086 - val_accuracy: 0.8049\n",
      "Epoch 215/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7895\n",
      "Epoch 00215: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6083 - accuracy: 0.7895 - val_loss: 0.6012 - val_accuracy: 0.8049\n",
      "Epoch 216/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6486 - accuracy: 0.7837\n",
      "Epoch 00216: val_loss did not improve from 0.58046\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6486 - accuracy: 0.7837 - val_loss: 0.5917 - val_accuracy: 0.8098\n",
      "Epoch 217/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.7808\n",
      "Epoch 00217: val_loss improved from 0.58046 to 0.56937, saving model to ./data/cvision/model_9-7\\model_9-7ep217-vl0.5694.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6325 - accuracy: 0.7808 - val_loss: 0.5694 - val_accuracy: 0.8098\n",
      "Epoch 218/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.7848\n",
      "Epoch 00218: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6254 - accuracy: 0.7848 - val_loss: 0.5992 - val_accuracy: 0.8000\n",
      "Epoch 219/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.7767\n",
      "Epoch 00219: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6496 - accuracy: 0.7767 - val_loss: 0.6105 - val_accuracy: 0.8049\n",
      "Epoch 220/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.7708\n",
      "Epoch 00220: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6407 - accuracy: 0.7708 - val_loss: 0.6039 - val_accuracy: 0.8000\n",
      "Epoch 221/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.7644\n",
      "Epoch 00221: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6820 - accuracy: 0.7644 - val_loss: 0.6162 - val_accuracy: 0.8049\n",
      "Epoch 222/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6553 - accuracy: 0.7883\n",
      "Epoch 00222: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6622 - accuracy: 0.7813 - val_loss: 0.6212 - val_accuracy: 0.8098\n",
      "Epoch 223/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.7843\n",
      "Epoch 00223: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6306 - accuracy: 0.7843 - val_loss: 0.5998 - val_accuracy: 0.8049\n",
      "Epoch 224/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.7778\n",
      "Epoch 00224: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6553 - accuracy: 0.7778 - val_loss: 0.5974 - val_accuracy: 0.8049\n",
      "Epoch 225/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7790\n",
      "Epoch 00225: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6311 - accuracy: 0.7790 - val_loss: 0.6015 - val_accuracy: 0.8049\n",
      "Epoch 226/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7907\n",
      "Epoch 00226: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5980 - accuracy: 0.7907 - val_loss: 0.5906 - val_accuracy: 0.8195\n",
      "Epoch 227/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6343 - accuracy: 0.7764\n",
      "Epoch 00227: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6319 - accuracy: 0.7767 - val_loss: 0.5937 - val_accuracy: 0.8098\n",
      "Epoch 228/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.7831\n",
      "Epoch 00228: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6359 - accuracy: 0.7831 - val_loss: 0.6076 - val_accuracy: 0.8000\n",
      "Epoch 229/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6363 - accuracy: 0.7831\n",
      "Epoch 00229: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6363 - accuracy: 0.7831 - val_loss: 0.5934 - val_accuracy: 0.8049\n",
      "Epoch 230/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.7796\n",
      "Epoch 00230: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6584 - accuracy: 0.7796 - val_loss: 0.5973 - val_accuracy: 0.8098\n",
      "Epoch 231/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6318 - accuracy: 0.7749\n",
      "Epoch 00231: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6318 - accuracy: 0.7749 - val_loss: 0.5937 - val_accuracy: 0.8000\n",
      "Epoch 232/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6579 - accuracy: 0.7820\n",
      "Epoch 00232: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6443 - accuracy: 0.7854 - val_loss: 0.5874 - val_accuracy: 0.8098\n",
      "Epoch 233/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.7889\n",
      "Epoch 00233: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6276 - accuracy: 0.7889 - val_loss: 0.5809 - val_accuracy: 0.8049\n",
      "Epoch 234/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6203 - accuracy: 0.7879\n",
      "Epoch 00234: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6204 - accuracy: 0.7885 - val_loss: 0.5958 - val_accuracy: 0.8098\n",
      "Epoch 235/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6333 - accuracy: 0.7722\n",
      "Epoch 00235: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6351 - accuracy: 0.7708 - val_loss: 0.5781 - val_accuracy: 0.8098\n",
      "Epoch 236/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.7813\n",
      "Epoch 00236: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6265 - accuracy: 0.7813 - val_loss: 0.5861 - val_accuracy: 0.8049\n",
      "Epoch 237/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6085 - accuracy: 0.7851\n",
      "Epoch 00237: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6268 - accuracy: 0.7790 - val_loss: 0.5720 - val_accuracy: 0.8098\n",
      "Epoch 238/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6419 - accuracy: 0.7807\n",
      "Epoch 00238: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6412 - accuracy: 0.7813 - val_loss: 0.5843 - val_accuracy: 0.8098\n",
      "Epoch 239/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.7988\n",
      "Epoch 00239: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5986 - accuracy: 0.7988 - val_loss: 0.5837 - val_accuracy: 0.8146\n",
      "Epoch 240/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7691\n",
      "Epoch 00240: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6368 - accuracy: 0.7691 - val_loss: 0.5978 - val_accuracy: 0.8049\n",
      "Epoch 241/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6486 - accuracy: 0.7851\n",
      "Epoch 00241: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6570 - accuracy: 0.7802 - val_loss: 0.5974 - val_accuracy: 0.8000\n",
      "Epoch 242/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6456 - accuracy: 0.7867\n",
      "Epoch 00242: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6426 - accuracy: 0.7874 - val_loss: 0.5924 - val_accuracy: 0.8098\n",
      "Epoch 243/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7726\n",
      "Epoch 00243: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6263 - accuracy: 0.7726 - val_loss: 0.5908 - val_accuracy: 0.8049\n",
      "Epoch 244/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.7819\n",
      "Epoch 00244: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6251 - accuracy: 0.7819 - val_loss: 0.6076 - val_accuracy: 0.8000\n",
      "Epoch 245/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.7872\n",
      "Epoch 00245: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6218 - accuracy: 0.7872 - val_loss: 0.6035 - val_accuracy: 0.8049\n",
      "Epoch 246/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6150 - accuracy: 0.7851\n",
      "Epoch 00246: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6157 - accuracy: 0.7813 - val_loss: 0.5779 - val_accuracy: 0.8146\n",
      "Epoch 247/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6739 - accuracy: 0.7732\n",
      "Epoch 00247: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6739 - accuracy: 0.7732 - val_loss: 0.5952 - val_accuracy: 0.8049\n",
      "Epoch 248/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7790\n",
      "Epoch 00248: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6305 - accuracy: 0.7790 - val_loss: 0.5891 - val_accuracy: 0.8049\n",
      "Epoch 249/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.7691\n",
      "Epoch 00249: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6407 - accuracy: 0.7691 - val_loss: 0.5900 - val_accuracy: 0.8049\n",
      "Epoch 250/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6486 - accuracy: 0.7858\n",
      "Epoch 00250: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6380 - accuracy: 0.7878 - val_loss: 0.6008 - val_accuracy: 0.8098\n",
      "Epoch 251/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.7848\n",
      "Epoch 00251: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6295 - accuracy: 0.7848 - val_loss: 0.5953 - val_accuracy: 0.8049\n",
      "Epoch 252/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.7918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00252: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6128 - accuracy: 0.7918 - val_loss: 0.6028 - val_accuracy: 0.8098\n",
      "Epoch 253/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.7883\n",
      "Epoch 00253: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5902 - accuracy: 0.7883 - val_loss: 0.5901 - val_accuracy: 0.8098\n",
      "Epoch 254/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5982 - accuracy: 0.7921\n",
      "Epoch 00254: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5854 - accuracy: 0.7965 - val_loss: 0.5915 - val_accuracy: 0.8049\n",
      "Epoch 255/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6391 - accuracy: 0.7746\n",
      "Epoch 00255: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6262 - accuracy: 0.7796 - val_loss: 0.6096 - val_accuracy: 0.8098\n",
      "Epoch 256/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.7784\n",
      "Epoch 00256: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6456 - accuracy: 0.7784 - val_loss: 0.6071 - val_accuracy: 0.7951\n",
      "Epoch 257/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6438 - accuracy: 0.7883\n",
      "Epoch 00257: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6443 - accuracy: 0.7831 - val_loss: 0.5868 - val_accuracy: 0.8049\n",
      "Epoch 258/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6149 - accuracy: 0.7895\n",
      "Epoch 00258: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6149 - accuracy: 0.7895 - val_loss: 0.6079 - val_accuracy: 0.8000\n",
      "Epoch 259/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.7872\n",
      "Epoch 00259: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6390 - accuracy: 0.7872 - val_loss: 0.5897 - val_accuracy: 0.8000\n",
      "Epoch 260/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5904 - accuracy: 0.7909\n",
      "Epoch 00260: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5907 - accuracy: 0.7901 - val_loss: 0.5933 - val_accuracy: 0.8049\n",
      "Epoch 261/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7889\n",
      "Epoch 00261: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6083 - accuracy: 0.7889 - val_loss: 0.6003 - val_accuracy: 0.8000\n",
      "Epoch 262/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6286 - accuracy: 0.7788\n",
      "Epoch 00262: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6292 - accuracy: 0.7807 - val_loss: 0.5963 - val_accuracy: 0.8049\n",
      "Epoch 263/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6427 - accuracy: 0.7776\n",
      "Epoch 00263: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6406 - accuracy: 0.7773 - val_loss: 0.6175 - val_accuracy: 0.7951\n",
      "Epoch 264/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6255 - accuracy: 0.7837\n",
      "Epoch 00264: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6282 - accuracy: 0.7790 - val_loss: 0.5881 - val_accuracy: 0.8049\n",
      "Epoch 265/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.7831\n",
      "Epoch 00265: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6252 - accuracy: 0.7831 - val_loss: 0.5897 - val_accuracy: 0.8049\n",
      "Epoch 266/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.7831\n",
      "Epoch 00266: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6152 - accuracy: 0.7831 - val_loss: 0.5990 - val_accuracy: 0.7902\n",
      "Epoch 267/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6154 - accuracy: 0.7885\n",
      "Epoch 00267: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6297 - accuracy: 0.7840 - val_loss: 0.6000 - val_accuracy: 0.7902\n",
      "Epoch 268/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6152 - accuracy: 0.7763\n",
      "Epoch 00268: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6152 - accuracy: 0.7767 - val_loss: 0.5782 - val_accuracy: 0.8049\n",
      "Epoch 269/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5972 - accuracy: 0.8116\n",
      "Epoch 00269: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6009 - accuracy: 0.8122 - val_loss: 0.5833 - val_accuracy: 0.7902\n",
      "Epoch 270/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.7837\n",
      "Epoch 00270: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6037 - accuracy: 0.7837 - val_loss: 0.5950 - val_accuracy: 0.7951\n",
      "Epoch 271/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6242 - accuracy: 0.7902\n",
      "Epoch 00271: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6272 - accuracy: 0.7883 - val_loss: 0.5857 - val_accuracy: 0.7951\n",
      "Epoch 272/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.7913\n",
      "Epoch 00272: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6118 - accuracy: 0.7913 - val_loss: 0.6062 - val_accuracy: 0.7951\n",
      "Epoch 273/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.7913\n",
      "Epoch 00273: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6031 - accuracy: 0.7913 - val_loss: 0.6061 - val_accuracy: 0.8049\n",
      "Epoch 274/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6540 - accuracy: 0.7800\n",
      "Epoch 00274: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6526 - accuracy: 0.7812 - val_loss: 0.5924 - val_accuracy: 0.8000\n",
      "Epoch 275/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.7994\n",
      "Epoch 00275: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5785 - accuracy: 0.7994 - val_loss: 0.5971 - val_accuracy: 0.7951\n",
      "Epoch 276/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5888 - accuracy: 0.7885\n",
      "Epoch 00276: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5889 - accuracy: 0.7902 - val_loss: 0.5843 - val_accuracy: 0.8049\n",
      "Epoch 277/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.7918\n",
      "Epoch 00277: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6031 - accuracy: 0.7918 - val_loss: 0.5923 - val_accuracy: 0.8049\n",
      "Epoch 278/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6364 - accuracy: 0.7845\n",
      "Epoch 00278: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6391 - accuracy: 0.7843 - val_loss: 0.5960 - val_accuracy: 0.7902\n",
      "Epoch 279/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.7878\n",
      "Epoch 00279: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6101 - accuracy: 0.7878 - val_loss: 0.6018 - val_accuracy: 0.7951\n",
      "Epoch 280/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5926 - accuracy: 0.8005\n",
      "Epoch 00280: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5957 - accuracy: 0.8000 - val_loss: 0.5954 - val_accuracy: 0.8049\n",
      "Epoch 281/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7965 ETA: 0s - loss: 0.6073 - accuracy: \n",
      "Epoch 00281: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6093 - accuracy: 0.7965 - val_loss: 0.5924 - val_accuracy: 0.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.7802\n",
      "Epoch 00282: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6084 - accuracy: 0.7802 - val_loss: 0.5977 - val_accuracy: 0.8049\n",
      "Epoch 283/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.7977\n",
      "Epoch 00283: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6116 - accuracy: 0.7977 - val_loss: 0.6091 - val_accuracy: 0.8000\n",
      "Epoch 284/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.7936\n",
      "Epoch 00284: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6239 - accuracy: 0.7936 - val_loss: 0.5870 - val_accuracy: 0.8049\n",
      "Epoch 285/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.7889\n",
      "Epoch 00285: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6218 - accuracy: 0.7889 - val_loss: 0.5892 - val_accuracy: 0.8049\n",
      "Epoch 286/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.7872\n",
      "Epoch 00286: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5887 - accuracy: 0.7872 - val_loss: 0.5935 - val_accuracy: 0.8000\n",
      "Epoch 287/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.7825\n",
      "Epoch 00287: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6282 - accuracy: 0.7825 - val_loss: 0.5894 - val_accuracy: 0.8000\n",
      "Epoch 288/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.7942\n",
      "Epoch 00288: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6078 - accuracy: 0.7942 - val_loss: 0.6059 - val_accuracy: 0.7951\n",
      "Epoch 289/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7843\n",
      "Epoch 00289: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6025 - accuracy: 0.7843 - val_loss: 0.5802 - val_accuracy: 0.8000\n",
      "Epoch 290/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7983\n",
      "Epoch 00290: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5628 - accuracy: 0.7983 - val_loss: 0.5702 - val_accuracy: 0.8000\n",
      "Epoch 291/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.7872\n",
      "Epoch 00291: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6101 - accuracy: 0.7872 - val_loss: 0.5913 - val_accuracy: 0.7951\n",
      "Epoch 292/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6238 - accuracy: 0.7921\n",
      "Epoch 00292: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6212 - accuracy: 0.7924 - val_loss: 0.5856 - val_accuracy: 0.8049\n",
      "Epoch 293/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.8035\n",
      "Epoch 00293: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5968 - accuracy: 0.8035 - val_loss: 0.6017 - val_accuracy: 0.8049\n",
      "Epoch 294/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.7977\n",
      "Epoch 00294: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5938 - accuracy: 0.7977 - val_loss: 0.5887 - val_accuracy: 0.8000\n",
      "Epoch 295/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7901\n",
      "Epoch 00295: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5981 - accuracy: 0.7901 - val_loss: 0.5918 - val_accuracy: 0.7951\n",
      "Epoch 296/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.8087\n",
      "Epoch 00296: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5588 - accuracy: 0.8087 - val_loss: 0.5991 - val_accuracy: 0.8049\n",
      "Epoch 297/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.7743\n",
      "Epoch 00297: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6158 - accuracy: 0.7743 - val_loss: 0.6134 - val_accuracy: 0.8000\n",
      "Epoch 298/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.7948\n",
      "Epoch 00298: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5862 - accuracy: 0.7948 - val_loss: 0.5960 - val_accuracy: 0.8049\n",
      "Epoch 299/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.7907\n",
      "Epoch 00299: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5890 - accuracy: 0.7907 - val_loss: 0.5892 - val_accuracy: 0.8000\n",
      "Epoch 300/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.7907\n",
      "Epoch 00300: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6176 - accuracy: 0.7907 - val_loss: 0.6003 - val_accuracy: 0.8000\n",
      "Epoch 301/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.7848\n",
      "Epoch 00301: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6077 - accuracy: 0.7848 - val_loss: 0.6025 - val_accuracy: 0.8000\n",
      "Epoch 302/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7860\n",
      "Epoch 00302: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5990 - accuracy: 0.7860 - val_loss: 0.5886 - val_accuracy: 0.8049\n",
      "Epoch 303/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5904 - accuracy: 0.8058\n",
      "Epoch 00303: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5904 - accuracy: 0.8058 - val_loss: 0.5907 - val_accuracy: 0.7951\n",
      "Epoch 304/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6107 - accuracy: 0.7876\n",
      "Epoch 00304: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5990 - accuracy: 0.7913 - val_loss: 0.6057 - val_accuracy: 0.7902\n",
      "Epoch 305/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.7977\n",
      "Epoch 00305: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5832 - accuracy: 0.7977 - val_loss: 0.5917 - val_accuracy: 0.8049\n",
      "Epoch 306/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6154 - accuracy: 0.7879\n",
      "Epoch 00306: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6074 - accuracy: 0.7919 - val_loss: 0.5825 - val_accuracy: 0.8000\n",
      "Epoch 307/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.8052\n",
      "Epoch 00307: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5532 - accuracy: 0.8052 - val_loss: 0.5830 - val_accuracy: 0.7902\n",
      "Epoch 308/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5935 - accuracy: 0.7918\n",
      "Epoch 00308: val_loss did not improve from 0.56937\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5935 - accuracy: 0.7918 - val_loss: 0.5832 - val_accuracy: 0.7951\n",
      "Epoch 00308: early stopping\n",
      "CNN: Epochs=3000, Train accuracy=0.81224, Validation accuracy=0.82439\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[es, cp]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1598336814187,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "mDdwZDDFkv5W",
    "outputId": "30b0e4a2-9738-4989-eb06-1c54ecef6b41"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQCvPGZks9v"
   },
   "outputs": [],
   "source": [
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'data/cvision/' + model_num + '.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.9073\n",
      "loss= 0.31749480962753296\n",
      "accuracy= 0.9073171019554138\n"
     ]
    }
   ],
   "source": [
    "#모델 불러와서 정확도 확인 및 예측\n",
    "hdf5_file = './data/cvision/model_9-7/model_9-7ep324-vl0.3175.hdf5'\n",
    "model.load_weights(hdf5_file)\n",
    "\n",
    "score = model.evaluate(valid_X, valid_y)\n",
    "print('loss=', score[0])        # loss\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8nb5jokyny"
   },
   "outputs": [],
   "source": [
    "#예측 진행\n",
    "test_X = test.drop(['id', 'letter'], axis=1).values\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X/255.\n",
    "\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('data/cvision/my_subm_9-7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_698 (Conv2D)          (None, 28, 28, 24)        240       \n",
      "_________________________________________________________________\n",
      "conv2d_699 (Conv2D)          (None, 28, 28, 24)        5208      \n",
      "_________________________________________________________________\n",
      "conv2d_700 (Conv2D)          (None, 28, 28, 24)        5208      \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 28, 28, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_701 (Conv2D)          (None, 28, 28, 32)        6944      \n",
      "_________________________________________________________________\n",
      "conv2d_702 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_703 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_704 (Conv2D)          (None, 14, 14, 48)        13872     \n",
      "_________________________________________________________________\n",
      "conv2d_705 (Conv2D)          (None, 7, 7, 48)          20784     \n",
      "_________________________________________________________________\n",
      "conv2d_706 (Conv2D)          (None, 4, 4, 48)          20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 50)                9650      \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 101,696\n",
      "Trainable params: 101,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model_9-8: \n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(24, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(48, (3,3), strides = (2,2), padding = 'same', activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1598347122783,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "tPFielyJ5yMH"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "lrs = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** x)\n",
    "\n",
    "#earlystopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=200)\n",
    "\n",
    "#modelcheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "model_num = 'model_9-8'\n",
    "MODEL_SAVE_FOLDER_PATH = './data/cvision/' + model_num + '/'\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + model_num +'ep{epoch:03d}-vl{val_loss:.4f}.hdf5'\n",
    "\n",
    "cp = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537462,
     "status": "error",
     "timestamp": 1598347660504,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "yeNCbX2vko4s",
    "outputId": "012ce4a9-2ca8-41d3-d1de-9331032469e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.3346 - accuracy: 0.0992\n",
      "Epoch 00001: val_loss improved from inf to 2.29752, saving model to ./data/cvision/model_9-8\\model_9-8ep01-vl2.2975.hdf5\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.3336 - accuracy: 0.0968 - val_loss: 2.2975 - val_accuracy: 0.1171\n",
      "Epoch 2/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2966 - accuracy: 0.1292\n",
      "Epoch 00002: val_loss improved from 2.29752 to 2.29497, saving model to ./data/cvision/model_9-8\\model_9-8ep02-vl2.2950.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2974 - accuracy: 0.1283 - val_loss: 2.2950 - val_accuracy: 0.1366\n",
      "Epoch 3/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2978 - accuracy: 0.1201\n",
      "Epoch 00003: val_loss improved from 2.29497 to 2.29371, saving model to ./data/cvision/model_9-8\\model_9-8ep03-vl2.2937.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2978 - accuracy: 0.1201 - val_loss: 2.2937 - val_accuracy: 0.1122\n",
      "Epoch 4/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2966 - accuracy: 0.1219\n",
      "Epoch 00004: val_loss improved from 2.29371 to 2.29069, saving model to ./data/cvision/model_9-8\\model_9-8ep04-vl2.2907.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2966 - accuracy: 0.1219 - val_loss: 2.2907 - val_accuracy: 0.1220\n",
      "Epoch 5/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2922 - accuracy: 0.1277\n",
      "Epoch 00005: val_loss did not improve from 2.29069\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.2922 - accuracy: 0.1277 - val_loss: 2.2923 - val_accuracy: 0.1122\n",
      "Epoch 6/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2876 - accuracy: 0.1254\n",
      "Epoch 00006: val_loss improved from 2.29069 to 2.28431, saving model to ./data/cvision/model_9-8\\model_9-8ep06-vl2.2843.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.2873 - accuracy: 0.1236 - val_loss: 2.2843 - val_accuracy: 0.1220\n",
      "Epoch 7/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2810 - accuracy: 0.1267\n",
      "Epoch 00007: val_loss improved from 2.28431 to 2.27751, saving model to ./data/cvision/model_9-8\\model_9-8ep07-vl2.2775.hdf5\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.2797 - accuracy: 0.1265 - val_loss: 2.2775 - val_accuracy: 0.1561\n",
      "Epoch 8/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.2793 - accuracy: 0.1300\n",
      "Epoch 00008: val_loss improved from 2.27751 to 2.26774, saving model to ./data/cvision/model_9-8\\model_9-8ep08-vl2.2677.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2793 - accuracy: 0.1300 - val_loss: 2.2677 - val_accuracy: 0.1659\n",
      "Epoch 9/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2711 - accuracy: 0.1418\n",
      "Epoch 00009: val_loss improved from 2.26774 to 2.24386, saving model to ./data/cvision/model_9-8\\model_9-8ep09-vl2.2439.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2686 - accuracy: 0.1440 - val_loss: 2.2439 - val_accuracy: 0.1415\n",
      "Epoch 10/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2470 - accuracy: 0.1605\n",
      "Epoch 00010: val_loss improved from 2.24386 to 2.23043, saving model to ./data/cvision/model_9-8\\model_9-8ep10-vl2.2304.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 2.2471 - accuracy: 0.1586 - val_loss: 2.2304 - val_accuracy: 0.2341\n",
      "Epoch 11/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2175 - accuracy: 0.1676\n",
      "Epoch 00011: val_loss improved from 2.23043 to 2.18006, saving model to ./data/cvision/model_9-8\\model_9-8ep11-vl2.1801.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.2168 - accuracy: 0.1668 - val_loss: 2.1801 - val_accuracy: 0.1756\n",
      "Epoch 12/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.2106 - accuracy: 0.1834\n",
      "Epoch 00012: val_loss did not improve from 2.18006\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 2.2103 - accuracy: 0.1819 - val_loss: 2.1990 - val_accuracy: 0.1902\n",
      "Epoch 13/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.1859 - accuracy: 0.1813\n",
      "Epoch 00013: val_loss improved from 2.18006 to 2.14883, saving model to ./data/cvision/model_9-8\\model_9-8ep13-vl2.1488.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.1859 - accuracy: 0.1813 - val_loss: 2.1488 - val_accuracy: 0.2000\n",
      "Epoch 14/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.1534 - accuracy: 0.2006\n",
      "Epoch 00014: val_loss improved from 2.14883 to 2.11425, saving model to ./data/cvision/model_9-8\\model_9-8ep14-vl2.1142.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.1534 - accuracy: 0.2006 - val_loss: 2.1142 - val_accuracy: 0.2000\n",
      "Epoch 15/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.1264 - accuracy: 0.2413\n",
      "Epoch 00015: val_loss improved from 2.11425 to 2.11323, saving model to ./data/cvision/model_9-8\\model_9-8ep15-vl2.1132.hdf5\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.1273 - accuracy: 0.2379 - val_loss: 2.1132 - val_accuracy: 0.2341\n",
      "Epoch 16/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0905 - accuracy: 0.2256\n",
      "Epoch 00016: val_loss improved from 2.11323 to 2.03532, saving model to ./data/cvision/model_9-8\\model_9-8ep16-vl2.0353.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.0856 - accuracy: 0.2251 - val_loss: 2.0353 - val_accuracy: 0.2488\n",
      "Epoch 17/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0340 - accuracy: 0.2583\n",
      "Epoch 00017: val_loss improved from 2.03532 to 1.99129, saving model to ./data/cvision/model_9-8\\model_9-8ep17-vl1.9913.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 2.0411 - accuracy: 0.2554 - val_loss: 1.9913 - val_accuracy: 0.2634\n",
      "Epoch 18/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0288 - accuracy: 0.2445\n",
      "Epoch 00018: val_loss improved from 1.99129 to 1.94708, saving model to ./data/cvision/model_9-8\\model_9-8ep18-vl1.9471.hdf5\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.0339 - accuracy: 0.2461 - val_loss: 1.9471 - val_accuracy: 0.3171\n",
      "Epoch 19/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0252 - accuracy: 0.2531\n",
      "Epoch 00019: val_loss improved from 1.94708 to 1.87320, saving model to ./data/cvision/model_9-8\\model_9-8ep19-vl1.8732.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 2.0252 - accuracy: 0.2531 - val_loss: 1.8732 - val_accuracy: 0.3122\n",
      "Epoch 20/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 2.0062 - accuracy: 0.2703\n",
      "Epoch 00020: val_loss improved from 1.87320 to 1.83934, saving model to ./data/cvision/model_9-8\\model_9-8ep20-vl1.8393.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.9968 - accuracy: 0.2746 - val_loss: 1.8393 - val_accuracy: 0.3220\n",
      "Epoch 21/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.9844 - accuracy: 0.2791\n",
      "Epoch 00021: val_loss improved from 1.83934 to 1.82677, saving model to ./data/cvision/model_9-8\\model_9-8ep21-vl1.8268.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.9814 - accuracy: 0.2816 - val_loss: 1.8268 - val_accuracy: 0.3805\n",
      "Epoch 22/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9335 - accuracy: 0.2968\n",
      "Epoch 00022: val_loss improved from 1.82677 to 1.79795, saving model to ./data/cvision/model_9-8\\model_9-8ep22-vl1.7980.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.9335 - accuracy: 0.2968 - val_loss: 1.7980 - val_accuracy: 0.4049\n",
      "Epoch 23/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.9349 - accuracy: 0.3114\n",
      "Epoch 00023: val_loss improved from 1.79795 to 1.77771, saving model to ./data/cvision/model_9-8\\model_9-8ep23-vl1.7777.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.9349 - accuracy: 0.3114 - val_loss: 1.7777 - val_accuracy: 0.4439\n",
      "Epoch 24/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8770 - accuracy: 0.3254\n",
      "Epoch 00024: val_loss improved from 1.77771 to 1.75383, saving model to ./data/cvision/model_9-8\\model_9-8ep24-vl1.7538.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.8770 - accuracy: 0.3254 - val_loss: 1.7538 - val_accuracy: 0.4146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8623 - accuracy: 0.3405\n",
      "Epoch 00025: val_loss improved from 1.75383 to 1.73938, saving model to ./data/cvision/model_9-8\\model_9-8ep25-vl1.7394.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.8623 - accuracy: 0.3405 - val_loss: 1.7394 - val_accuracy: 0.3707\n",
      "Epoch 26/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8952 - accuracy: 0.3259\n",
      "Epoch 00026: val_loss did not improve from 1.73938\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.8952 - accuracy: 0.3259 - val_loss: 1.7701 - val_accuracy: 0.4634\n",
      "Epoch 27/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8593 - accuracy: 0.3388\n",
      "Epoch 00027: val_loss improved from 1.73938 to 1.67148, saving model to ./data/cvision/model_9-8\\model_9-8ep27-vl1.6715.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.8593 - accuracy: 0.3388 - val_loss: 1.6715 - val_accuracy: 0.5317\n",
      "Epoch 28/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.8306 - accuracy: 0.3498\n",
      "Epoch 00028: val_loss improved from 1.67148 to 1.55049, saving model to ./data/cvision/model_9-8\\model_9-8ep28-vl1.5505.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.8301 - accuracy: 0.3481 - val_loss: 1.5505 - val_accuracy: 0.5024\n",
      "Epoch 29/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7617 - accuracy: 0.3726\n",
      "Epoch 00029: val_loss improved from 1.55049 to 1.46659, saving model to ./data/cvision/model_9-8\\model_9-8ep29-vl1.4666.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.7617 - accuracy: 0.3726 - val_loss: 1.4666 - val_accuracy: 0.5659\n",
      "Epoch 30/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.7894 - accuracy: 0.3617\n",
      "Epoch 00030: val_loss did not improve from 1.46659\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7753 - accuracy: 0.3656 - val_loss: 1.4960 - val_accuracy: 0.5707\n",
      "Epoch 31/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7119 - accuracy: 0.3889\n",
      "Epoch 00031: val_loss did not improve from 1.46659\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7119 - accuracy: 0.3889 - val_loss: 1.5051 - val_accuracy: 0.5366\n",
      "Epoch 32/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.7203 - accuracy: 0.3900\n",
      "Epoch 00032: val_loss improved from 1.46659 to 1.45150, saving model to ./data/cvision/model_9-8\\model_9-8ep32-vl1.4515.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.7193 - accuracy: 0.3907 - val_loss: 1.4515 - val_accuracy: 0.4878\n",
      "Epoch 33/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7036 - accuracy: 0.3953\n",
      "Epoch 00033: val_loss did not improve from 1.45150\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.7036 - accuracy: 0.3953 - val_loss: 1.4632 - val_accuracy: 0.5463\n",
      "Epoch 34/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6506 - accuracy: 0.4280\n",
      "Epoch 00034: val_loss improved from 1.45150 to 1.34906, saving model to ./data/cvision/model_9-8\\model_9-8ep34-vl1.3491.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.6506 - accuracy: 0.4280 - val_loss: 1.3491 - val_accuracy: 0.6439\n",
      "Epoch 35/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.6087 - accuracy: 0.4285\n",
      "Epoch 00035: val_loss improved from 1.34906 to 1.27086, saving model to ./data/cvision/model_9-8\\model_9-8ep35-vl1.2709.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.6183 - accuracy: 0.4239 - val_loss: 1.2709 - val_accuracy: 0.6146\n",
      "Epoch 36/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.6078 - accuracy: 0.4348\n",
      "Epoch 00036: val_loss did not improve from 1.27086\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.6131 - accuracy: 0.4321 - val_loss: 1.3479 - val_accuracy: 0.6098\n",
      "Epoch 37/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5725 - accuracy: 0.4513\n",
      "Epoch 00037: val_loss improved from 1.27086 to 1.20189, saving model to ./data/cvision/model_9-8\\model_9-8ep37-vl1.2019.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.5725 - accuracy: 0.4513 - val_loss: 1.2019 - val_accuracy: 0.6488\n",
      "Epoch 38/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5682 - accuracy: 0.4420\n",
      "Epoch 00038: val_loss did not improve from 1.20189\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.5682 - accuracy: 0.4420 - val_loss: 1.2344 - val_accuracy: 0.6488\n",
      "Epoch 39/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5916 - accuracy: 0.4385\n",
      "Epoch 00039: val_loss improved from 1.20189 to 1.17262, saving model to ./data/cvision/model_9-8\\model_9-8ep39-vl1.1726.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.5916 - accuracy: 0.4385 - val_loss: 1.1726 - val_accuracy: 0.6585\n",
      "Epoch 40/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.5801 - accuracy: 0.4480\n",
      "Epoch 00040: val_loss did not improve from 1.17262\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.5725 - accuracy: 0.4513 - val_loss: 1.2286 - val_accuracy: 0.6439\n",
      "Epoch 41/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5275 - accuracy: 0.4758\n",
      "Epoch 00041: val_loss improved from 1.17262 to 1.12935, saving model to ./data/cvision/model_9-8\\model_9-8ep41-vl1.1293.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.5275 - accuracy: 0.4758 - val_loss: 1.1293 - val_accuracy: 0.6732\n",
      "Epoch 42/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.5141 - accuracy: 0.4688\n",
      "Epoch 00042: val_loss did not improve from 1.12935\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.5111 - accuracy: 0.4676 - val_loss: 1.1752 - val_accuracy: 0.6488\n",
      "Epoch 43/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4768 - accuracy: 0.4968\n",
      "Epoch 00043: val_loss improved from 1.12935 to 1.06089, saving model to ./data/cvision/model_9-8\\model_9-8ep43-vl1.0609.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.4768 - accuracy: 0.4968 - val_loss: 1.0609 - val_accuracy: 0.7073\n",
      "Epoch 44/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.4517 - accuracy: 0.5054\n",
      "Epoch 00044: val_loss did not improve from 1.06089\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.4516 - accuracy: 0.5038 - val_loss: 1.0631 - val_accuracy: 0.7366\n",
      "Epoch 45/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.4590 - accuracy: 0.4988\n",
      "Epoch 00045: val_loss did not improve from 1.06089\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.4517 - accuracy: 0.4978 - val_loss: 1.0978 - val_accuracy: 0.6634\n",
      "Epoch 46/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3998 - accuracy: 0.5178\n",
      "Epoch 00046: val_loss did not improve from 1.06089\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3998 - accuracy: 0.5178 - val_loss: 1.1027 - val_accuracy: 0.6634\n",
      "Epoch 47/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3550 - accuracy: 0.5446\n",
      "Epoch 00047: val_loss improved from 1.06089 to 1.04781, saving model to ./data/cvision/model_9-8\\model_9-8ep47-vl1.0478.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.3550 - accuracy: 0.5446 - val_loss: 1.0478 - val_accuracy: 0.6585\n",
      "Epoch 48/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.3755 - accuracy: 0.5144\n",
      "Epoch 00048: val_loss did not improve from 1.04781\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3757 - accuracy: 0.5149 - val_loss: 1.0830 - val_accuracy: 0.6537\n",
      "Epoch 49/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.3926 - accuracy: 0.5246\n",
      "Epoch 00049: val_loss improved from 1.04781 to 0.98430, saving model to ./data/cvision/model_9-8\\model_9-8ep49-vl0.9843.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.3948 - accuracy: 0.5248 - val_loss: 0.9843 - val_accuracy: 0.7415\n",
      "Epoch 50/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.3569 - accuracy: 0.5457\n",
      "Epoch 00050: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.3472 - accuracy: 0.5446 - val_loss: 1.1018 - val_accuracy: 0.6537\n",
      "Epoch 51/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 1.3665 - accuracy: 0.5394\n",
      "Epoch 00051: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3665 - accuracy: 0.5394 - val_loss: 1.0605 - val_accuracy: 0.6829\n",
      "Epoch 52/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3152 - accuracy: 0.5359\n",
      "Epoch 00052: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3152 - accuracy: 0.5359 - val_loss: 1.0086 - val_accuracy: 0.7024\n",
      "Epoch 53/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.3121 - accuracy: 0.5495\n",
      "Epoch 00053: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.3097 - accuracy: 0.5475 - val_loss: 1.0121 - val_accuracy: 0.6927\n",
      "Epoch 54/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3185 - accuracy: 0.5475\n",
      "Epoch 00054: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3185 - accuracy: 0.5475 - val_loss: 1.0398 - val_accuracy: 0.6683\n",
      "Epoch 55/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3233 - accuracy: 0.5429\n",
      "Epoch 00055: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.3233 - accuracy: 0.5429 - val_loss: 1.1172 - val_accuracy: 0.6683\n",
      "Epoch 56/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2459 - accuracy: 0.5810\n",
      "Epoch 00056: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2480 - accuracy: 0.5784 - val_loss: 1.0433 - val_accuracy: 0.6634\n",
      "Epoch 57/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2409 - accuracy: 0.5696\n",
      "Epoch 00057: val_loss did not improve from 0.98430\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2493 - accuracy: 0.5679 - val_loss: 1.0192 - val_accuracy: 0.6878\n",
      "Epoch 58/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2591 - accuracy: 0.5691\n",
      "Epoch 00058: val_loss improved from 0.98430 to 0.95331, saving model to ./data/cvision/model_9-8\\model_9-8ep58-vl0.9533.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 1.2591 - accuracy: 0.5691 - val_loss: 0.9533 - val_accuracy: 0.7268\n",
      "Epoch 59/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1943 - accuracy: 0.5956\n",
      "Epoch 00059: val_loss improved from 0.95331 to 0.87202, saving model to ./data/cvision/model_9-8\\model_9-8ep59-vl0.8720.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.1938 - accuracy: 0.5977 - val_loss: 0.8720 - val_accuracy: 0.7561\n",
      "Epoch 60/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2450 - accuracy: 0.5761\n",
      "Epoch 00060: val_loss did not improve from 0.87202\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2450 - accuracy: 0.5761 - val_loss: 0.8984 - val_accuracy: 0.7268\n",
      "Epoch 61/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2255 - accuracy: 0.5865\n",
      "Epoch 00061: val_loss did not improve from 0.87202\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.2195 - accuracy: 0.5887 - val_loss: 0.8964 - val_accuracy: 0.7366\n",
      "Epoch 62/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1512 - accuracy: 0.6037\n",
      "Epoch 00062: val_loss did not improve from 0.87202\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1575 - accuracy: 0.6012 - val_loss: 0.9254 - val_accuracy: 0.6878\n",
      "Epoch 63/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1942 - accuracy: 0.5872\n",
      "Epoch 00063: val_loss improved from 0.87202 to 0.85257, saving model to ./data/cvision/model_9-8\\model_9-8ep63-vl0.8526.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.1942 - accuracy: 0.5872 - val_loss: 0.8526 - val_accuracy: 0.7415\n",
      "Epoch 64/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1929 - accuracy: 0.5766\n",
      "Epoch 00064: val_loss did not improve from 0.85257\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1981 - accuracy: 0.5773 - val_loss: 0.8695 - val_accuracy: 0.7463\n",
      "Epoch 65/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.2090 - accuracy: 0.5911\n",
      "Epoch 00065: val_loss did not improve from 0.85257\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.2099 - accuracy: 0.5883 - val_loss: 1.0107 - val_accuracy: 0.6439\n",
      "Epoch 66/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1897 - accuracy: 0.5889\n",
      "Epoch 00066: val_loss improved from 0.85257 to 0.84414, saving model to ./data/cvision/model_9-8\\model_9-8ep66-vl0.8441.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.1897 - accuracy: 0.5889 - val_loss: 0.8441 - val_accuracy: 0.7512\n",
      "Epoch 67/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1325 - accuracy: 0.6202\n",
      "Epoch 00067: val_loss did not improve from 0.84414\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1396 - accuracy: 0.6210 - val_loss: 0.8547 - val_accuracy: 0.7171\n",
      "Epoch 68/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1380 - accuracy: 0.6150\n",
      "Epoch 00068: val_loss improved from 0.84414 to 0.82596, saving model to ./data/cvision/model_9-8\\model_9-8ep68-vl0.8260.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.1402 - accuracy: 0.6187 - val_loss: 0.8260 - val_accuracy: 0.7220\n",
      "Epoch 69/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1222 - accuracy: 0.6216\n",
      "Epoch 00069: val_loss improved from 0.82596 to 0.80286, saving model to ./data/cvision/model_9-8\\model_9-8ep69-vl0.8029.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.1222 - accuracy: 0.6216 - val_loss: 0.8029 - val_accuracy: 0.7561\n",
      "Epoch 70/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0755 - accuracy: 0.6427\n",
      "Epoch 00070: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0792 - accuracy: 0.6397 - val_loss: 0.9272 - val_accuracy: 0.6927\n",
      "Epoch 71/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.1011 - accuracy: 0.6301\n",
      "Epoch 00071: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.0959 - accuracy: 0.6338 - val_loss: 0.8780 - val_accuracy: 0.7366\n",
      "Epoch 72/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1204 - accuracy: 0.6157\n",
      "Epoch 00072: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1204 - accuracy: 0.6157 - val_loss: 0.8637 - val_accuracy: 0.7171\n",
      "Epoch 73/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0926 - accuracy: 0.6188\n",
      "Epoch 00073: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0978 - accuracy: 0.6187 - val_loss: 0.8825 - val_accuracy: 0.7122\n",
      "Epoch 74/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0828 - accuracy: 0.6420\n",
      "Epoch 00074: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0828 - accuracy: 0.6420 - val_loss: 0.8186 - val_accuracy: 0.7268\n",
      "Epoch 75/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0699 - accuracy: 0.6452\n",
      "Epoch 00075: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0714 - accuracy: 0.6461 - val_loss: 0.8727 - val_accuracy: 0.7122\n",
      "Epoch 76/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1192 - accuracy: 0.6169\n",
      "Epoch 00076: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1192 - accuracy: 0.6169 - val_loss: 0.8415 - val_accuracy: 0.7171\n",
      "Epoch 77/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0631 - accuracy: 0.6391\n",
      "Epoch 00077: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0631 - accuracy: 0.6391 - val_loss: 0.8702 - val_accuracy: 0.7073\n",
      "Epoch 78/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0334 - accuracy: 0.6484\n",
      "Epoch 00078: val_loss did not improve from 0.80286\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 1.0370 - accuracy: 0.6466 - val_loss: 0.8902 - val_accuracy: 0.7171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0682 - accuracy: 0.6204\n",
      "Epoch 00079: val_loss improved from 0.80286 to 0.78220, saving model to ./data/cvision/model_9-8\\model_9-8ep79-vl0.7822.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.0682 - accuracy: 0.6204 - val_loss: 0.7822 - val_accuracy: 0.7951\n",
      "Epoch 80/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0438 - accuracy: 0.6436\n",
      "Epoch 00080: val_loss did not improve from 0.78220\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0404 - accuracy: 0.6455 - val_loss: 0.7941 - val_accuracy: 0.7512\n",
      "Epoch 81/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0233 - accuracy: 0.6648\n",
      "Epoch 00081: val_loss improved from 0.78220 to 0.75056, saving model to ./data/cvision/model_9-8\\model_9-8ep81-vl0.7506.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 1.0075 - accuracy: 0.6706 - val_loss: 0.7506 - val_accuracy: 0.7756\n",
      "Epoch 82/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0493 - accuracy: 0.6514\n",
      "Epoch 00082: val_loss did not improve from 0.75056\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0498 - accuracy: 0.6490 - val_loss: 0.8200 - val_accuracy: 0.7366\n",
      "Epoch 83/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0095 - accuracy: 0.6534\n",
      "Epoch 00083: val_loss did not improve from 0.75056\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0093 - accuracy: 0.6560 - val_loss: 0.7511 - val_accuracy: 0.7756\n",
      "Epoch 84/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9753 - accuracy: 0.6698\n",
      "Epoch 00084: val_loss did not improve from 0.75056\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9913 - accuracy: 0.6676 - val_loss: 0.7605 - val_accuracy: 0.7512\n",
      "Epoch 85/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9999 - accuracy: 0.6601\n",
      "Epoch 00085: val_loss did not improve from 0.75056\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9999 - accuracy: 0.6601 - val_loss: 0.7811 - val_accuracy: 0.7512\n",
      "Epoch 86/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 1.0411 - accuracy: 0.6578\n",
      "Epoch 00086: val_loss did not improve from 0.75056\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.0361 - accuracy: 0.6583 - val_loss: 0.8878 - val_accuracy: 0.6878\n",
      "Epoch 87/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9869 - accuracy: 0.6729\n",
      "Epoch 00087: val_loss improved from 0.75056 to 0.74896, saving model to ./data/cvision/model_9-8\\model_9-8ep87-vl0.7490.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9869 - accuracy: 0.6729 - val_loss: 0.7490 - val_accuracy: 0.7659\n",
      "Epoch 88/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9903 - accuracy: 0.6575\n",
      "Epoch 00088: val_loss did not improve from 0.74896\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9944 - accuracy: 0.6574 - val_loss: 0.7903 - val_accuracy: 0.7415\n",
      "Epoch 89/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9918 - accuracy: 0.6606\n",
      "Epoch 00089: val_loss improved from 0.74896 to 0.73022, saving model to ./data/cvision/model_9-8\\model_9-8ep89-vl0.7302.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9918 - accuracy: 0.6606 - val_loss: 0.7302 - val_accuracy: 0.7707\n",
      "Epoch 90/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.6746\n",
      "Epoch 00090: val_loss did not improve from 0.73022\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9662 - accuracy: 0.6746 - val_loss: 0.8112 - val_accuracy: 0.7024\n",
      "Epoch 91/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9615 - accuracy: 0.6694\n",
      "Epoch 00091: val_loss improved from 0.73022 to 0.66961, saving model to ./data/cvision/model_9-8\\model_9-8ep91-vl0.6696.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9615 - accuracy: 0.6694 - val_loss: 0.6696 - val_accuracy: 0.7854\n",
      "Epoch 92/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9905 - accuracy: 0.6496\n",
      "Epoch 00092: val_loss did not improve from 0.66961\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9905 - accuracy: 0.6496 - val_loss: 0.7424 - val_accuracy: 0.7659\n",
      "Epoch 93/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9544 - accuracy: 0.6786\n",
      "Epoch 00093: val_loss did not improve from 0.66961\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.9531 - accuracy: 0.6752 - val_loss: 0.6906 - val_accuracy: 0.7902\n",
      "Epoch 94/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9315 - accuracy: 0.6905 ETA: 0s - loss: 0.9852 - accuracy\n",
      "Epoch 00094: val_loss did not improve from 0.66961\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9433 - accuracy: 0.6858 - val_loss: 0.7866 - val_accuracy: 0.7366\n",
      "Epoch 95/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9372 - accuracy: 0.6774\n",
      "Epoch 00095: val_loss did not improve from 0.66961\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9405 - accuracy: 0.6746 - val_loss: 0.6779 - val_accuracy: 0.7756\n",
      "Epoch 96/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9203 - accuracy: 0.6900\n",
      "Epoch 00096: val_loss did not improve from 0.66961\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9157 - accuracy: 0.6933 - val_loss: 0.6894 - val_accuracy: 0.7707\n",
      "Epoch 97/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8811 - accuracy: 0.7007\n",
      "Epoch 00097: val_loss improved from 0.66961 to 0.65412, saving model to ./data/cvision/model_9-8\\model_9-8ep97-vl0.6541.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.8905 - accuracy: 0.6945 - val_loss: 0.6541 - val_accuracy: 0.7902\n",
      "Epoch 98/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.6816\n",
      "Epoch 00098: val_loss did not improve from 0.65412\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9212 - accuracy: 0.6816 - val_loss: 0.6751 - val_accuracy: 0.7610\n",
      "Epoch 99/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9351 - accuracy: 0.6774\n",
      "Epoch 00099: val_loss did not improve from 0.65412\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9326 - accuracy: 0.6764 - val_loss: 0.6864 - val_accuracy: 0.7805\n",
      "Epoch 100/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9416 - accuracy: 0.6797\n",
      "Epoch 00100: val_loss did not improve from 0.65412\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9452 - accuracy: 0.6810 - val_loss: 0.6778 - val_accuracy: 0.8146\n",
      "Epoch 101/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9394 - accuracy: 0.6793\n",
      "Epoch 00101: val_loss did not improve from 0.65412\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.9289 - accuracy: 0.6840 - val_loss: 0.6835 - val_accuracy: 0.7854\n",
      "Epoch 102/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9100 - accuracy: 0.6950\n",
      "Epoch 00102: val_loss improved from 0.65412 to 0.64881, saving model to ./data/cvision/model_9-8\\model_9-8ep102-vl0.6488.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9217 - accuracy: 0.6880 - val_loss: 0.6488 - val_accuracy: 0.8195\n",
      "Epoch 103/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.9518 - accuracy: 0.6755\n",
      "Epoch 00103: val_loss did not improve from 0.64881\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.9577 - accuracy: 0.6723 - val_loss: 0.6692 - val_accuracy: 0.8098\n",
      "Epoch 104/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8938 - accuracy: 0.6906\n",
      "Epoch 00104: val_loss improved from 0.64881 to 0.63282, saving model to ./data/cvision/model_9-8\\model_9-8ep104-vl0.6328.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8937 - accuracy: 0.6927 - val_loss: 0.6328 - val_accuracy: 0.8049\n",
      "Epoch 105/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8974 - accuracy: 0.6862\n",
      "Epoch 00105: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8767 - accuracy: 0.6945 - val_loss: 0.7071 - val_accuracy: 0.7707\n",
      "Epoch 106/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.6915\n",
      "Epoch 00106: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.9048 - accuracy: 0.6915 - val_loss: 0.6821 - val_accuracy: 0.7512\n",
      "Epoch 107/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8832 - accuracy: 0.7050\n",
      "Epoch 00107: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8832 - accuracy: 0.7050 - val_loss: 0.7061 - val_accuracy: 0.7805\n",
      "Epoch 108/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8802 - accuracy: 0.7007\n",
      "Epoch 00108: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8778 - accuracy: 0.7026 - val_loss: 0.6592 - val_accuracy: 0.7756\n",
      "Epoch 109/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8536 - accuracy: 0.7139\n",
      "Epoch 00109: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8545 - accuracy: 0.7149 - val_loss: 0.6654 - val_accuracy: 0.7951\n",
      "Epoch 110/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8587 - accuracy: 0.7089\n",
      "Epoch 00110: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8598 - accuracy: 0.7102 - val_loss: 0.6359 - val_accuracy: 0.8098\n",
      "Epoch 111/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8850 - accuracy: 0.7026\n",
      "Epoch 00111: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8850 - accuracy: 0.7026 - val_loss: 0.6751 - val_accuracy: 0.7951\n",
      "Epoch 112/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8175 - accuracy: 0.7194\n",
      "Epoch 00112: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8116 - accuracy: 0.7204 - val_loss: 0.6829 - val_accuracy: 0.7610\n",
      "Epoch 113/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8414 - accuracy: 0.7196\n",
      "Epoch 00113: val_loss did not improve from 0.63282\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.8399 - accuracy: 0.7195 - val_loss: 0.6385 - val_accuracy: 0.7854\n",
      "Epoch 114/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8060 - accuracy: 0.7158\n",
      "Epoch 00114: val_loss improved from 0.63282 to 0.61798, saving model to ./data/cvision/model_9-8\\model_9-8ep114-vl0.6180.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8115 - accuracy: 0.7155 - val_loss: 0.6180 - val_accuracy: 0.8000\n",
      "Epoch 115/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8381 - accuracy: 0.7009\n",
      "Epoch 00115: val_loss improved from 0.61798 to 0.61086, saving model to ./data/cvision/model_9-8\\model_9-8ep115-vl0.6109.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.8381 - accuracy: 0.7009 - val_loss: 0.6109 - val_accuracy: 0.8049\n",
      "Epoch 116/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8512 - accuracy: 0.7076\n",
      "Epoch 00116: val_loss improved from 0.61086 to 0.60677, saving model to ./data/cvision/model_9-8\\model_9-8ep116-vl0.6068.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8507 - accuracy: 0.7090 - val_loss: 0.6068 - val_accuracy: 0.7902\n",
      "Epoch 117/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8358 - accuracy: 0.7146\n",
      "Epoch 00117: val_loss did not improve from 0.60677\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8356 - accuracy: 0.7137 - val_loss: 0.6584 - val_accuracy: 0.7512\n",
      "Epoch 118/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7989 - accuracy: 0.7353\n",
      "Epoch 00118: val_loss did not improve from 0.60677\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7989 - accuracy: 0.7353 - val_loss: 0.6328 - val_accuracy: 0.7707\n",
      "Epoch 119/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8326 - accuracy: 0.7114\n",
      "Epoch 00119: val_loss improved from 0.60677 to 0.57988, saving model to ./data/cvision/model_9-8\\model_9-8ep119-vl0.5799.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8365 - accuracy: 0.7120 - val_loss: 0.5799 - val_accuracy: 0.7902\n",
      "Epoch 120/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7920 - accuracy: 0.7314\n",
      "Epoch 00120: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.7936 - accuracy: 0.7294 - val_loss: 0.6145 - val_accuracy: 0.7902\n",
      "Epoch 121/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8630 - accuracy: 0.7095\n",
      "Epoch 00121: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8636 - accuracy: 0.7061 - val_loss: 0.6960 - val_accuracy: 0.7561\n",
      "Epoch 122/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8173 - accuracy: 0.7297\n",
      "Epoch 00122: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.8215 - accuracy: 0.7289 - val_loss: 0.6008 - val_accuracy: 0.8000\n",
      "Epoch 123/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8037 - accuracy: 0.7254\n",
      "Epoch 00123: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8037 - accuracy: 0.7254 - val_loss: 0.6334 - val_accuracy: 0.8293\n",
      "Epoch 124/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8280 - accuracy: 0.7127\n",
      "Epoch 00124: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8296 - accuracy: 0.7125 - val_loss: 0.5981 - val_accuracy: 0.8000\n",
      "Epoch 125/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8174 - accuracy: 0.7253\n",
      "Epoch 00125: val_loss did not improve from 0.57988\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.8004 - accuracy: 0.7318 - val_loss: 0.6646 - val_accuracy: 0.7902\n",
      "Epoch 126/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7662 - accuracy: 0.7448\n",
      "Epoch 00126: val_loss improved from 0.57988 to 0.57717, saving model to ./data/cvision/model_9-8\\model_9-8ep126-vl0.5772.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7855 - accuracy: 0.7382 - val_loss: 0.5772 - val_accuracy: 0.8146\n",
      "Epoch 127/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.8011 - accuracy: 0.7404\n",
      "Epoch 00127: val_loss did not improve from 0.57717\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.8018 - accuracy: 0.7382 - val_loss: 0.6246 - val_accuracy: 0.8098\n",
      "Epoch 128/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7763 - accuracy: 0.7283\n",
      "Epoch 00128: val_loss did not improve from 0.57717\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7763 - accuracy: 0.7283 - val_loss: 0.6184 - val_accuracy: 0.8098\n",
      "Epoch 129/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7809 - accuracy: 0.7290\n",
      "Epoch 00129: val_loss did not improve from 0.57717\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7872 - accuracy: 0.7265 - val_loss: 0.6019 - val_accuracy: 0.8146\n",
      "Epoch 130/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7912 - accuracy: 0.7386\n",
      "Epoch 00130: val_loss improved from 0.57717 to 0.55898, saving model to ./data/cvision/model_9-8\\model_9-8ep130-vl0.5590.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7946 - accuracy: 0.7399 - val_loss: 0.5590 - val_accuracy: 0.8244\n",
      "Epoch 131/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7782 - accuracy: 0.7328\n",
      "Epoch 00131: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7714 - accuracy: 0.7353 - val_loss: 0.6555 - val_accuracy: 0.7659\n",
      "Epoch 132/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.7306\n",
      "Epoch 00132: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7649 - accuracy: 0.7306 - val_loss: 0.5776 - val_accuracy: 0.8049\n",
      "Epoch 133/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7796 - accuracy: 0.7335\n",
      "Epoch 00133: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7704 - accuracy: 0.7341 - val_loss: 0.5951 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7567 - accuracy: 0.7353\n",
      "Epoch 00134: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7567 - accuracy: 0.7353 - val_loss: 0.5663 - val_accuracy: 0.8195\n",
      "Epoch 135/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7397 - accuracy: 0.7512\n",
      "Epoch 00135: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7436 - accuracy: 0.7499 - val_loss: 0.6636 - val_accuracy: 0.7707\n",
      "Epoch 136/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7822 - accuracy: 0.7435\n",
      "Epoch 00136: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7821 - accuracy: 0.7423 - val_loss: 0.6254 - val_accuracy: 0.7756\n",
      "Epoch 137/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7927 - accuracy: 0.7300\n",
      "Epoch 00137: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7927 - accuracy: 0.7300 - val_loss: 0.5974 - val_accuracy: 0.7951\n",
      "Epoch 138/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7345 - accuracy: 0.7473\n",
      "Epoch 00138: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7474 - accuracy: 0.7446 - val_loss: 0.5884 - val_accuracy: 0.7951\n",
      "Epoch 139/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7298 - accuracy: 0.7517\n",
      "Epoch 00139: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7226 - accuracy: 0.7539 - val_loss: 0.6107 - val_accuracy: 0.7951\n",
      "Epoch 140/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7186 - accuracy: 0.7536\n",
      "Epoch 00140: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7219 - accuracy: 0.7534 - val_loss: 0.5802 - val_accuracy: 0.8098\n",
      "Epoch 141/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7550 - accuracy: 0.7461\n",
      "Epoch 00141: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7432 - accuracy: 0.7504 - val_loss: 0.5867 - val_accuracy: 0.8146\n",
      "Epoch 142/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7147 - accuracy: 0.7536\n",
      "Epoch 00142: val_loss did not improve from 0.55898\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7199 - accuracy: 0.7522 - val_loss: 0.6088 - val_accuracy: 0.7902\n",
      "Epoch 143/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7451 - accuracy: 0.7469\n",
      "Epoch 00143: val_loss improved from 0.55898 to 0.55825, saving model to ./data/cvision/model_9-8\\model_9-8ep143-vl0.5582.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7451 - accuracy: 0.7469 - val_loss: 0.5582 - val_accuracy: 0.8146\n",
      "Epoch 144/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7607 - accuracy: 0.7504\n",
      "Epoch 00144: val_loss did not improve from 0.55825\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7607 - accuracy: 0.7504 - val_loss: 0.5773 - val_accuracy: 0.8195\n",
      "Epoch 145/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7473 - accuracy: 0.7461\n",
      "Epoch 00145: val_loss did not improve from 0.55825\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7511 - accuracy: 0.7469 - val_loss: 0.6012 - val_accuracy: 0.8000\n",
      "Epoch 146/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7445 - accuracy: 0.7440\n",
      "Epoch 00146: val_loss did not improve from 0.55825\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.7395 - accuracy: 0.7475 - val_loss: 0.5976 - val_accuracy: 0.8000\n",
      "Epoch 147/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7463 - accuracy: 0.7470\n",
      "Epoch 00147: val_loss did not improve from 0.55825\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7414 - accuracy: 0.7489 - val_loss: 0.6054 - val_accuracy: 0.7951\n",
      "Epoch 148/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7339 - accuracy: 0.7370\n",
      "Epoch 00148: val_loss improved from 0.55825 to 0.55435, saving model to ./data/cvision/model_9-8\\model_9-8ep148-vl0.5544.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7339 - accuracy: 0.7370 - val_loss: 0.5544 - val_accuracy: 0.8098\n",
      "Epoch 149/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7098 - accuracy: 0.7569\n",
      "Epoch 00149: val_loss improved from 0.55435 to 0.54773, saving model to ./data/cvision/model_9-8\\model_9-8ep149-vl0.5477.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7098 - accuracy: 0.7569 - val_loss: 0.5477 - val_accuracy: 0.8146\n",
      "Epoch 150/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.7499\n",
      "Epoch 00150: val_loss did not improve from 0.54773\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7369 - accuracy: 0.7499 - val_loss: 0.6041 - val_accuracy: 0.8098\n",
      "Epoch 151/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7202 - accuracy: 0.7493\n",
      "Epoch 00151: val_loss did not improve from 0.54773\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7202 - accuracy: 0.7493 - val_loss: 0.5643 - val_accuracy: 0.7902\n",
      "Epoch 152/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7073 - accuracy: 0.7602\n",
      "Epoch 00152: val_loss did not improve from 0.54773\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7191 - accuracy: 0.7584 - val_loss: 0.5546 - val_accuracy: 0.8293\n",
      "Epoch 153/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6958 - accuracy: 0.7618\n",
      "Epoch 00153: val_loss improved from 0.54773 to 0.52311, saving model to ./data/cvision/model_9-8\\model_9-8ep153-vl0.5231.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.7057 - accuracy: 0.7551 - val_loss: 0.5231 - val_accuracy: 0.8293\n",
      "Epoch 154/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.7656\n",
      "Epoch 00154: val_loss did not improve from 0.52311\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7005 - accuracy: 0.7656 - val_loss: 0.5805 - val_accuracy: 0.8146\n",
      "Epoch 155/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7703\n",
      "Epoch 00155: val_loss did not improve from 0.52311\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6875 - accuracy: 0.7703 - val_loss: 0.5943 - val_accuracy: 0.8098\n",
      "Epoch 156/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7259 - accuracy: 0.7669\n",
      "Epoch 00156: val_loss did not improve from 0.52311\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7254 - accuracy: 0.7691 - val_loss: 0.5640 - val_accuracy: 0.8390\n",
      "Epoch 157/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7039 - accuracy: 0.7587\n",
      "Epoch 00157: val_loss did not improve from 0.52311\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.7041 - accuracy: 0.7563 - val_loss: 0.5605 - val_accuracy: 0.8098\n",
      "Epoch 158/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7136 - accuracy: 0.7524\n",
      "Epoch 00158: val_loss did not improve from 0.52311\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7113 - accuracy: 0.7563 - val_loss: 0.5464 - val_accuracy: 0.8341\n",
      "Epoch 159/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6817 - accuracy: 0.7643\n",
      "Epoch 00159: val_loss improved from 0.52311 to 0.51971, saving model to ./data/cvision/model_9-8\\model_9-8ep159-vl0.5197.hdf5\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.6787 - accuracy: 0.7644 - val_loss: 0.5197 - val_accuracy: 0.8293\n",
      "Epoch 160/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7685\n",
      "Epoch 00160: val_loss did not improve from 0.51971\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6968 - accuracy: 0.7685 - val_loss: 0.5764 - val_accuracy: 0.8244\n",
      "Epoch 161/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6654 - accuracy: 0.7801\n",
      "Epoch 00161: val_loss did not improve from 0.51971\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6662 - accuracy: 0.7802 - val_loss: 0.5254 - val_accuracy: 0.8488\n",
      "Epoch 162/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6735 - accuracy: 0.7719\n",
      "Epoch 00162: val_loss did not improve from 0.51971\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6719 - accuracy: 0.7703 - val_loss: 0.5462 - val_accuracy: 0.8293\n",
      "Epoch 163/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7068 - accuracy: 0.7656\n",
      "Epoch 00163: val_loss did not improve from 0.51971\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.7156 - accuracy: 0.7638 - val_loss: 0.5524 - val_accuracy: 0.8244\n",
      "Epoch 164/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.7644\n",
      "Epoch 00164: val_loss did not improve from 0.51971\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6856 - accuracy: 0.7644 - val_loss: 0.5434 - val_accuracy: 0.8098\n",
      "Epoch 165/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.7697\n",
      "Epoch 00165: val_loss improved from 0.51971 to 0.49944, saving model to ./data/cvision/model_9-8\\model_9-8ep165-vl0.4994.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.6540 - accuracy: 0.7697 - val_loss: 0.4994 - val_accuracy: 0.8537\n",
      "Epoch 166/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7118 - accuracy: 0.7574\n",
      "Epoch 00166: val_loss improved from 0.49944 to 0.49774, saving model to ./data/cvision/model_9-8\\model_9-8ep166-vl0.4977.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6989 - accuracy: 0.7615 - val_loss: 0.4977 - val_accuracy: 0.8585\n",
      "Epoch 167/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.7004 - accuracy: 0.7454\n",
      "Epoch 00167: val_loss did not improve from 0.49774\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6993 - accuracy: 0.7458 - val_loss: 0.5352 - val_accuracy: 0.8195\n",
      "Epoch 168/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.7819\n",
      "Epoch 00168: val_loss did not improve from 0.49774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6340 - accuracy: 0.7819 - val_loss: 0.5154 - val_accuracy: 0.8390\n",
      "Epoch 169/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6570 - accuracy: 0.7675\n",
      "Epoch 00169: val_loss did not improve from 0.49774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6628 - accuracy: 0.7633 - val_loss: 0.5022 - val_accuracy: 0.8439\n",
      "Epoch 170/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.7848\n",
      "Epoch 00170: val_loss did not improve from 0.49774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6570 - accuracy: 0.7848 - val_loss: 0.5186 - val_accuracy: 0.8488\n",
      "Epoch 171/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6549 - accuracy: 0.7757\n",
      "Epoch 00171: val_loss improved from 0.49774 to 0.48881, saving model to ./data/cvision/model_9-8\\model_9-8ep171-vl0.4888.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.6571 - accuracy: 0.7749 - val_loss: 0.4888 - val_accuracy: 0.8488\n",
      "Epoch 172/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6773 - accuracy: 0.7725\n",
      "Epoch 00172: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6840 - accuracy: 0.7714 - val_loss: 0.5098 - val_accuracy: 0.8439\n",
      "Epoch 173/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6483 - accuracy: 0.7700\n",
      "Epoch 00173: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6502 - accuracy: 0.7708 - val_loss: 0.5128 - val_accuracy: 0.8537\n",
      "Epoch 174/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6324 - accuracy: 0.7876\n",
      "Epoch 00174: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6335 - accuracy: 0.7854 - val_loss: 0.5052 - val_accuracy: 0.8488\n",
      "Epoch 175/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.7895\n",
      "Epoch 00175: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6221 - accuracy: 0.7895 - val_loss: 0.5005 - val_accuracy: 0.8488\n",
      "Epoch 176/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6687 - accuracy: 0.7703\n",
      "Epoch 00176: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6687 - accuracy: 0.7703 - val_loss: 0.5009 - val_accuracy: 0.8341\n",
      "Epoch 177/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6244 - accuracy: 0.7851\n",
      "Epoch 00177: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6378 - accuracy: 0.7819 - val_loss: 0.5318 - val_accuracy: 0.8390\n",
      "Epoch 178/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6640 - accuracy: 0.7706\n",
      "Epoch 00178: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6631 - accuracy: 0.7743 - val_loss: 0.5009 - val_accuracy: 0.8341\n",
      "Epoch 179/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6527 - accuracy: 0.7738\n",
      "Epoch 00179: val_loss did not improve from 0.48881\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6424 - accuracy: 0.7784 - val_loss: 0.4910 - val_accuracy: 0.8244\n",
      "Epoch 180/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6232 - accuracy: 0.7897\n",
      "Epoch 00180: val_loss improved from 0.48881 to 0.47382, saving model to ./data/cvision/model_9-8\\model_9-8ep180-vl0.4738.hdf5\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.6252 - accuracy: 0.7878 - val_loss: 0.4738 - val_accuracy: 0.8439\n",
      "Epoch 181/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7831\n",
      "Epoch 00181: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6548 - accuracy: 0.7831 - val_loss: 0.5161 - val_accuracy: 0.8098\n",
      "Epoch 182/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6318 - accuracy: 0.7883\n",
      "Epoch 00182: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.6327 - accuracy: 0.7872 - val_loss: 0.4986 - val_accuracy: 0.8244\n",
      "Epoch 183/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6312 - accuracy: 0.7782\n",
      "Epoch 00183: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.6287 - accuracy: 0.7790 - val_loss: 0.4951 - val_accuracy: 0.8341\n",
      "Epoch 184/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7837\n",
      "Epoch 00184: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.6431 - accuracy: 0.7837 - val_loss: 0.5028 - val_accuracy: 0.8488\n",
      "Epoch 185/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6024 - accuracy: 0.7952\n",
      "Epoch 00185: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6033 - accuracy: 0.7930 - val_loss: 0.5778 - val_accuracy: 0.8244\n",
      "Epoch 186/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6418 - accuracy: 0.7769\n",
      "Epoch 00186: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6428 - accuracy: 0.7767 - val_loss: 0.4853 - val_accuracy: 0.8488\n",
      "Epoch 187/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.8035\n",
      "Epoch 00187: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5954 - accuracy: 0.8035 - val_loss: 0.5067 - val_accuracy: 0.8195\n",
      "Epoch 188/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6439 - accuracy: 0.7825\n",
      "Epoch 00188: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6410 - accuracy: 0.7840 - val_loss: 0.4959 - val_accuracy: 0.8341\n",
      "Epoch 189/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7860\n",
      "Epoch 00189: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6055 - accuracy: 0.7860 - val_loss: 0.5231 - val_accuracy: 0.8488\n",
      "Epoch 190/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6216 - accuracy: 0.7719 ETA: 0s - loss: 0.5944 - accuracy\n",
      "Epoch 00190: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6243 - accuracy: 0.7720 - val_loss: 0.5413 - val_accuracy: 0.8244\n",
      "Epoch 191/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5848 - accuracy: 0.7902\n",
      "Epoch 00191: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5889 - accuracy: 0.7907 - val_loss: 0.4819 - val_accuracy: 0.8488\n",
      "Epoch 192/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6364 - accuracy: 0.7872\n",
      "Epoch 00192: val_loss did not improve from 0.47382\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6364 - accuracy: 0.7872 - val_loss: 0.4782 - val_accuracy: 0.8341\n",
      "Epoch 193/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6190 - accuracy: 0.7895\n",
      "Epoch 00193: val_loss improved from 0.47382 to 0.46024, saving model to ./data/cvision/model_9-8\\model_9-8ep193-vl0.4602.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.6143 - accuracy: 0.7924 - val_loss: 0.4602 - val_accuracy: 0.8634\n",
      "Epoch 194/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6196 - accuracy: 0.7918\n",
      "Epoch 00194: val_loss did not improve from 0.46024\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6196 - accuracy: 0.7918 - val_loss: 0.4933 - val_accuracy: 0.8488\n",
      "Epoch 195/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.7796\n",
      "Epoch 00195: val_loss did not improve from 0.46024\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6351 - accuracy: 0.7796 - val_loss: 0.5318 - val_accuracy: 0.8146\n",
      "Epoch 196/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5650 - accuracy: 0.8084\n",
      "Epoch 00196: val_loss improved from 0.46024 to 0.45931, saving model to ./data/cvision/model_9-8\\model_9-8ep196-vl0.4593.hdf5\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5755 - accuracy: 0.8035 - val_loss: 0.4593 - val_accuracy: 0.8439\n",
      "Epoch 197/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5790 - accuracy: 0.8129\n",
      "Epoch 00197: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5715 - accuracy: 0.8117 - val_loss: 0.4696 - val_accuracy: 0.8634\n",
      "Epoch 198/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7901\n",
      "Epoch 00198: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6000 - accuracy: 0.7901 - val_loss: 0.4947 - val_accuracy: 0.8439\n",
      "Epoch 199/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.7866\n",
      "Epoch 00199: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5925 - accuracy: 0.7866 - val_loss: 0.4796 - val_accuracy: 0.8488\n",
      "Epoch 200/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.7831\n",
      "Epoch 00200: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.6125 - accuracy: 0.7831 - val_loss: 0.4822 - val_accuracy: 0.8439\n",
      "Epoch 201/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5919 - accuracy: 0.7895\n",
      "Epoch 00201: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6096 - accuracy: 0.7883 - val_loss: 0.5167 - val_accuracy: 0.8244\n",
      "Epoch 202/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5939 - accuracy: 0.8059\n",
      "Epoch 00202: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5915 - accuracy: 0.8058 - val_loss: 0.4725 - val_accuracy: 0.8585\n",
      "Epoch 203/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6534 - accuracy: 0.7782\n",
      "Epoch 00203: val_loss did not improve from 0.45931\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6420 - accuracy: 0.7819 - val_loss: 0.5064 - val_accuracy: 0.8439\n",
      "Epoch 204/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5985 - accuracy: 0.7883\n",
      "Epoch 00204: val_loss improved from 0.45931 to 0.44784, saving model to ./data/cvision/model_9-8\\model_9-8ep204-vl0.4478.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.6020 - accuracy: 0.7878 - val_loss: 0.4478 - val_accuracy: 0.8634\n",
      "Epoch 205/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5930 - accuracy: 0.7994\n",
      "Epoch 00205: val_loss did not improve from 0.44784\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5930 - accuracy: 0.7994 - val_loss: 0.4863 - val_accuracy: 0.8390\n",
      "Epoch 206/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.8134\n",
      "Epoch 00206: val_loss did not improve from 0.44784\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5650 - accuracy: 0.8134 - val_loss: 0.4810 - val_accuracy: 0.8488\n",
      "Epoch 207/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5823 - accuracy: 0.7921\n",
      "Epoch 00207: val_loss did not improve from 0.44784\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5858 - accuracy: 0.7959 - val_loss: 0.5233 - val_accuracy: 0.8244\n",
      "Epoch 208/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.8064\n",
      "Epoch 00208: val_loss improved from 0.44784 to 0.42996, saving model to ./data/cvision/model_9-8\\model_9-8ep208-vl0.4300.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5738 - accuracy: 0.8064 - val_loss: 0.4300 - val_accuracy: 0.8390\n",
      "Epoch 209/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5763 - accuracy: 0.7990\n",
      "Epoch 00209: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5786 - accuracy: 0.7983 - val_loss: 0.4708 - val_accuracy: 0.8439\n",
      "Epoch 210/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6332 - accuracy: 0.7883 ETA: 0s - loss: 0.5715 - accuracy: \n",
      "Epoch 00210: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.6221 - accuracy: 0.7913 - val_loss: 0.4731 - val_accuracy: 0.8244\n",
      "Epoch 211/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5635 - accuracy: 0.8122\n",
      "Epoch 00211: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5627 - accuracy: 0.8111 - val_loss: 0.4362 - val_accuracy: 0.8488\n",
      "Epoch 212/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5733 - accuracy: 0.7999\n",
      "Epoch 00212: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5684 - accuracy: 0.8012 - val_loss: 0.4552 - val_accuracy: 0.8439\n",
      "Epoch 213/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5844 - accuracy: 0.7908\n",
      "Epoch 00213: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5867 - accuracy: 0.7878 - val_loss: 0.4616 - val_accuracy: 0.8488\n",
      "Epoch 214/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.7988\n",
      "Epoch 00214: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5728 - accuracy: 0.7988 - val_loss: 0.4393 - val_accuracy: 0.8439\n",
      "Epoch 215/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5933 - accuracy: 0.7990\n",
      "Epoch 00215: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5913 - accuracy: 0.8012 - val_loss: 0.4368 - val_accuracy: 0.8488\n",
      "Epoch 216/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5495 - accuracy: 0.8137\n",
      "Epoch 00216: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5511 - accuracy: 0.8108 - val_loss: 0.4884 - val_accuracy: 0.8195\n",
      "Epoch 217/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.7965\n",
      "Epoch 00217: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5798 - accuracy: 0.7965 - val_loss: 0.4526 - val_accuracy: 0.8390\n",
      "Epoch 218/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.8117\n",
      "Epoch 00218: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5523 - accuracy: 0.8117 - val_loss: 0.4702 - val_accuracy: 0.8439\n",
      "Epoch 219/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.8070\n",
      "Epoch 00219: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5661 - accuracy: 0.8070 - val_loss: 0.4516 - val_accuracy: 0.8488\n",
      "Epoch 220/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.7889\n",
      "Epoch 00220: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5817 - accuracy: 0.7889 - val_loss: 0.4905 - val_accuracy: 0.8439\n",
      "Epoch 221/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8181\n",
      "Epoch 00221: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5360 - accuracy: 0.8181 - val_loss: 0.4755 - val_accuracy: 0.8341\n",
      "Epoch 222/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5966 - accuracy: 0.7879\n",
      "Epoch 00222: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5977 - accuracy: 0.7878 - val_loss: 0.4645 - val_accuracy: 0.8585\n",
      "Epoch 223/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5729 - accuracy: 0.8059\n",
      "Epoch 00223: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5828 - accuracy: 0.8000 - val_loss: 0.4387 - val_accuracy: 0.8488\n",
      "Epoch 224/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.8181\n",
      "Epoch 00224: val_loss did not improve from 0.42996\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5241 - accuracy: 0.8181 - val_loss: 0.4860 - val_accuracy: 0.8390\n",
      "Epoch 225/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5608 - accuracy: 0.8011\n",
      "Epoch 00225: val_loss improved from 0.42996 to 0.42771, saving model to ./data/cvision/model_9-8\\model_9-8ep225-vl0.4277.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5659 - accuracy: 0.7983 - val_loss: 0.4277 - val_accuracy: 0.8585\n",
      "Epoch 226/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5517 - accuracy: 0.8078\n",
      "Epoch 00226: val_loss did not improve from 0.42771\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5546 - accuracy: 0.8082 - val_loss: 0.4326 - val_accuracy: 0.8634\n",
      "Epoch 227/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5492 - accuracy: 0.8083\n",
      "Epoch 00227: val_loss did not improve from 0.42771\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5498 - accuracy: 0.8076 - val_loss: 0.4733 - val_accuracy: 0.8488\n",
      "Epoch 228/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.8041\n",
      "Epoch 00228: val_loss did not improve from 0.42771\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5796 - accuracy: 0.8041 - val_loss: 0.4438 - val_accuracy: 0.8341\n",
      "Epoch 229/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5305 - accuracy: 0.8198\n",
      "Epoch 00229: val_loss improved from 0.42771 to 0.41664, saving model to ./data/cvision/model_9-8\\model_9-8ep229-vl0.4166.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.5303 - accuracy: 0.8152 - val_loss: 0.4166 - val_accuracy: 0.8537\n",
      "Epoch 230/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5219 - accuracy: 0.8318\n",
      "Epoch 00230: val_loss did not improve from 0.41664\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5189 - accuracy: 0.8315 - val_loss: 0.4711 - val_accuracy: 0.8585\n",
      "Epoch 231/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8222\n",
      "Epoch 00231: val_loss improved from 0.41664 to 0.40774, saving model to ./data/cvision/model_9-8\\model_9-8ep231-vl0.4077.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5103 - accuracy: 0.8222 - val_loss: 0.4077 - val_accuracy: 0.8683\n",
      "Epoch 232/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4704 - accuracy: 0.8412\n",
      "Epoch 00232: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4790 - accuracy: 0.8385 - val_loss: 0.4160 - val_accuracy: 0.8537\n",
      "Epoch 233/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5264 - accuracy: 0.8173\n",
      "Epoch 00233: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5369 - accuracy: 0.8146 - val_loss: 0.4155 - val_accuracy: 0.8439\n",
      "Epoch 234/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5264 - accuracy: 0.8192\n",
      "Epoch 00234: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5298 - accuracy: 0.8192 - val_loss: 0.4362 - val_accuracy: 0.8634\n",
      "Epoch 235/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5314 - accuracy: 0.8116\n",
      "Epoch 00235: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5263 - accuracy: 0.8140 - val_loss: 0.4363 - val_accuracy: 0.8390\n",
      "Epoch 236/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5200 - accuracy: 0.8192\n",
      "Epoch 00236: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5253 - accuracy: 0.8192 - val_loss: 0.4355 - val_accuracy: 0.8585\n",
      "Epoch 237/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5616 - accuracy: 0.7933\n",
      "Epoch 00237: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5535 - accuracy: 0.7969 - val_loss: 0.4169 - val_accuracy: 0.8683\n",
      "Epoch 238/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5517 - accuracy: 0.8122\n",
      "Epoch 00238: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5490 - accuracy: 0.8140 - val_loss: 0.4535 - val_accuracy: 0.8634\n",
      "Epoch 239/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5707 - accuracy: 0.7996\n",
      "Epoch 00239: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5662 - accuracy: 0.8041 - val_loss: 0.4466 - val_accuracy: 0.8439\n",
      "Epoch 240/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8152\n",
      "Epoch 00240: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5297 - accuracy: 0.8152 - val_loss: 0.4104 - val_accuracy: 0.8390\n",
      "Epoch 241/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5469 - accuracy: 0.8047\n",
      "Epoch 00241: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5407 - accuracy: 0.8070 - val_loss: 0.4364 - val_accuracy: 0.8585\n",
      "Epoch 242/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.8064\n",
      "Epoch 00242: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5414 - accuracy: 0.8064 - val_loss: 0.4405 - val_accuracy: 0.8293\n",
      "Epoch 243/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.8146\n",
      "Epoch 00243: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5343 - accuracy: 0.8146 - val_loss: 0.4585 - val_accuracy: 0.8293\n",
      "Epoch 244/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8023\n",
      "Epoch 00244: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5619 - accuracy: 0.8023 - val_loss: 0.4192 - val_accuracy: 0.8634\n",
      "Epoch 245/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.8297\n",
      "Epoch 00245: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5245 - accuracy: 0.8297 - val_loss: 0.4343 - val_accuracy: 0.8341\n",
      "Epoch 246/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.8146\n",
      "Epoch 00246: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5552 - accuracy: 0.8146 - val_loss: 0.4738 - val_accuracy: 0.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5610 - accuracy: 0.8135\n",
      "Epoch 00247: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5611 - accuracy: 0.8134 - val_loss: 0.4303 - val_accuracy: 0.8488\n",
      "Epoch 248/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4846 - accuracy: 0.8374\n",
      "Epoch 00248: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4835 - accuracy: 0.8397 - val_loss: 0.4661 - val_accuracy: 0.8293\n",
      "Epoch 249/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4931 - accuracy: 0.8236\n",
      "Epoch 00249: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4956 - accuracy: 0.8227 - val_loss: 0.4663 - val_accuracy: 0.8537\n",
      "Epoch 250/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5059 - accuracy: 0.8204\n",
      "Epoch 00250: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5229 - accuracy: 0.8163 - val_loss: 0.4295 - val_accuracy: 0.8488\n",
      "Epoch 251/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.8111\n",
      "Epoch 00251: val_loss did not improve from 0.40774\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5269 - accuracy: 0.8111 - val_loss: 0.4279 - val_accuracy: 0.8537\n",
      "Epoch 252/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8204\n",
      "Epoch 00252: val_loss improved from 0.40774 to 0.39807, saving model to ./data/cvision/model_9-8\\model_9-8ep252-vl0.3981.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5108 - accuracy: 0.8204 - val_loss: 0.3981 - val_accuracy: 0.8488\n",
      "Epoch 253/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5005 - accuracy: 0.8317\n",
      "Epoch 00253: val_loss did not improve from 0.39807\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5023 - accuracy: 0.8292 - val_loss: 0.4394 - val_accuracy: 0.8585\n",
      "Epoch 254/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8239\n",
      "Epoch 00254: val_loss did not improve from 0.39807\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5173 - accuracy: 0.8239 - val_loss: 0.4262 - val_accuracy: 0.8585\n",
      "Epoch 255/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5175 - accuracy: 0.8280\n",
      "Epoch 00255: val_loss did not improve from 0.39807\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5175 - accuracy: 0.8280 - val_loss: 0.3984 - val_accuracy: 0.8488\n",
      "Epoch 256/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5258 - accuracy: 0.8161\n",
      "Epoch 00256: val_loss did not improve from 0.39807\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5221 - accuracy: 0.8186 - val_loss: 0.4887 - val_accuracy: 0.8293\n",
      "Epoch 257/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5266 - accuracy: 0.8129\n",
      "Epoch 00257: val_loss did not improve from 0.39807\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5385 - accuracy: 0.8111 - val_loss: 0.4563 - val_accuracy: 0.8244\n",
      "Epoch 258/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4965 - accuracy: 0.8263\n",
      "Epoch 00258: val_loss improved from 0.39807 to 0.39282, saving model to ./data/cvision/model_9-8\\model_9-8ep258-vl0.3928.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5022 - accuracy: 0.8239 - val_loss: 0.3928 - val_accuracy: 0.8488\n",
      "Epoch 259/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4970 - accuracy: 0.8299\n",
      "Epoch 00259: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4974 - accuracy: 0.8297 - val_loss: 0.4278 - val_accuracy: 0.8537\n",
      "Epoch 260/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5058 - accuracy: 0.8167\n",
      "Epoch 00260: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5028 - accuracy: 0.8192 - val_loss: 0.4425 - val_accuracy: 0.8390\n",
      "Epoch 261/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.8373\n",
      "Epoch 00261: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4857 - accuracy: 0.8373 - val_loss: 0.4517 - val_accuracy: 0.8293\n",
      "Epoch 262/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4823 - accuracy: 0.8343\n",
      "Epoch 00262: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4851 - accuracy: 0.8327 - val_loss: 0.4757 - val_accuracy: 0.8244\n",
      "Epoch 263/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4765 - accuracy: 0.8450\n",
      "Epoch 00263: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4843 - accuracy: 0.8379 - val_loss: 0.4465 - val_accuracy: 0.8195\n",
      "Epoch 264/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.8111\n",
      "Epoch 00264: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5206 - accuracy: 0.8111 - val_loss: 0.4651 - val_accuracy: 0.8537\n",
      "Epoch 265/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.8245\n",
      "Epoch 00265: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4888 - accuracy: 0.8245 - val_loss: 0.4491 - val_accuracy: 0.8439\n",
      "Epoch 266/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4628 - accuracy: 0.8377\n",
      "Epoch 00266: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4670 - accuracy: 0.8367 - val_loss: 0.4455 - val_accuracy: 0.8341\n",
      "Epoch 267/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5190 - accuracy: 0.8280\n",
      "Epoch 00267: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5201 - accuracy: 0.8292 - val_loss: 0.5000 - val_accuracy: 0.8390\n",
      "Epoch 268/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5097 - accuracy: 0.8255\n",
      "Epoch 00268: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5219 - accuracy: 0.8204 - val_loss: 0.4432 - val_accuracy: 0.8439\n",
      "Epoch 269/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4964 - accuracy: 0.8280\n",
      "Epoch 00269: val_loss did not improve from 0.39282\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5011 - accuracy: 0.8292 - val_loss: 0.4319 - val_accuracy: 0.8683\n",
      "Epoch 270/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.8373\n",
      "Epoch 00270: val_loss improved from 0.39282 to 0.37562, saving model to ./data/cvision/model_9-8\\model_9-8ep270-vl0.3756.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4563 - accuracy: 0.8373 - val_loss: 0.3756 - val_accuracy: 0.8683\n",
      "Epoch 271/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4779 - accuracy: 0.8395\n",
      "Epoch 00271: val_loss did not improve from 0.37562\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4743 - accuracy: 0.8414 - val_loss: 0.4622 - val_accuracy: 0.8488\n",
      "Epoch 272/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.8344\n",
      "Epoch 00272: val_loss improved from 0.37562 to 0.37331, saving model to ./data/cvision/model_9-8\\model_9-8ep272-vl0.3733.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.4934 - accuracy: 0.8344 - val_loss: 0.3733 - val_accuracy: 0.8780\n",
      "Epoch 273/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4768 - accuracy: 0.8286\n",
      "Epoch 00273: val_loss did not improve from 0.37331\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4739 - accuracy: 0.8297 - val_loss: 0.4264 - val_accuracy: 0.8683\n",
      "Epoch 274/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.8431\n",
      "Epoch 00274: val_loss did not improve from 0.37331\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4492 - accuracy: 0.8431 - val_loss: 0.3787 - val_accuracy: 0.8488\n",
      "Epoch 275/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4676 - accuracy: 0.8381\n",
      "Epoch 00275: val_loss did not improve from 0.37331\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4703 - accuracy: 0.8373 - val_loss: 0.4412 - val_accuracy: 0.8390\n",
      "Epoch 276/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.5060 - accuracy: 0.8273\n",
      "Epoch 00276: val_loss did not improve from 0.37331\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5039 - accuracy: 0.8274 - val_loss: 0.4727 - val_accuracy: 0.8341\n",
      "Epoch 277/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4880 - accuracy: 0.8343\n",
      "Epoch 00277: val_loss did not improve from 0.37331\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4789 - accuracy: 0.8373 - val_loss: 0.4189 - val_accuracy: 0.8585\n",
      "Epoch 278/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.8309\n",
      "Epoch 00278: val_loss improved from 0.37331 to 0.36142, saving model to ./data/cvision/model_9-8\\model_9-8ep278-vl0.3614.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4936 - accuracy: 0.8309 - val_loss: 0.3614 - val_accuracy: 0.8683\n",
      "Epoch 279/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.8455\n",
      "Epoch 00279: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4652 - accuracy: 0.8455 - val_loss: 0.3857 - val_accuracy: 0.8780\n",
      "Epoch 280/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4533 - accuracy: 0.8321\n",
      "Epoch 00280: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4533 - accuracy: 0.8321 - val_loss: 0.3916 - val_accuracy: 0.8634\n",
      "Epoch 281/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4644 - accuracy: 0.8286\n",
      "Epoch 00281: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4640 - accuracy: 0.8303 - val_loss: 0.3819 - val_accuracy: 0.8585\n",
      "Epoch 282/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4511 - accuracy: 0.8437\n",
      "Epoch 00282: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4530 - accuracy: 0.8420 - val_loss: 0.3933 - val_accuracy: 0.8488\n",
      "Epoch 283/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4772 - accuracy: 0.8336\n",
      "Epoch 00283: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4758 - accuracy: 0.8350 - val_loss: 0.4436 - val_accuracy: 0.8244\n",
      "Epoch 284/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.8192\n",
      "Epoch 00284: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5268 - accuracy: 0.8192 - val_loss: 0.4326 - val_accuracy: 0.8683\n",
      "Epoch 285/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.8362\n",
      "Epoch 00285: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4720 - accuracy: 0.8362 - val_loss: 0.4622 - val_accuracy: 0.8439\n",
      "Epoch 286/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8286\n",
      "Epoch 00286: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4619 - accuracy: 0.8286 - val_loss: 0.4487 - val_accuracy: 0.8439\n",
      "Epoch 287/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4774 - accuracy: 0.8286\n",
      "Epoch 00287: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4783 - accuracy: 0.8292 - val_loss: 0.4235 - val_accuracy: 0.8585\n",
      "Epoch 288/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.8327\n",
      "Epoch 00288: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5210 - accuracy: 0.8327 - val_loss: 0.3909 - val_accuracy: 0.8634\n",
      "Epoch 289/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.8362\n",
      "Epoch 00289: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4545 - accuracy: 0.8362 - val_loss: 0.4051 - val_accuracy: 0.8537\n",
      "Epoch 290/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4711 - accuracy: 0.8292\n",
      "Epoch 00290: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4654 - accuracy: 0.8332 - val_loss: 0.3892 - val_accuracy: 0.8634\n",
      "Epoch 291/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.8408\n",
      "Epoch 00291: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4693 - accuracy: 0.8408 - val_loss: 0.4144 - val_accuracy: 0.8537\n",
      "Epoch 292/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4704 - accuracy: 0.8349\n",
      "Epoch 00292: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4696 - accuracy: 0.8350 - val_loss: 0.3921 - val_accuracy: 0.8780\n",
      "Epoch 293/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.8350\n",
      "Epoch 00293: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4870 - accuracy: 0.8350 - val_loss: 0.4065 - val_accuracy: 0.8585\n",
      "Epoch 294/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4810 - accuracy: 0.8324\n",
      "Epoch 00294: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4819 - accuracy: 0.8350 - val_loss: 0.4533 - val_accuracy: 0.8390\n",
      "Epoch 295/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4330 - accuracy: 0.8418\n",
      "Epoch 00295: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4328 - accuracy: 0.8420 - val_loss: 0.3780 - val_accuracy: 0.8732\n",
      "Epoch 296/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4856 - accuracy: 0.8248\n",
      "Epoch 00296: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4820 - accuracy: 0.8274 - val_loss: 0.4201 - val_accuracy: 0.8390\n",
      "Epoch 297/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8437\n",
      "Epoch 00297: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4430 - accuracy: 0.8437 - val_loss: 0.4363 - val_accuracy: 0.8293\n",
      "Epoch 298/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4181 - accuracy: 0.8557\n",
      "Epoch 00298: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4156 - accuracy: 0.8566 - val_loss: 0.4478 - val_accuracy: 0.8439\n",
      "Epoch 299/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4263 - accuracy: 0.8614\n",
      "Epoch 00299: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4249 - accuracy: 0.8601 - val_loss: 0.4289 - val_accuracy: 0.8634\n",
      "Epoch 300/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4358 - accuracy: 0.8469\n",
      "Epoch 00300: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4323 - accuracy: 0.8455 - val_loss: 0.4362 - val_accuracy: 0.8488\n",
      "Epoch 301/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4403 - accuracy: 0.8399\n",
      "Epoch 00301: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4313 - accuracy: 0.8437 - val_loss: 0.4368 - val_accuracy: 0.8585\n",
      "Epoch 302/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4749 - accuracy: 0.8513\n",
      "Epoch 00302: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4721 - accuracy: 0.8525 - val_loss: 0.3698 - val_accuracy: 0.8683\n",
      "Epoch 303/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4528 - accuracy: 0.8444\n",
      "Epoch 00303: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4530 - accuracy: 0.8461 - val_loss: 0.3957 - val_accuracy: 0.8732\n",
      "Epoch 304/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4570 - accuracy: 0.8389\n",
      "Epoch 00304: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4523 - accuracy: 0.8402 - val_loss: 0.4086 - val_accuracy: 0.8634\n",
      "Epoch 305/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4287 - accuracy: 0.8551\n",
      "Epoch 00305: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4340 - accuracy: 0.8525 - val_loss: 0.4001 - val_accuracy: 0.8488\n",
      "Epoch 306/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4327 - accuracy: 0.8557\n",
      "Epoch 00306: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4372 - accuracy: 0.8525 - val_loss: 0.3657 - val_accuracy: 0.8829\n",
      "Epoch 307/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4435 - accuracy: 0.8399\n",
      "Epoch 00307: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4529 - accuracy: 0.8373 - val_loss: 0.3682 - val_accuracy: 0.8732\n",
      "Epoch 308/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4581 - accuracy: 0.8381\n",
      "Epoch 00308: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4543 - accuracy: 0.8391 - val_loss: 0.3886 - val_accuracy: 0.8780\n",
      "Epoch 309/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4633 - accuracy: 0.8330\n",
      "Epoch 00309: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4581 - accuracy: 0.8332 - val_loss: 0.3797 - val_accuracy: 0.8732\n",
      "Epoch 310/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8449\n",
      "Epoch 00310: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4536 - accuracy: 0.8449 - val_loss: 0.4404 - val_accuracy: 0.8390\n",
      "Epoch 311/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4623 - accuracy: 0.8371\n",
      "Epoch 00311: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4612 - accuracy: 0.8385 - val_loss: 0.4100 - val_accuracy: 0.8780\n",
      "Epoch 312/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4429 - accuracy: 0.8444\n",
      "Epoch 00312: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4460 - accuracy: 0.8449 - val_loss: 0.3894 - val_accuracy: 0.8732\n",
      "Epoch 313/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4487 - accuracy: 0.8425\n",
      "Epoch 00313: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4501 - accuracy: 0.8431 - val_loss: 0.4270 - val_accuracy: 0.8537\n",
      "Epoch 314/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4159 - accuracy: 0.8576\n",
      "Epoch 00314: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4072 - accuracy: 0.8618 - val_loss: 0.3937 - val_accuracy: 0.8732\n",
      "Epoch 315/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.8455\n",
      "Epoch 00315: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4381 - accuracy: 0.8455 - val_loss: 0.4202 - val_accuracy: 0.8732\n",
      "Epoch 316/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.8367\n",
      "Epoch 00316: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4628 - accuracy: 0.8367 - val_loss: 0.4148 - val_accuracy: 0.8732\n",
      "Epoch 317/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.8700\n",
      "Epoch 00317: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4142 - accuracy: 0.8700 - val_loss: 0.3816 - val_accuracy: 0.8732\n",
      "Epoch 318/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4568 - accuracy: 0.8406\n",
      "Epoch 00318: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4655 - accuracy: 0.8362 - val_loss: 0.4112 - val_accuracy: 0.8780\n",
      "Epoch 319/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8542\n",
      "Epoch 00319: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4350 - accuracy: 0.8542 - val_loss: 0.3859 - val_accuracy: 0.8780\n",
      "Epoch 320/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8414\n",
      "Epoch 00320: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4513 - accuracy: 0.8414 - val_loss: 0.3934 - val_accuracy: 0.8683\n",
      "Epoch 321/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8531\n",
      "Epoch 00321: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4221 - accuracy: 0.8531 - val_loss: 0.4318 - val_accuracy: 0.8537\n",
      "Epoch 322/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8379\n",
      "Epoch 00322: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4636 - accuracy: 0.8379 - val_loss: 0.4275 - val_accuracy: 0.8585\n",
      "Epoch 323/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.8478\n",
      "Epoch 00323: val_loss did not improve from 0.36142\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4217 - accuracy: 0.8478 - val_loss: 0.4209 - val_accuracy: 0.8634\n",
      "Epoch 324/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4579 - accuracy: 0.8513\n",
      "Epoch 00324: val_loss improved from 0.36142 to 0.35618, saving model to ./data/cvision/model_9-8\\model_9-8ep324-vl0.3562.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4572 - accuracy: 0.8519 - val_loss: 0.3562 - val_accuracy: 0.8829\n",
      "Epoch 325/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.8501\n",
      "Epoch 00325: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4212 - accuracy: 0.8501 - val_loss: 0.4052 - val_accuracy: 0.8683\n",
      "Epoch 326/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8507\n",
      "Epoch 00326: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4159 - accuracy: 0.8507 - val_loss: 0.4283 - val_accuracy: 0.8585\n",
      "Epoch 327/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8665\n",
      "Epoch 00327: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4169 - accuracy: 0.8665 - val_loss: 0.4270 - val_accuracy: 0.8683\n",
      "Epoch 328/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8630\n",
      "Epoch 00328: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3922 - accuracy: 0.8630 - val_loss: 0.3748 - val_accuracy: 0.8683\n",
      "Epoch 329/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4048 - accuracy: 0.8658\n",
      "Epoch 00329: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3997 - accuracy: 0.8671 - val_loss: 0.4464 - val_accuracy: 0.8585\n",
      "Epoch 330/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3820 - accuracy: 0.8670\n",
      "Epoch 00330: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3909 - accuracy: 0.8641 - val_loss: 0.4568 - val_accuracy: 0.8488\n",
      "Epoch 331/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3987 - accuracy: 0.8536\n",
      "Epoch 00331: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3987 - accuracy: 0.8536 - val_loss: 0.4423 - val_accuracy: 0.8390\n",
      "Epoch 332/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8694\n",
      "Epoch 00332: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3720 - accuracy: 0.8694 - val_loss: 0.4195 - val_accuracy: 0.8829\n",
      "Epoch 333/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3926 - accuracy: 0.8684\n",
      "Epoch 00333: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3892 - accuracy: 0.8694 - val_loss: 0.3604 - val_accuracy: 0.8780\n",
      "Epoch 334/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4597 - accuracy: 0.8444\n",
      "Epoch 00334: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4592 - accuracy: 0.8443 - val_loss: 0.4055 - val_accuracy: 0.8488\n",
      "Epoch 335/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4083 - accuracy: 0.8557\n",
      "Epoch 00335: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4073 - accuracy: 0.8566 - val_loss: 0.4111 - val_accuracy: 0.8585\n",
      "Epoch 336/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4152 - accuracy: 0.8633\n",
      "Epoch 00336: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4151 - accuracy: 0.8606 - val_loss: 0.3867 - val_accuracy: 0.8829\n",
      "Epoch 337/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8548\n",
      "Epoch 00337: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4250 - accuracy: 0.8548 - val_loss: 0.4381 - val_accuracy: 0.8341\n",
      "Epoch 338/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4222 - accuracy: 0.8601\n",
      "Epoch 00338: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4278 - accuracy: 0.8595 - val_loss: 0.4483 - val_accuracy: 0.8390\n",
      "Epoch 339/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3813 - accuracy: 0.8702\n",
      "Epoch 00339: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3913 - accuracy: 0.8665 - val_loss: 0.4327 - val_accuracy: 0.8439\n",
      "Epoch 340/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4022 - accuracy: 0.8595\n",
      "Epoch 00340: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4133 - accuracy: 0.8560 - val_loss: 0.4015 - val_accuracy: 0.8585\n",
      "Epoch 341/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4334 - accuracy: 0.8463\n",
      "Epoch 00341: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4319 - accuracy: 0.8472 - val_loss: 0.4241 - val_accuracy: 0.8537\n",
      "Epoch 342/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4332 - accuracy: 0.8519\n",
      "Epoch 00342: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4366 - accuracy: 0.8501 - val_loss: 0.4237 - val_accuracy: 0.8683\n",
      "Epoch 343/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4201 - accuracy: 0.8601\n",
      "Epoch 00343: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4154 - accuracy: 0.8630 - val_loss: 0.4131 - val_accuracy: 0.8488\n",
      "Epoch 344/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.8636\n",
      "Epoch 00344: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3891 - accuracy: 0.8636 - val_loss: 0.4298 - val_accuracy: 0.8195\n",
      "Epoch 345/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3806 - accuracy: 0.8614\n",
      "Epoch 00345: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3733 - accuracy: 0.8636 - val_loss: 0.4317 - val_accuracy: 0.8244\n",
      "Epoch 346/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4073 - accuracy: 0.8683\n",
      "Epoch 00346: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4017 - accuracy: 0.8711 - val_loss: 0.4358 - val_accuracy: 0.8439\n",
      "Epoch 347/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8566\n",
      "Epoch 00347: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3924 - accuracy: 0.8566 - val_loss: 0.4333 - val_accuracy: 0.8488\n",
      "Epoch 348/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8601\n",
      "Epoch 00348: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4214 - accuracy: 0.8601 - val_loss: 0.4171 - val_accuracy: 0.8829\n",
      "Epoch 349/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4101 - accuracy: 0.8589\n",
      "Epoch 00349: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4045 - accuracy: 0.8630 - val_loss: 0.4376 - val_accuracy: 0.8585\n",
      "Epoch 350/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4422 - accuracy: 0.8462\n",
      "Epoch 00350: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4383 - accuracy: 0.8471 - val_loss: 0.4654 - val_accuracy: 0.8585\n",
      "Epoch 351/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3852 - accuracy: 0.8683\n",
      "Epoch 00351: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3907 - accuracy: 0.8659 - val_loss: 0.3967 - val_accuracy: 0.8634\n",
      "Epoch 352/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8501\n",
      "Epoch 00352: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4268 - accuracy: 0.8501 - val_loss: 0.3845 - val_accuracy: 0.8585\n",
      "Epoch 353/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4012 - accuracy: 0.8683\n",
      "Epoch 00353: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4044 - accuracy: 0.8682 - val_loss: 0.3895 - val_accuracy: 0.8683\n",
      "Epoch 354/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4077 - accuracy: 0.86 - ETA: 0s - loss: 0.4136 - accuracy: 0.8607\n",
      "Epoch 00354: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4047 - accuracy: 0.8612 - val_loss: 0.4060 - val_accuracy: 0.8732\n",
      "Epoch 355/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3992 - accuracy: 0.8652\n",
      "Epoch 00355: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3998 - accuracy: 0.8653 - val_loss: 0.4052 - val_accuracy: 0.8585\n",
      "Epoch 356/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3798 - accuracy: 0.8740\n",
      "Epoch 00356: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3741 - accuracy: 0.8770 - val_loss: 0.4388 - val_accuracy: 0.8537\n",
      "Epoch 357/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4039 - accuracy: 0.8540\n",
      "Epoch 00357: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4013 - accuracy: 0.8548 - val_loss: 0.4023 - val_accuracy: 0.8537\n",
      "Epoch 358/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.8583\n",
      "Epoch 00358: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3852 - accuracy: 0.8583 - val_loss: 0.4313 - val_accuracy: 0.8439\n",
      "Epoch 359/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8618\n",
      "Epoch 00359: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3914 - accuracy: 0.8618 - val_loss: 0.4440 - val_accuracy: 0.8098\n",
      "Epoch 360/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8560\n",
      "Epoch 00360: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4154 - accuracy: 0.8560 - val_loss: 0.4191 - val_accuracy: 0.8537\n",
      "Epoch 361/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4257 - accuracy: 0.8570\n",
      "Epoch 00361: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4192 - accuracy: 0.8605 - val_loss: 0.4040 - val_accuracy: 0.8390\n",
      "Epoch 362/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3927 - accuracy: 0.8696\n",
      "Epoch 00362: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3896 - accuracy: 0.8717 - val_loss: 0.4954 - val_accuracy: 0.8195\n",
      "Epoch 363/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3760 - accuracy: 0.8690\n",
      "Epoch 00363: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3721 - accuracy: 0.8683 - val_loss: 0.4007 - val_accuracy: 0.8634\n",
      "Epoch 364/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8606\n",
      "Epoch 00364: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3875 - accuracy: 0.8606 - val_loss: 0.4226 - val_accuracy: 0.8780\n",
      "Epoch 365/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3898 - accuracy: 0.8607\n",
      "Epoch 00365: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3892 - accuracy: 0.8618 - val_loss: 0.4202 - val_accuracy: 0.8683\n",
      "Epoch 366/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4003 - accuracy: 0.8645\n",
      "Epoch 00366: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3951 - accuracy: 0.8653 - val_loss: 0.4189 - val_accuracy: 0.8829\n",
      "Epoch 367/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3919 - accuracy: 0.8614\n",
      "Epoch 00367: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3939 - accuracy: 0.8601 - val_loss: 0.4228 - val_accuracy: 0.8585\n",
      "Epoch 368/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.8694\n",
      "Epoch 00368: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3750 - accuracy: 0.8694 - val_loss: 0.4414 - val_accuracy: 0.8488\n",
      "Epoch 369/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3896 - accuracy: 0.8614\n",
      "Epoch 00369: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3880 - accuracy: 0.8618 - val_loss: 0.4372 - val_accuracy: 0.8439\n",
      "Epoch 370/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.8577 ETA: 0s - loss: 0.4276 - accuracy: \n",
      "Epoch 00370: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4038 - accuracy: 0.8577 - val_loss: 0.4289 - val_accuracy: 0.8634\n",
      "Epoch 371/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3903 - accuracy: 0.8654\n",
      "Epoch 00371: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4023 - accuracy: 0.8616 - val_loss: 0.4385 - val_accuracy: 0.8488\n",
      "Epoch 372/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3706 - accuracy: 0.8708\n",
      "Epoch 00372: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3803 - accuracy: 0.8682 - val_loss: 0.3930 - val_accuracy: 0.8634\n",
      "Epoch 373/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3897 - accuracy: 0.8645\n",
      "Epoch 00373: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3916 - accuracy: 0.8630 - val_loss: 0.4333 - val_accuracy: 0.8488\n",
      "Epoch 374/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.8606\n",
      "Epoch 00374: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3934 - accuracy: 0.8606 - val_loss: 0.4513 - val_accuracy: 0.8683\n",
      "Epoch 375/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3559 - accuracy: 0.8759\n",
      "Epoch 00375: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3733 - accuracy: 0.8700 - val_loss: 0.3733 - val_accuracy: 0.8732\n",
      "Epoch 376/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3567 - accuracy: 0.8759\n",
      "Epoch 00376: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3656 - accuracy: 0.8711 - val_loss: 0.4375 - val_accuracy: 0.8585\n",
      "Epoch 377/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4001 - accuracy: 0.8488\n",
      "Epoch 00377: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4060 - accuracy: 0.8496 - val_loss: 0.3931 - val_accuracy: 0.8634\n",
      "Epoch 378/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8612\n",
      "Epoch 00378: val_loss did not improve from 0.35618\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4149 - accuracy: 0.8612 - val_loss: 0.4053 - val_accuracy: 0.8732\n",
      "Epoch 379/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3938 - accuracy: 0.8633\n",
      "Epoch 00379: val_loss improved from 0.35618 to 0.34974, saving model to ./data/cvision/model_9-8\\model_9-8ep379-vl0.3497.hdf5\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.3922 - accuracy: 0.8647 - val_loss: 0.3497 - val_accuracy: 0.8732\n",
      "Epoch 380/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8729\n",
      "Epoch 00380: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3658 - accuracy: 0.8729 - val_loss: 0.4812 - val_accuracy: 0.8537\n",
      "Epoch 381/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3509 - accuracy: 0.8684\n",
      "Epoch 00381: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3597 - accuracy: 0.8659 - val_loss: 0.4690 - val_accuracy: 0.8585\n",
      "Epoch 382/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3882 - accuracy: 0.8696\n",
      "Epoch 00382: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3849 - accuracy: 0.8700 - val_loss: 0.3885 - val_accuracy: 0.8683\n",
      "Epoch 383/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8606\n",
      "Epoch 00383: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4169 - accuracy: 0.8606 - val_loss: 0.3815 - val_accuracy: 0.8780\n",
      "Epoch 384/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8776\n",
      "Epoch 00384: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3718 - accuracy: 0.8776 - val_loss: 0.3887 - val_accuracy: 0.8878\n",
      "Epoch 385/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3674 - accuracy: 0.8708\n",
      "Epoch 00385: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3776 - accuracy: 0.8700 - val_loss: 0.3914 - val_accuracy: 0.8634\n",
      "Epoch 386/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.8536\n",
      "Epoch 00386: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4044 - accuracy: 0.8536 - val_loss: 0.4330 - val_accuracy: 0.8390\n",
      "Epoch 387/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4056 - accuracy: 0.8538\n",
      "Epoch 00387: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4025 - accuracy: 0.8548 - val_loss: 0.4770 - val_accuracy: 0.8293\n",
      "Epoch 388/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3856 - accuracy: 0.8576\n",
      "Epoch 00388: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3812 - accuracy: 0.8606 - val_loss: 0.4035 - val_accuracy: 0.8732\n",
      "Epoch 389/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3955 - accuracy: 0.8601\n",
      "Epoch 00389: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3955 - accuracy: 0.8601 - val_loss: 0.4302 - val_accuracy: 0.8537\n",
      "Epoch 390/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3776 - accuracy: 0.8645\n",
      "Epoch 00390: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3753 - accuracy: 0.8659 - val_loss: 0.3931 - val_accuracy: 0.8780\n",
      "Epoch 391/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3475 - accuracy: 0.8708\n",
      "Epoch 00391: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3558 - accuracy: 0.8659 - val_loss: 0.4255 - val_accuracy: 0.8683\n",
      "Epoch 392/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.8770\n",
      "Epoch 00392: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3622 - accuracy: 0.8770 - val_loss: 0.4899 - val_accuracy: 0.8537\n",
      "Epoch 393/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3738 - accuracy: 0.8752\n",
      "Epoch 00393: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3740 - accuracy: 0.8735 - val_loss: 0.4039 - val_accuracy: 0.8683\n",
      "Epoch 394/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.8671\n",
      "Epoch 00394: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3808 - accuracy: 0.8671 - val_loss: 0.3964 - val_accuracy: 0.8927\n",
      "Epoch 395/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8653\n",
      "Epoch 00395: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3865 - accuracy: 0.8653 - val_loss: 0.4047 - val_accuracy: 0.8878\n",
      "Epoch 396/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4092 - accuracy: 0.8531\n",
      "Epoch 00396: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4092 - accuracy: 0.8531 - val_loss: 0.4459 - val_accuracy: 0.8585\n",
      "Epoch 397/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3716 - accuracy: 0.8582\n",
      "Epoch 00397: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3778 - accuracy: 0.8583 - val_loss: 0.3642 - val_accuracy: 0.8780\n",
      "Epoch 398/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3922 - accuracy: 0.8727\n",
      "Epoch 00398: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3806 - accuracy: 0.8770 - val_loss: 0.3969 - val_accuracy: 0.8585\n",
      "Epoch 399/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3793 - accuracy: 0.8689\n",
      "Epoch 00399: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3759 - accuracy: 0.8723 - val_loss: 0.4228 - val_accuracy: 0.8732\n",
      "Epoch 400/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8752\n",
      "Epoch 00400: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3648 - accuracy: 0.8752 - val_loss: 0.3804 - val_accuracy: 0.8732\n",
      "Epoch 401/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8583\n",
      "Epoch 00401: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3872 - accuracy: 0.8583 - val_loss: 0.4484 - val_accuracy: 0.8634\n",
      "Epoch 402/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.8700\n",
      "Epoch 00402: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3797 - accuracy: 0.8700 - val_loss: 0.3985 - val_accuracy: 0.8537\n",
      "Epoch 403/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8776\n",
      "Epoch 00403: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3626 - accuracy: 0.8776 - val_loss: 0.3932 - val_accuracy: 0.8683\n",
      "Epoch 404/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8682\n",
      "Epoch 00404: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3872 - accuracy: 0.8682 - val_loss: 0.4462 - val_accuracy: 0.8488\n",
      "Epoch 405/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3378 - accuracy: 0.8803\n",
      "Epoch 00405: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3341 - accuracy: 0.8816 - val_loss: 0.4570 - val_accuracy: 0.8683\n",
      "Epoch 406/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.8735\n",
      "Epoch 00406: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3860 - accuracy: 0.8735 - val_loss: 0.4174 - val_accuracy: 0.8878\n",
      "Epoch 407/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8770\n",
      "Epoch 00407: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3594 - accuracy: 0.8770 - val_loss: 0.4346 - val_accuracy: 0.8390\n",
      "Epoch 408/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3736 - accuracy: 0.8683\n",
      "Epoch 00408: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3850 - accuracy: 0.8641 - val_loss: 0.3986 - val_accuracy: 0.8829\n",
      "Epoch 409/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.8659\n",
      "Epoch 00409: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3868 - accuracy: 0.8659 - val_loss: 0.4416 - val_accuracy: 0.8585\n",
      "Epoch 410/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3684 - accuracy: 0.8765\n",
      "Epoch 00410: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3638 - accuracy: 0.8770 - val_loss: 0.3905 - val_accuracy: 0.8683\n",
      "Epoch 411/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3858 - accuracy: 0.8652\n",
      "Epoch 00411: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3852 - accuracy: 0.8647 - val_loss: 0.3620 - val_accuracy: 0.8927\n",
      "Epoch 412/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3677 - accuracy: 0.8752\n",
      "Epoch 00412: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3612 - accuracy: 0.8770 - val_loss: 0.4185 - val_accuracy: 0.8683\n",
      "Epoch 413/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3783 - accuracy: 0.8683\n",
      "Epoch 00413: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3835 - accuracy: 0.8676 - val_loss: 0.4242 - val_accuracy: 0.8683\n",
      "Epoch 414/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3514 - accuracy: 0.8864\n",
      "Epoch 00414: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3511 - accuracy: 0.8857 - val_loss: 0.3789 - val_accuracy: 0.8829\n",
      "Epoch 415/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3491 - accuracy: 0.8746\n",
      "Epoch 00415: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3548 - accuracy: 0.8735 - val_loss: 0.4289 - val_accuracy: 0.8732\n",
      "Epoch 416/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.8723\n",
      "Epoch 00416: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3619 - accuracy: 0.8723 - val_loss: 0.3699 - val_accuracy: 0.8780\n",
      "Epoch 417/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.8851\n",
      "Epoch 00417: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3467 - accuracy: 0.8851 - val_loss: 0.3750 - val_accuracy: 0.8878\n",
      "Epoch 418/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3723 - accuracy: 0.8740\n",
      "Epoch 00418: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3709 - accuracy: 0.8776 - val_loss: 0.3827 - val_accuracy: 0.8878\n",
      "Epoch 419/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3844 - accuracy: 0.8689\n",
      "Epoch 00419: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3776 - accuracy: 0.8700 - val_loss: 0.4363 - val_accuracy: 0.8634\n",
      "Epoch 420/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3411 - accuracy: 0.8756\n",
      "Epoch 00420: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3350 - accuracy: 0.8787 - val_loss: 0.4050 - val_accuracy: 0.8439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3844 - accuracy: 0.8727\n",
      "Epoch 00421: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3758 - accuracy: 0.8741 - val_loss: 0.3507 - val_accuracy: 0.8780\n",
      "Epoch 422/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8723\n",
      "Epoch 00422: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3631 - accuracy: 0.8723 - val_loss: 0.3928 - val_accuracy: 0.8780\n",
      "Epoch 423/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3503 - accuracy: 0.8721\n",
      "Epoch 00423: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3472 - accuracy: 0.8735 - val_loss: 0.3888 - val_accuracy: 0.8585\n",
      "Epoch 424/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3540 - accuracy: 0.8815\n",
      "Epoch 00424: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3445 - accuracy: 0.8851 - val_loss: 0.3803 - val_accuracy: 0.8732\n",
      "Epoch 425/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8729\n",
      "Epoch 00425: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3526 - accuracy: 0.8729 - val_loss: 0.4810 - val_accuracy: 0.8341\n",
      "Epoch 426/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8828\n",
      "Epoch 00426: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3465 - accuracy: 0.8828 - val_loss: 0.4389 - val_accuracy: 0.8488\n",
      "Epoch 427/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3339 - accuracy: 0.8732\n",
      "Epoch 00427: val_loss did not improve from 0.34974\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3303 - accuracy: 0.8750 - val_loss: 0.4344 - val_accuracy: 0.8537\n",
      "Epoch 428/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3167 - accuracy: 0.8942\n",
      "Epoch 00428: val_loss improved from 0.34974 to 0.34842, saving model to ./data/cvision/model_9-8\\model_9-8ep428-vl0.3484.hdf5\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3148 - accuracy: 0.8950 - val_loss: 0.3484 - val_accuracy: 0.8780\n",
      "Epoch 429/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3348 - accuracy: 0.8803\n",
      "Epoch 00429: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3378 - accuracy: 0.8793 - val_loss: 0.3681 - val_accuracy: 0.8829\n",
      "Epoch 430/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.8799\n",
      "Epoch 00430: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3397 - accuracy: 0.8799 - val_loss: 0.3825 - val_accuracy: 0.8829\n",
      "Epoch 431/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3244 - accuracy: 0.8918\n",
      "Epoch 00431: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3211 - accuracy: 0.8929 - val_loss: 0.3904 - val_accuracy: 0.8829\n",
      "Epoch 432/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3421 - accuracy: 0.8815\n",
      "Epoch 00432: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3395 - accuracy: 0.8828 - val_loss: 0.3763 - val_accuracy: 0.8634\n",
      "Epoch 433/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3227 - accuracy: 0.8935\n",
      "Epoch 00433: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3239 - accuracy: 0.8933 - val_loss: 0.4853 - val_accuracy: 0.8537\n",
      "Epoch 434/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8869\n",
      "Epoch 00434: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3258 - accuracy: 0.8869 - val_loss: 0.4087 - val_accuracy: 0.8585\n",
      "Epoch 435/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3259 - accuracy: 0.8771\n",
      "Epoch 00435: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3273 - accuracy: 0.8770 - val_loss: 0.4350 - val_accuracy: 0.8585\n",
      "Epoch 436/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3316 - accuracy: 0.8891\n",
      "Epoch 00436: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3361 - accuracy: 0.8869 - val_loss: 0.4039 - val_accuracy: 0.8537\n",
      "Epoch 437/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3601 - accuracy: 0.8790\n",
      "Epoch 00437: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3572 - accuracy: 0.8810 - val_loss: 0.3995 - val_accuracy: 0.8634\n",
      "Epoch 438/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8735\n",
      "Epoch 00438: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3594 - accuracy: 0.8735 - val_loss: 0.3960 - val_accuracy: 0.8732\n",
      "Epoch 439/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3242 - accuracy: 0.8872\n",
      "Epoch 00439: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3268 - accuracy: 0.8840 - val_loss: 0.3721 - val_accuracy: 0.8732\n",
      "Epoch 440/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.4143 - accuracy: 0.8526\n",
      "Epoch 00440: val_loss did not improve from 0.34842\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4053 - accuracy: 0.8577 - val_loss: 0.4245 - val_accuracy: 0.8683\n",
      "Epoch 441/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3692 - accuracy: 0.8633\n",
      "Epoch 00441: val_loss improved from 0.34842 to 0.34625, saving model to ./data/cvision/model_9-8\\model_9-8ep441-vl0.3462.hdf5\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.3661 - accuracy: 0.8636 - val_loss: 0.3462 - val_accuracy: 0.8976\n",
      "Epoch 442/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3185 - accuracy: 0.8948\n",
      "Epoch 00442: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3209 - accuracy: 0.8962 - val_loss: 0.4036 - val_accuracy: 0.8780\n",
      "Epoch 443/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8793\n",
      "Epoch 00443: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3191 - accuracy: 0.8793 - val_loss: 0.4199 - val_accuracy: 0.8732\n",
      "Epoch 444/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3242 - accuracy: 0.8852\n",
      "Epoch 00444: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3219 - accuracy: 0.8862 - val_loss: 0.3828 - val_accuracy: 0.8683\n",
      "Epoch 445/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.8915\n",
      "Epoch 00445: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3311 - accuracy: 0.8915 - val_loss: 0.3863 - val_accuracy: 0.8878\n",
      "Epoch 446/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3480 - accuracy: 0.8876\n",
      "Epoch 00446: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3455 - accuracy: 0.8875 - val_loss: 0.3937 - val_accuracy: 0.8878\n",
      "Epoch 447/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3298 - accuracy: 0.8866\n",
      "Epoch 00447: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3326 - accuracy: 0.8840 - val_loss: 0.4500 - val_accuracy: 0.8683\n",
      "Epoch 448/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3538 - accuracy: 0.8815\n",
      "Epoch 00448: val_loss did not improve from 0.34625\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3500 - accuracy: 0.8816 - val_loss: 0.3825 - val_accuracy: 0.8780\n",
      "Epoch 449/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8956\n",
      "Epoch 00449: val_loss improved from 0.34625 to 0.32798, saving model to ./data/cvision/model_9-8\\model_9-8ep449-vl0.3280.hdf5\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3046 - accuracy: 0.8956 - val_loss: 0.3280 - val_accuracy: 0.8927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3482 - accuracy: 0.8721\n",
      "Epoch 00450: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3378 - accuracy: 0.8764 - val_loss: 0.3918 - val_accuracy: 0.8634\n",
      "Epoch 451/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.8840\n",
      "Epoch 00451: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3285 - accuracy: 0.8840 - val_loss: 0.4370 - val_accuracy: 0.8488\n",
      "Epoch 452/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3527 - accuracy: 0.8658\n",
      "Epoch 00452: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3554 - accuracy: 0.8641 - val_loss: 0.3914 - val_accuracy: 0.8780\n",
      "Epoch 453/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3275 - accuracy: 0.8846\n",
      "Epoch 00453: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3313 - accuracy: 0.8834 - val_loss: 0.3922 - val_accuracy: 0.8683\n",
      "Epoch 454/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8851\n",
      "Epoch 00454: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3181 - accuracy: 0.8851 - val_loss: 0.3947 - val_accuracy: 0.8927\n",
      "Epoch 455/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.8828\n",
      "Epoch 00455: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3106 - accuracy: 0.8828 - val_loss: 0.4976 - val_accuracy: 0.8585\n",
      "Epoch 456/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8758\n",
      "Epoch 00456: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3444 - accuracy: 0.8758 - val_loss: 0.3498 - val_accuracy: 0.8878\n",
      "Epoch 457/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8647\n",
      "Epoch 00457: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3698 - accuracy: 0.8647 - val_loss: 0.3989 - val_accuracy: 0.8634\n",
      "Epoch 458/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3363 - accuracy: 0.8853 ETA: 0s - loss: 0.3251 - accuracy\n",
      "Epoch 00458: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3382 - accuracy: 0.8851 - val_loss: 0.3595 - val_accuracy: 0.8780\n",
      "Epoch 459/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3343 - accuracy: 0.8778\n",
      "Epoch 00459: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3333 - accuracy: 0.8764 - val_loss: 0.3329 - val_accuracy: 0.8878\n",
      "Epoch 460/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8735\n",
      "Epoch 00460: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3409 - accuracy: 0.8735 - val_loss: 0.3762 - val_accuracy: 0.8976\n",
      "Epoch 461/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3321 - accuracy: 0.8863\n",
      "Epoch 00461: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3321 - accuracy: 0.8863 - val_loss: 0.3784 - val_accuracy: 0.8780\n",
      "Epoch 462/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3126 - accuracy: 0.8929\n",
      "Epoch 00462: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3207 - accuracy: 0.8915 - val_loss: 0.3700 - val_accuracy: 0.8829\n",
      "Epoch 463/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8676\n",
      "Epoch 00463: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3486 - accuracy: 0.8676 - val_loss: 0.3539 - val_accuracy: 0.8976\n",
      "Epoch 464/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.8805\n",
      "Epoch 00464: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3352 - accuracy: 0.8805 - val_loss: 0.3996 - val_accuracy: 0.8732\n",
      "Epoch 465/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3407 - accuracy: 0.8771\n",
      "Epoch 00465: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3484 - accuracy: 0.8741 - val_loss: 0.3878 - val_accuracy: 0.8732\n",
      "Epoch 466/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.8869\n",
      "Epoch 00466: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3269 - accuracy: 0.8869 - val_loss: 0.3986 - val_accuracy: 0.8780\n",
      "Epoch 467/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3221 - accuracy: 0.8853\n",
      "Epoch 00467: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3231 - accuracy: 0.8851 - val_loss: 0.3557 - val_accuracy: 0.8780\n",
      "Epoch 468/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3676 - accuracy: 0.8771\n",
      "Epoch 00468: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3619 - accuracy: 0.8805 - val_loss: 0.3680 - val_accuracy: 0.8829\n",
      "Epoch 469/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8962\n",
      "Epoch 00469: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3026 - accuracy: 0.8962 - val_loss: 0.3852 - val_accuracy: 0.8780\n",
      "Epoch 470/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8939\n",
      "Epoch 00470: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2940 - accuracy: 0.8939 - val_loss: 0.3836 - val_accuracy: 0.8829\n",
      "Epoch 471/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3219 - accuracy: 0.8810\n",
      "Epoch 00471: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3220 - accuracy: 0.8816 - val_loss: 0.3683 - val_accuracy: 0.8780\n",
      "Epoch 472/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8997\n",
      "Epoch 00472: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2872 - accuracy: 0.8997 - val_loss: 0.3310 - val_accuracy: 0.8927\n",
      "Epoch 473/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8997\n",
      "Epoch 00473: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2913 - accuracy: 0.8997 - val_loss: 0.4228 - val_accuracy: 0.8878\n",
      "Epoch 474/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3252 - accuracy: 0.8858\n",
      "Epoch 00474: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3237 - accuracy: 0.8857 - val_loss: 0.3770 - val_accuracy: 0.8829\n",
      "Epoch 475/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.8764\n",
      "Epoch 00475: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3357 - accuracy: 0.8764 - val_loss: 0.3955 - val_accuracy: 0.8878\n",
      "Epoch 476/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3200 - accuracy: 0.8891\n",
      "Epoch 00476: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3141 - accuracy: 0.8921 - val_loss: 0.4113 - val_accuracy: 0.8683\n",
      "Epoch 477/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3322 - accuracy: 0.8828\n",
      "Epoch 00477: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3300 - accuracy: 0.8805 - val_loss: 0.4220 - val_accuracy: 0.8732\n",
      "Epoch 478/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.8927\n",
      "Epoch 00478: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2910 - accuracy: 0.8927 - val_loss: 0.3788 - val_accuracy: 0.8829\n",
      "Epoch 479/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9067\n",
      "Epoch 00479: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2782 - accuracy: 0.9067 - val_loss: 0.4027 - val_accuracy: 0.8829\n",
      "Epoch 480/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9009\n",
      "Epoch 00480: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2930 - accuracy: 0.9009 - val_loss: 0.3825 - val_accuracy: 0.8780\n",
      "Epoch 481/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8851\n",
      "Epoch 00481: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3191 - accuracy: 0.8851 - val_loss: 0.4412 - val_accuracy: 0.8634\n",
      "Epoch 482/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3167 - accuracy: 0.8897\n",
      "Epoch 00482: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3252 - accuracy: 0.8875 - val_loss: 0.3593 - val_accuracy: 0.9024\n",
      "Epoch 483/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8904\n",
      "Epoch 00483: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3253 - accuracy: 0.8904 - val_loss: 0.4556 - val_accuracy: 0.8390\n",
      "Epoch 484/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3129 - accuracy: 0.8897\n",
      "Epoch 00484: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3128 - accuracy: 0.8910 - val_loss: 0.3709 - val_accuracy: 0.8683\n",
      "Epoch 485/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.8991\n",
      "Epoch 00485: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2857 - accuracy: 0.8991 - val_loss: 0.4503 - val_accuracy: 0.8585\n",
      "Epoch 486/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3207 - accuracy: 0.8840\n",
      "Epoch 00486: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3187 - accuracy: 0.8840 - val_loss: 0.4057 - val_accuracy: 0.8829\n",
      "Epoch 487/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3023 - accuracy: 0.9030\n",
      "Epoch 00487: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2936 - accuracy: 0.9067 - val_loss: 0.4042 - val_accuracy: 0.8732\n",
      "Epoch 488/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3126 - accuracy: 0.8847\n",
      "Epoch 00488: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3112 - accuracy: 0.8851 - val_loss: 0.4704 - val_accuracy: 0.8341\n",
      "Epoch 489/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.8915\n",
      "Epoch 00489: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3186 - accuracy: 0.8915 - val_loss: 0.4061 - val_accuracy: 0.8634\n",
      "Epoch 490/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2979 - accuracy: 0.8948\n",
      "Epoch 00490: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2892 - accuracy: 0.8979 - val_loss: 0.4300 - val_accuracy: 0.8634\n",
      "Epoch 491/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9067\n",
      "Epoch 00491: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2733 - accuracy: 0.9067 - val_loss: 0.4409 - val_accuracy: 0.8683\n",
      "Epoch 492/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3257 - accuracy: 0.8948\n",
      "Epoch 00492: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3236 - accuracy: 0.8956 - val_loss: 0.4244 - val_accuracy: 0.8732\n",
      "Epoch 493/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.8915\n",
      "Epoch 00493: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3354 - accuracy: 0.8915 - val_loss: 0.3828 - val_accuracy: 0.8878\n",
      "Epoch 494/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8898\n",
      "Epoch 00494: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3207 - accuracy: 0.8898 - val_loss: 0.3979 - val_accuracy: 0.8976\n",
      "Epoch 495/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.8886\n",
      "Epoch 00495: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3178 - accuracy: 0.8886 - val_loss: 0.4540 - val_accuracy: 0.8634\n",
      "Epoch 496/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2916 - accuracy: 0.8978\n",
      "Epoch 00496: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2870 - accuracy: 0.9003 - val_loss: 0.3930 - val_accuracy: 0.8927\n",
      "Epoch 497/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3035 - accuracy: 0.8922\n",
      "Epoch 00497: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3001 - accuracy: 0.8950 - val_loss: 0.3895 - val_accuracy: 0.8829\n",
      "Epoch 498/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3114 - accuracy: 0.8910\n",
      "Epoch 00498: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3071 - accuracy: 0.8939 - val_loss: 0.3822 - val_accuracy: 0.8878\n",
      "Epoch 499/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3106 - accuracy: 0.8859\n",
      "Epoch 00499: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3221 - accuracy: 0.8816 - val_loss: 0.3773 - val_accuracy: 0.8732\n",
      "Epoch 500/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3158 - accuracy: 0.8922\n",
      "Epoch 00500: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3182 - accuracy: 0.8898 - val_loss: 0.3757 - val_accuracy: 0.8585\n",
      "Epoch 501/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.8921\n",
      "Epoch 00501: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3060 - accuracy: 0.8921 - val_loss: 0.3367 - val_accuracy: 0.8780\n",
      "Epoch 502/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2872 - accuracy: 0.9004\n",
      "Epoch 00502: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2827 - accuracy: 0.9020 - val_loss: 0.3799 - val_accuracy: 0.8829\n",
      "Epoch 503/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2850 - accuracy: 0.9049\n",
      "Epoch 00503: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2911 - accuracy: 0.9009 - val_loss: 0.3896 - val_accuracy: 0.8780\n",
      "Epoch 504/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9125\n",
      "Epoch 00504: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2751 - accuracy: 0.9125 - val_loss: 0.4358 - val_accuracy: 0.8634\n",
      "Epoch 505/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8910\n",
      "Epoch 00505: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3011 - accuracy: 0.8910 - val_loss: 0.3530 - val_accuracy: 0.8537\n",
      "Epoch 506/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3099 - accuracy: 0.8866\n",
      "Epoch 00506: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3057 - accuracy: 0.8880 - val_loss: 0.3783 - val_accuracy: 0.8634\n",
      "Epoch 507/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3204 - accuracy: 0.8796\n",
      "Epoch 00507: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3197 - accuracy: 0.8799 - val_loss: 0.4015 - val_accuracy: 0.8829\n",
      "Epoch 508/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3244 - accuracy: 0.8804\n",
      "Epoch 00508: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3198 - accuracy: 0.8828 - val_loss: 0.4192 - val_accuracy: 0.8634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 509/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.8968\n",
      "Epoch 00509: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2979 - accuracy: 0.8968 - val_loss: 0.4224 - val_accuracy: 0.8390\n",
      "Epoch 510/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3123 - accuracy: 0.8897\n",
      "Epoch 00510: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3182 - accuracy: 0.8904 - val_loss: 0.4090 - val_accuracy: 0.8537\n",
      "Epoch 511/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3208 - accuracy: 0.8834\n",
      "Epoch 00511: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3231 - accuracy: 0.8840 - val_loss: 0.4063 - val_accuracy: 0.8780\n",
      "Epoch 512/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2917 - accuracy: 0.8979\n",
      "Epoch 00512: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2921 - accuracy: 0.8968 - val_loss: 0.3874 - val_accuracy: 0.8683\n",
      "Epoch 513/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2954 - accuracy: 0.9017\n",
      "Epoch 00513: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2926 - accuracy: 0.9026 - val_loss: 0.3886 - val_accuracy: 0.8732\n",
      "Epoch 514/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3035 - accuracy: 0.8954\n",
      "Epoch 00514: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3073 - accuracy: 0.8939 - val_loss: 0.4108 - val_accuracy: 0.8732\n",
      "Epoch 515/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9026\n",
      "Epoch 00515: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2960 - accuracy: 0.9026 - val_loss: 0.3826 - val_accuracy: 0.8927\n",
      "Epoch 516/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3083 - accuracy: 0.8906\n",
      "Epoch 00516: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3048 - accuracy: 0.8923 - val_loss: 0.4047 - val_accuracy: 0.8585\n",
      "Epoch 517/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.8939\n",
      "Epoch 00517: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3313 - accuracy: 0.8939 - val_loss: 0.4098 - val_accuracy: 0.8537\n",
      "Epoch 518/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2960 - accuracy: 0.8992\n",
      "Epoch 00518: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3006 - accuracy: 0.8985 - val_loss: 0.4031 - val_accuracy: 0.8683\n",
      "Epoch 519/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8915\n",
      "Epoch 00519: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3065 - accuracy: 0.8915 - val_loss: 0.4318 - val_accuracy: 0.8634\n",
      "Epoch 520/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8956\n",
      "Epoch 00520: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2971 - accuracy: 0.8956 - val_loss: 0.4220 - val_accuracy: 0.8634\n",
      "Epoch 521/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2585 - accuracy: 0.9137\n",
      "Epoch 00521: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2638 - accuracy: 0.9114 - val_loss: 0.4150 - val_accuracy: 0.8927\n",
      "Epoch 522/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8921\n",
      "Epoch 00522: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2926 - accuracy: 0.8921 - val_loss: 0.3952 - val_accuracy: 0.8927\n",
      "Epoch 523/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8962\n",
      "Epoch 00523: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2970 - accuracy: 0.8962 - val_loss: 0.4099 - val_accuracy: 0.8878\n",
      "Epoch 524/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8933\n",
      "Epoch 00524: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3162 - accuracy: 0.8933 - val_loss: 0.3465 - val_accuracy: 0.8927\n",
      "Epoch 525/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3210 - accuracy: 0.8866\n",
      "Epoch 00525: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3147 - accuracy: 0.8898 - val_loss: 0.3527 - val_accuracy: 0.9024\n",
      "Epoch 526/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.9032\n",
      "Epoch 00526: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2828 - accuracy: 0.9032 - val_loss: 0.4195 - val_accuracy: 0.8683\n",
      "Epoch 527/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3248 - accuracy: 0.8866\n",
      "Epoch 00527: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3252 - accuracy: 0.8857 - val_loss: 0.4338 - val_accuracy: 0.8634\n",
      "Epoch 528/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8904\n",
      "Epoch 00528: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2899 - accuracy: 0.8904 - val_loss: 0.3634 - val_accuracy: 0.8829\n",
      "Epoch 529/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2768 - accuracy: 0.8941\n",
      "Epoch 00529: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2718 - accuracy: 0.8950 - val_loss: 0.4268 - val_accuracy: 0.8780\n",
      "Epoch 530/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2743 - accuracy: 0.9030\n",
      "Epoch 00530: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2734 - accuracy: 0.9044 - val_loss: 0.4146 - val_accuracy: 0.8634\n",
      "Epoch 531/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2919 - accuracy: 0.8929\n",
      "Epoch 00531: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3009 - accuracy: 0.8898 - val_loss: 0.3824 - val_accuracy: 0.8927\n",
      "Epoch 532/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9073\n",
      "Epoch 00532: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2668 - accuracy: 0.9073 - val_loss: 0.4356 - val_accuracy: 0.8585\n",
      "Epoch 533/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2979 - accuracy: 0.8967\n",
      "Epoch 00533: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3054 - accuracy: 0.8945 - val_loss: 0.3999 - val_accuracy: 0.8829\n",
      "Epoch 534/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2944 - accuracy: 0.8973\n",
      "Epoch 00534: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2877 - accuracy: 0.8985 - val_loss: 0.4455 - val_accuracy: 0.8585\n",
      "Epoch 535/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8968\n",
      "Epoch 00535: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2848 - accuracy: 0.8968 - val_loss: 0.4221 - val_accuracy: 0.8732\n",
      "Epoch 536/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.8945\n",
      "Epoch 00536: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3013 - accuracy: 0.8945 - val_loss: 0.3825 - val_accuracy: 0.8732\n",
      "Epoch 537/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9050\n",
      "Epoch 00537: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2667 - accuracy: 0.9050 - val_loss: 0.4437 - val_accuracy: 0.8683\n",
      "Epoch 538/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8945\n",
      "Epoch 00538: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3041 - accuracy: 0.8945 - val_loss: 0.4218 - val_accuracy: 0.8634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 539/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2673 - accuracy: 0.9080\n",
      "Epoch 00539: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2685 - accuracy: 0.9050 - val_loss: 0.3947 - val_accuracy: 0.8829\n",
      "Epoch 540/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2813 - accuracy: 0.9055\n",
      "Epoch 00540: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2762 - accuracy: 0.9073 - val_loss: 0.4148 - val_accuracy: 0.8878\n",
      "Epoch 541/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2902 - accuracy: 0.9036\n",
      "Epoch 00541: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2882 - accuracy: 0.9032 - val_loss: 0.4054 - val_accuracy: 0.8780\n",
      "Epoch 542/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3081 - accuracy: 0.8941\n",
      "Epoch 00542: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3032 - accuracy: 0.8945 - val_loss: 0.4066 - val_accuracy: 0.8683\n",
      "Epoch 543/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9038\n",
      "Epoch 00543: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2962 - accuracy: 0.9038 - val_loss: 0.4363 - val_accuracy: 0.8585\n",
      "Epoch 544/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9137\n",
      "Epoch 00544: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2526 - accuracy: 0.9137 - val_loss: 0.4195 - val_accuracy: 0.8927\n",
      "Epoch 545/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9061\n",
      "Epoch 00545: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2707 - accuracy: 0.9061 - val_loss: 0.4794 - val_accuracy: 0.8488\n",
      "Epoch 546/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2869 - accuracy: 0.9023\n",
      "Epoch 00546: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2916 - accuracy: 0.9009 - val_loss: 0.4200 - val_accuracy: 0.8634\n",
      "Epoch 547/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2874 - accuracy: 0.9004\n",
      "Epoch 00547: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2855 - accuracy: 0.9015 - val_loss: 0.4447 - val_accuracy: 0.8585\n",
      "Epoch 548/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2721 - accuracy: 0.9011\n",
      "Epoch 00548: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2660 - accuracy: 0.9032 - val_loss: 0.4168 - val_accuracy: 0.8683\n",
      "Epoch 549/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9108\n",
      "Epoch 00549: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2602 - accuracy: 0.9108 - val_loss: 0.5192 - val_accuracy: 0.8634\n",
      "Epoch 550/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8886\n",
      "Epoch 00550: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3375 - accuracy: 0.8886 - val_loss: 0.3730 - val_accuracy: 0.8829\n",
      "Epoch 551/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8933\n",
      "Epoch 00551: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2912 - accuracy: 0.8933 - val_loss: 0.4303 - val_accuracy: 0.8683\n",
      "Epoch 552/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2592 - accuracy: 0.9093\n",
      "Epoch 00552: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2605 - accuracy: 0.9085 - val_loss: 0.4189 - val_accuracy: 0.8683\n",
      "Epoch 553/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2705 - accuracy: 0.9014\n",
      "Epoch 00553: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2708 - accuracy: 0.9032 - val_loss: 0.3531 - val_accuracy: 0.9024\n",
      "Epoch 554/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2562 - accuracy: 0.9069\n",
      "Epoch 00554: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2547 - accuracy: 0.9085 - val_loss: 0.4255 - val_accuracy: 0.8634\n",
      "Epoch 555/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8980\n",
      "Epoch 00555: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2949 - accuracy: 0.8980 - val_loss: 0.4350 - val_accuracy: 0.8585\n",
      "Epoch 556/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2510 - accuracy: 0.9168\n",
      "Epoch 00556: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2531 - accuracy: 0.9143 - val_loss: 0.3780 - val_accuracy: 0.8829\n",
      "Epoch 557/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2759 - accuracy: 0.9020\n",
      "Epoch 00557: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2777 - accuracy: 0.9007 - val_loss: 0.4372 - val_accuracy: 0.8732\n",
      "Epoch 558/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9172\n",
      "Epoch 00558: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2463 - accuracy: 0.9172 - val_loss: 0.3726 - val_accuracy: 0.8829\n",
      "Epoch 559/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9137\n",
      "Epoch 00559: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2760 - accuracy: 0.9137 - val_loss: 0.4295 - val_accuracy: 0.8634\n",
      "Epoch 560/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9044\n",
      "Epoch 00560: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2730 - accuracy: 0.9044 - val_loss: 0.3833 - val_accuracy: 0.8976\n",
      "Epoch 561/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9073\n",
      "Epoch 00561: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2607 - accuracy: 0.9073 - val_loss: 0.4280 - val_accuracy: 0.8683\n",
      "Epoch 562/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.8857\n",
      "Epoch 00562: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2968 - accuracy: 0.8857 - val_loss: 0.4117 - val_accuracy: 0.8780\n",
      "Epoch 563/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.8945\n",
      "Epoch 00563: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2985 - accuracy: 0.8945 - val_loss: 0.3877 - val_accuracy: 0.8634\n",
      "Epoch 564/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9003\n",
      "Epoch 00564: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2852 - accuracy: 0.9003 - val_loss: 0.3684 - val_accuracy: 0.8927\n",
      "Epoch 565/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2719 - accuracy: 0.9049\n",
      "Epoch 00565: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2794 - accuracy: 0.9038 - val_loss: 0.3703 - val_accuracy: 0.8878\n",
      "Epoch 566/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2576 - accuracy: 0.9080\n",
      "Epoch 00566: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2566 - accuracy: 0.9085 - val_loss: 0.3985 - val_accuracy: 0.8829\n",
      "Epoch 567/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8997 ETA: 0s - loss: 0.2756 - accuracy: 0.\n",
      "Epoch 00567: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2762 - accuracy: 0.8997 - val_loss: 0.4105 - val_accuracy: 0.8780\n",
      "Epoch 568/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9108\n",
      "Epoch 00568: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2663 - accuracy: 0.9108 - val_loss: 0.3546 - val_accuracy: 0.8927\n",
      "Epoch 569/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2597 - accuracy: 0.9105\n",
      "Epoch 00569: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2641 - accuracy: 0.9102 - val_loss: 0.4457 - val_accuracy: 0.8829\n",
      "Epoch 570/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2803 - accuracy: 0.9017\n",
      "Epoch 00570: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2791 - accuracy: 0.9026 - val_loss: 0.4063 - val_accuracy: 0.8732\n",
      "Epoch 571/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2725 - accuracy: 0.9067\n",
      "Epoch 00571: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2747 - accuracy: 0.9055 - val_loss: 0.3965 - val_accuracy: 0.8634\n",
      "Epoch 572/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9102\n",
      "Epoch 00572: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2745 - accuracy: 0.9102 - val_loss: 0.3882 - val_accuracy: 0.8585\n",
      "Epoch 573/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2845 - accuracy: 0.9042\n",
      "Epoch 00573: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2828 - accuracy: 0.9044 - val_loss: 0.4295 - val_accuracy: 0.8634\n",
      "Epoch 574/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9137\n",
      "Epoch 00574: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2596 - accuracy: 0.9137 - val_loss: 0.4374 - val_accuracy: 0.8537\n",
      "Epoch 575/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2973 - accuracy: 0.8929\n",
      "Epoch 00575: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2897 - accuracy: 0.8950 - val_loss: 0.4013 - val_accuracy: 0.8537\n",
      "Epoch 576/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.8985\n",
      "Epoch 00576: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2785 - accuracy: 0.8985 - val_loss: 0.4401 - val_accuracy: 0.8683\n",
      "Epoch 577/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2924 - accuracy: 0.8973\n",
      "Epoch 00577: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2974 - accuracy: 0.8945 - val_loss: 0.4351 - val_accuracy: 0.8732\n",
      "Epoch 578/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2756 - accuracy: 0.9014\n",
      "Epoch 00578: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2753 - accuracy: 0.8991 - val_loss: 0.3782 - val_accuracy: 0.8683\n",
      "Epoch 579/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8910\n",
      "Epoch 00579: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2975 - accuracy: 0.8910 - val_loss: 0.4072 - val_accuracy: 0.8683\n",
      "Epoch 580/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.8927\n",
      "Epoch 00580: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3040 - accuracy: 0.8927 - val_loss: 0.4697 - val_accuracy: 0.8585\n",
      "Epoch 581/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3133 - accuracy: 0.8960\n",
      "Epoch 00581: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3090 - accuracy: 0.8980 - val_loss: 0.3749 - val_accuracy: 0.8732\n",
      "Epoch 582/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9061\n",
      "Epoch 00582: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2777 - accuracy: 0.9061 - val_loss: 0.3901 - val_accuracy: 0.8927\n",
      "Epoch 583/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2679 - accuracy: 0.9099\n",
      "Epoch 00583: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2705 - accuracy: 0.9073 - val_loss: 0.3614 - val_accuracy: 0.8927\n",
      "Epoch 584/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.8915\n",
      "Epoch 00584: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2896 - accuracy: 0.8915 - val_loss: 0.3916 - val_accuracy: 0.8829\n",
      "Epoch 585/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2744 - accuracy: 0.9086\n",
      "Epoch 00585: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2725 - accuracy: 0.9096 - val_loss: 0.3336 - val_accuracy: 0.8878\n",
      "Epoch 586/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9038\n",
      "Epoch 00586: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2751 - accuracy: 0.9038 - val_loss: 0.3660 - val_accuracy: 0.8780\n",
      "Epoch 587/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9061\n",
      "Epoch 00587: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2714 - accuracy: 0.9061 - val_loss: 0.3675 - val_accuracy: 0.8878\n",
      "Epoch 588/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2615 - accuracy: 0.9061\n",
      "Epoch 00588: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2634 - accuracy: 0.9079 - val_loss: 0.4280 - val_accuracy: 0.8732\n",
      "Epoch 589/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2454 - accuracy: 0.9206\n",
      "Epoch 00589: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2548 - accuracy: 0.9166 - val_loss: 0.3859 - val_accuracy: 0.8732\n",
      "Epoch 590/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9160\n",
      "Epoch 00590: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2515 - accuracy: 0.9160 - val_loss: 0.4056 - val_accuracy: 0.8732\n",
      "Epoch 591/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8985\n",
      "Epoch 00591: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2951 - accuracy: 0.8985 - val_loss: 0.3761 - val_accuracy: 0.8829\n",
      "Epoch 592/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2761 - accuracy: 0.8941\n",
      "Epoch 00592: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2716 - accuracy: 0.8950 - val_loss: 0.4116 - val_accuracy: 0.8585\n",
      "Epoch 593/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.9015\n",
      "Epoch 00593: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2772 - accuracy: 0.9015 - val_loss: 0.4672 - val_accuracy: 0.8439\n",
      "Epoch 594/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2859 - accuracy: 0.8998\n",
      "Epoch 00594: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2790 - accuracy: 0.9015 - val_loss: 0.3938 - val_accuracy: 0.8585\n",
      "Epoch 595/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2764 - accuracy: 0.9011\n",
      "Epoch 00595: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2733 - accuracy: 0.9015 - val_loss: 0.3969 - val_accuracy: 0.8829\n",
      "Epoch 596/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9137\n",
      "Epoch 00596: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2639 - accuracy: 0.9137 - val_loss: 0.4681 - val_accuracy: 0.8732\n",
      "Epoch 597/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.3036 - accuracy: 0.8878\n",
      "Epoch 00597: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3020 - accuracy: 0.8875 - val_loss: 0.3328 - val_accuracy: 0.8927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8991\n",
      "Epoch 00598: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2697 - accuracy: 0.8991 - val_loss: 0.3690 - val_accuracy: 0.8829\n",
      "Epoch 599/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9090\n",
      "Epoch 00599: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2623 - accuracy: 0.9090 - val_loss: 0.3893 - val_accuracy: 0.8634\n",
      "Epoch 600/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9125\n",
      "Epoch 00600: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2510 - accuracy: 0.9125 - val_loss: 0.3600 - val_accuracy: 0.8732\n",
      "Epoch 601/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2694 - accuracy: 0.9036\n",
      "Epoch 00601: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2754 - accuracy: 0.9020 - val_loss: 0.3889 - val_accuracy: 0.8780\n",
      "Epoch 602/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.8991\n",
      "Epoch 00602: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2806 - accuracy: 0.8991 - val_loss: 0.4092 - val_accuracy: 0.8732\n",
      "Epoch 603/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2635 - accuracy: 0.9162\n",
      "Epoch 00603: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2589 - accuracy: 0.9172 - val_loss: 0.3454 - val_accuracy: 0.8878\n",
      "Epoch 604/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2464 - accuracy: 0.9112\n",
      "Epoch 00604: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2485 - accuracy: 0.9114 - val_loss: 0.4036 - val_accuracy: 0.8780\n",
      "Epoch 605/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9090\n",
      "Epoch 00605: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2519 - accuracy: 0.9090 - val_loss: 0.3926 - val_accuracy: 0.8829\n",
      "Epoch 606/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2516 - accuracy: 0.9130\n",
      "Epoch 00606: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2496 - accuracy: 0.9137 - val_loss: 0.3295 - val_accuracy: 0.8927\n",
      "Epoch 607/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2537 - accuracy: 0.9074\n",
      "Epoch 00607: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2481 - accuracy: 0.9096 - val_loss: 0.3702 - val_accuracy: 0.8976\n",
      "Epoch 608/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2790 - accuracy: 0.9086\n",
      "Epoch 00608: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2723 - accuracy: 0.9102 - val_loss: 0.3728 - val_accuracy: 0.8780\n",
      "Epoch 609/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2469 - accuracy: 0.9056\n",
      "Epoch 00609: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2444 - accuracy: 0.9085 - val_loss: 0.3826 - val_accuracy: 0.8732\n",
      "Epoch 610/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2431 - accuracy: 0.9168\n",
      "Epoch 00610: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2481 - accuracy: 0.9155 - val_loss: 0.3858 - val_accuracy: 0.8780\n",
      "Epoch 611/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2405 - accuracy: 0.9149\n",
      "Epoch 00611: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2405 - accuracy: 0.9155 - val_loss: 0.4057 - val_accuracy: 0.8732\n",
      "Epoch 612/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9073\n",
      "Epoch 00612: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2615 - accuracy: 0.9073 - val_loss: 0.4339 - val_accuracy: 0.8683\n",
      "Epoch 613/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9055\n",
      "Epoch 00613: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2734 - accuracy: 0.9055 - val_loss: 0.3914 - val_accuracy: 0.8780\n",
      "Epoch 614/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2482 - accuracy: 0.9168\n",
      "Epoch 00614: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2530 - accuracy: 0.9137 - val_loss: 0.3957 - val_accuracy: 0.8732\n",
      "Epoch 615/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2617 - accuracy: 0.9074\n",
      "Epoch 00615: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2631 - accuracy: 0.9067 - val_loss: 0.4061 - val_accuracy: 0.8488\n",
      "Epoch 616/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2563 - accuracy: 0.9124\n",
      "Epoch 00616: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2610 - accuracy: 0.9114 - val_loss: 0.4562 - val_accuracy: 0.8537\n",
      "Epoch 617/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2373 - accuracy: 0.9200\n",
      "Epoch 00617: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2308 - accuracy: 0.9230 - val_loss: 0.3873 - val_accuracy: 0.8780\n",
      "Epoch 618/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2770 - accuracy: 0.9017\n",
      "Epoch 00618: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2778 - accuracy: 0.9026 - val_loss: 0.3567 - val_accuracy: 0.8927\n",
      "Epoch 619/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2411 - accuracy: 0.9219 ETA: 0s - loss: 0.2448 - accura\n",
      "Epoch 00619: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2400 - accuracy: 0.9201 - val_loss: 0.4091 - val_accuracy: 0.8976\n",
      "Epoch 620/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2209 - accuracy: 0.9225\n",
      "Epoch 00620: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2269 - accuracy: 0.9201 - val_loss: 0.3995 - val_accuracy: 0.8683\n",
      "Epoch 621/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9143\n",
      "Epoch 00621: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2385 - accuracy: 0.9143 - val_loss: 0.3628 - val_accuracy: 0.8927\n",
      "Epoch 622/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2491 - accuracy: 0.9147\n",
      "Epoch 00622: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2484 - accuracy: 0.9155 - val_loss: 0.3747 - val_accuracy: 0.8829\n",
      "Epoch 623/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9201\n",
      "Epoch 00623: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2411 - accuracy: 0.9201 - val_loss: 0.4484 - val_accuracy: 0.8732\n",
      "Epoch 624/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2530 - accuracy: 0.9049\n",
      "Epoch 00624: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2542 - accuracy: 0.9050 - val_loss: 0.4186 - val_accuracy: 0.8634\n",
      "Epoch 625/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2437 - accuracy: 0.9171\n",
      "Epoch 00625: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2407 - accuracy: 0.9185 - val_loss: 0.4164 - val_accuracy: 0.8585\n",
      "Epoch 626/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9213\n",
      "Epoch 00626: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2451 - accuracy: 0.9213 - val_loss: 0.3669 - val_accuracy: 0.8732\n",
      "Epoch 627/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2424 - accuracy: 0.9099\n",
      "Epoch 00627: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2430 - accuracy: 0.9096 - val_loss: 0.4283 - val_accuracy: 0.8683\n",
      "Epoch 628/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9055\n",
      "Epoch 00628: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2725 - accuracy: 0.9055 - val_loss: 0.4956 - val_accuracy: 0.8341\n",
      "Epoch 629/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2838 - accuracy: 0.9002\n",
      "Epoch 00629: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2849 - accuracy: 0.9001 - val_loss: 0.4466 - val_accuracy: 0.8585\n",
      "Epoch 630/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2846 - accuracy: 0.9023\n",
      "Epoch 00630: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2819 - accuracy: 0.9038 - val_loss: 0.4048 - val_accuracy: 0.8780\n",
      "Epoch 631/3000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9137\n",
      "Epoch 00631: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2526 - accuracy: 0.9137 - val_loss: 0.3961 - val_accuracy: 0.8683\n",
      "Epoch 632/3000\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.2260 - accuracy: 0.9195\n",
      "Epoch 00632: val_loss did not improve from 0.32798\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2307 - accuracy: 0.9185 - val_loss: 0.4219 - val_accuracy: 0.8732\n",
      "Epoch 00632: early stopping\n",
      "CNN: Epochs=3000, Train accuracy=0.92303, Validation accuracy=0.90244\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[es, cp]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1598336814187,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "mDdwZDDFkv5W",
    "outputId": "30b0e4a2-9738-4989-eb06-1c54ecef6b41"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQCvPGZks9v"
   },
   "outputs": [],
   "source": [
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'data/cvision/model_9-8.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8927\n",
      "loss= 0.3279798924922943\n",
      "accuracy= 0.8926829099655151\n"
     ]
    }
   ],
   "source": [
    "#모델 불러와서 정확도 확인 및 예측\n",
    "hdf5_file = './data/cvision/model_9-8/model_9-8ep449-vl0.3280.hdf5'\n",
    "model.load_weights(hdf5_file)\n",
    "\n",
    "score = model.evaluate(valid_X, valid_y)\n",
    "print('loss=', score[0])        # loss\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8nb5jokyny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-dbea146f3fce>:6: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "#예측 진행\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('data/cvision/my_subm_9-5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1598337602169,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "bgfT4Y6_HFgw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 14, 14, 32)   1600        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 14, 14, 32)   128         conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 14, 14, 32)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 6, 6, 32)     0           activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 6, 6, 32)     1056        max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 6, 6, 32)     128         conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 6, 6, 32)     0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 6, 6, 32)     9248        activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 6, 6, 32)     128         conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 6, 6, 32)     0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 6, 6, 32)     1056        activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 6, 6, 32)     1056        max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 6, 6, 32)     128         conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 6, 6, 32)     128         conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 6, 6, 32)     0           batch_normalization_398[0][0]    \n",
      "                                                                 batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 6, 6, 32)     0           add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 6, 6, 32)     0           activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 3, 3, 32)     1056        dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 3, 3, 32)     128         conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 3, 3, 32)     0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 3, 3, 32)     9248        activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 3, 3, 32)     128         conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 3, 3, 32)     0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 3, 3, 32)     1056        activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 3, 3, 32)     1056        dropout_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 3, 3, 32)     128         conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 3, 3, 32)     128         conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 3, 3, 32)     0           batch_normalization_402[0][0]    \n",
      "                                                                 batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 3, 3, 32)     0           add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 3, 3, 32)     0           activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 2, 2, 32)     1056        dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 2, 2, 32)     128         conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 2, 2, 32)     0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 2, 2, 32)     9248        activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 2, 2, 32)     128         conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 2, 2, 32)     0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 2, 2, 32)     1056        activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 2, 2, 32)     1056        dropout_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 2, 2, 32)     128         conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 2, 2, 32)     128         conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 2, 2, 32)     0           batch_normalization_406[0][0]    \n",
      "                                                                 batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 2, 2, 32)     0           add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_14 (Gl (None, 32)           0           activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 50)           1650        global_average_pooling2d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 50)           0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 10)           510         dropout_96[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,672\n",
      "Trainable params: 41,840\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model_10: ResNet 응용\n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adagrad\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
    ")\n",
    "\n",
    "def conv1(x, filter_in=16):\n",
    "    x = Conv2D(filter_in, (7,7), strides=(2,2), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv2(x, layers=3, filter_in=16, filter_out=32):\n",
    "    x = MaxPooling2D((3,3), 2)(x)\n",
    "    \n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(layers):\n",
    "        if (i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "        \n",
    "    return x\n",
    "\n",
    "def conv3(x, layers=4, filter_in=32, filter_out=48):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(layers):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv4(x, layers=6, filter_in=48, filter_out=64):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(layers):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv5(x, layers=3, filter_in=64, filter_out=128):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(layers):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "#모델링\n",
    "classes = 10\n",
    "tensor_in = Input(shape= train_X.shape[1:], dtype='float32', name='input')\n",
    "\n",
    "x = conv1(tensor_in, 32)\n",
    "x = conv2(x, 1, 32, 32)\n",
    "x = Dropout(0.4)(x)\n",
    "x = conv3(x, 1, 32, 32)\n",
    "x = Dropout(0.4)(x)\n",
    "x = conv4(x, 1, 32, 32)\n",
    "#x = Dropout(0.4)(x)\n",
    "#x = conv5(x, 1)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(50, activation = 'relu', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "tensor_out = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(tensor_in, tensor_out)\n",
    "model.compile(optimizer=RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "#Adam(0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1598347122783,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "tPFielyJ5yMH"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** x)\n",
    "\n",
    "#earlystopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=75)\n",
    "\n",
    "#modelcheckpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "MODEL_SAVE_FOLDER_PATH = './data/cvision/model/'\n",
    "model_num = 'model_10'\n",
    "\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + model_num +'{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "cp = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537462,
     "status": "error",
     "timestamp": 1598347660504,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "yeNCbX2vko4s",
    "outputId": "012ce4a9-2ca8-41d3-d1de-9331032469e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.6673 - accuracy: 0.1033\n",
      "Epoch 00001: val_loss improved from inf to 2.34812, saving model to ./data/cvision/model\\model_1001-2.3481.hdf5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 2.6673 - accuracy: 0.1033 - val_loss: 2.3481 - val_accuracy: 0.1073\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.4720 - accuracy: 0.1128\n",
      "Epoch 00002: val_loss improved from 2.34812 to 2.32268, saving model to ./data/cvision/model\\model_1002-2.3227.hdf5\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.4720 - accuracy: 0.1128 - val_loss: 2.3227 - val_accuracy: 0.0634\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.4339 - accuracy: 0.1159\n",
      "Epoch 00003: val_loss improved from 2.32268 to 2.31328, saving model to ./data/cvision/model\\model_1003-2.3133.hdf5\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.4339 - accuracy: 0.1159 - val_loss: 2.3133 - val_accuracy: 0.0585\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.4331 - accuracy: 0.0989\n",
      "Epoch 00004: val_loss improved from 2.31328 to 2.31039, saving model to ./data/cvision/model\\model_1004-2.3104.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.4331 - accuracy: 0.0989 - val_loss: 2.3104 - val_accuracy: 0.0976\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3921 - accuracy: 0.1065\n",
      "Epoch 00005: val_loss improved from 2.31039 to 2.30728, saving model to ./data/cvision/model\\model_1005-2.3073.hdf5\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.3921 - accuracy: 0.1065 - val_loss: 2.3073 - val_accuracy: 0.0780\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3632 - accuracy: 0.1197\n",
      "Epoch 00006: val_loss improved from 2.30728 to 2.30362, saving model to ./data/cvision/model\\model_1006-2.3036.hdf5\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.3632 - accuracy: 0.1197 - val_loss: 2.3036 - val_accuracy: 0.1024\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3538 - accuracy: 0.1052\n",
      "Epoch 00007: val_loss improved from 2.30362 to 2.29901, saving model to ./data/cvision/model\\model_1007-2.2990.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.3538 - accuracy: 0.1052 - val_loss: 2.2990 - val_accuracy: 0.1220\n",
      "Epoch 8/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.3186 - accuracy: 0.1225\n",
      "Epoch 00008: val_loss improved from 2.29901 to 2.29632, saving model to ./data/cvision/model\\model_1008-2.2963.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.3253 - accuracy: 0.1222 - val_loss: 2.2963 - val_accuracy: 0.1171\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3239 - accuracy: 0.1250\n",
      "Epoch 00009: val_loss improved from 2.29632 to 2.29009, saving model to ./data/cvision/model\\model_1009-2.2901.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.3239 - accuracy: 0.1250 - val_loss: 2.2901 - val_accuracy: 0.1220\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3237 - accuracy: 0.1103\n",
      "Epoch 00010: val_loss improved from 2.29009 to 2.28836, saving model to ./data/cvision/model\\model_1010-2.2884.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.3237 - accuracy: 0.1103 - val_loss: 2.2884 - val_accuracy: 0.1463\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2953 - accuracy: 0.1330\n",
      "Epoch 00011: val_loss improved from 2.28836 to 2.28641, saving model to ./data/cvision/model\\model_1011-2.2864.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2953 - accuracy: 0.1330 - val_loss: 2.2864 - val_accuracy: 0.1512\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3068 - accuracy: 0.1267\n",
      "Epoch 00012: val_loss improved from 2.28641 to 2.28317, saving model to ./data/cvision/model\\model_1012-2.2832.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.3068 - accuracy: 0.1267 - val_loss: 2.2832 - val_accuracy: 0.1512\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3016 - accuracy: 0.1222\n",
      "Epoch 00013: val_loss improved from 2.28317 to 2.27836, saving model to ./data/cvision/model\\model_1013-2.2784.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.3016 - accuracy: 0.1222 - val_loss: 2.2784 - val_accuracy: 0.1415\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2972 - accuracy: 0.1128 ETA: 0s - loss: 2.2889 - accuracy: \n",
      "Epoch 00014: val_loss did not improve from 2.27836\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 2.2972 - accuracy: 0.1128 - val_loss: 2.2800 - val_accuracy: 0.1610\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2774 - accuracy: 0.1399\n",
      "Epoch 00015: val_loss did not improve from 2.27836\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 2.2774 - accuracy: 0.1399 - val_loss: 2.2787 - val_accuracy: 0.1659\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2828 - accuracy: 0.1393\n",
      "Epoch 00016: val_loss improved from 2.27836 to 2.27325, saving model to ./data/cvision/model\\model_1016-2.2733.hdf5\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2828 - accuracy: 0.1393 - val_loss: 2.2733 - val_accuracy: 0.1659\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2822 - accuracy: 0.1355\n",
      "Epoch 00017: val_loss improved from 2.27325 to 2.27190, saving model to ./data/cvision/model\\model_1017-2.2719.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2822 - accuracy: 0.1355 - val_loss: 2.2719 - val_accuracy: 0.1610\n",
      "Epoch 18/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2866 - accuracy: 0.1224\n",
      "Epoch 00018: val_loss improved from 2.27190 to 2.26591, saving model to ./data/cvision/model\\model_1018-2.2659.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2865 - accuracy: 0.1229 - val_loss: 2.2659 - val_accuracy: 0.1659\n",
      "Epoch 19/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2739 - accuracy: 0.1452\n",
      "Epoch 00019: val_loss improved from 2.26591 to 2.26083, saving model to ./data/cvision/model\\model_1019-2.2608.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2738 - accuracy: 0.1437 - val_loss: 2.2608 - val_accuracy: 0.1756\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2708 - accuracy: 0.1468\n",
      "Epoch 00020: val_loss improved from 2.26083 to 2.25303, saving model to ./data/cvision/model\\model_1020-2.2530.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 2.2708 - accuracy: 0.1468 - val_loss: 2.2530 - val_accuracy: 0.1610\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2661 - accuracy: 0.1424\n",
      "Epoch 00021: val_loss improved from 2.25303 to 2.24671, saving model to ./data/cvision/model\\model_1021-2.2467.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 2.2661 - accuracy: 0.1424 - val_loss: 2.2467 - val_accuracy: 0.1659\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2481 - accuracy: 0.1493\n",
      "Epoch 00022: val_loss improved from 2.24671 to 2.23763, saving model to ./data/cvision/model\\model_1022-2.2376.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2481 - accuracy: 0.1493 - val_loss: 2.2376 - val_accuracy: 0.2049\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2698 - accuracy: 0.1424\n",
      "Epoch 00023: val_loss improved from 2.23763 to 2.22973, saving model to ./data/cvision/model\\model_1023-2.2297.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2698 - accuracy: 0.1424 - val_loss: 2.2297 - val_accuracy: 0.1854\n",
      "Epoch 24/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2443 - accuracy: 0.1478\n",
      "Epoch 00024: val_loss improved from 2.22973 to 2.21877, saving model to ./data/cvision/model\\model_1024-2.2188.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2443 - accuracy: 0.1462 - val_loss: 2.2188 - val_accuracy: 0.1854\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2343 - accuracy: 0.1544 ETA: 0s - loss: 2.2470 - accuracy: \n",
      "Epoch 00025: val_loss improved from 2.21877 to 2.20875, saving model to ./data/cvision/model\\model_1025-2.2088.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2343 - accuracy: 0.1544 - val_loss: 2.2088 - val_accuracy: 0.1951\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2520 - accuracy: 0.1601\n",
      "Epoch 00026: val_loss improved from 2.20875 to 2.20697, saving model to ./data/cvision/model\\model_1026-2.2070.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2520 - accuracy: 0.1601 - val_loss: 2.2070 - val_accuracy: 0.2146\n",
      "Epoch 27/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2181 - accuracy: 0.1543\n",
      "Epoch 00027: val_loss improved from 2.20697 to 2.19791, saving model to ./data/cvision/model\\model_1027-2.1979.hdf5\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2210 - accuracy: 0.1496 - val_loss: 2.1979 - val_accuracy: 0.2146\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2253 - accuracy: 0.1674\n",
      "Epoch 00028: val_loss improved from 2.19791 to 2.17724, saving model to ./data/cvision/model\\model_1028-2.1772.hdf5\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2253 - accuracy: 0.1674 - val_loss: 2.1772 - val_accuracy: 0.2244\n",
      "Epoch 29/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2210 - accuracy: 0.1647\n",
      "Epoch 00029: val_loss improved from 2.17724 to 2.16782, saving model to ./data/cvision/model\\model_1029-2.1678.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2229 - accuracy: 0.1638 - val_loss: 2.1678 - val_accuracy: 0.2098\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2290 - accuracy: 0.1657\n",
      "Epoch 00030: val_loss improved from 2.16782 to 2.15271, saving model to ./data/cvision/model\\model_1030-2.1527.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2290 - accuracy: 0.1657 - val_loss: 2.1527 - val_accuracy: 0.2146\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.2190 - accuracy: 0.1708\n",
      "Epoch 00031: val_loss improved from 2.15271 to 2.14789, saving model to ./data/cvision/model\\model_1031-2.1479.hdf5\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2190 - accuracy: 0.1708 - val_loss: 2.1479 - val_accuracy: 0.2293\n",
      "Epoch 32/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.2250 - accuracy: 0.1654\n",
      "Epoch 00032: val_loss improved from 2.14789 to 2.14587, saving model to ./data/cvision/model\\model_1032-2.1459.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.2253 - accuracy: 0.1651 - val_loss: 2.1459 - val_accuracy: 0.2098\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1893 - accuracy: 0.1777\n",
      "Epoch 00033: val_loss improved from 2.14587 to 2.13463, saving model to ./data/cvision/model\\model_1033-2.1346.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1893 - accuracy: 0.1777 - val_loss: 2.1346 - val_accuracy: 0.2244\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1979 - accuracy: 0.1853\n",
      "Epoch 00034: val_loss improved from 2.13463 to 2.11651, saving model to ./data/cvision/model\\model_1034-2.1165.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1979 - accuracy: 0.1853 - val_loss: 2.1165 - val_accuracy: 0.2390\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1768 - accuracy: 0.1871\n",
      "Epoch 00035: val_loss improved from 2.11651 to 2.10929, saving model to ./data/cvision/model\\model_1035-2.1093.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1768 - accuracy: 0.1871 - val_loss: 2.1093 - val_accuracy: 0.2439\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1780 - accuracy: 0.2003\n",
      "Epoch 00036: val_loss improved from 2.10929 to 2.09767, saving model to ./data/cvision/model\\model_1036-2.0977.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1780 - accuracy: 0.2003 - val_loss: 2.0977 - val_accuracy: 0.2537\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1513 - accuracy: 0.2149\n",
      "Epoch 00037: val_loss improved from 2.09767 to 2.08915, saving model to ./data/cvision/model\\model_1037-2.0891.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1513 - accuracy: 0.2149 - val_loss: 2.0891 - val_accuracy: 0.2634\n",
      "Epoch 38/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.1635 - accuracy: 0.1966\n",
      "Epoch 00038: val_loss improved from 2.08915 to 2.07439, saving model to ./data/cvision/model\\model_1038-2.0744.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1649 - accuracy: 0.1953 - val_loss: 2.0744 - val_accuracy: 0.2732\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1361 - accuracy: 0.2098\n",
      "Epoch 00039: val_loss improved from 2.07439 to 2.05516, saving model to ./data/cvision/model\\model_1039-2.0552.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1361 - accuracy: 0.2098 - val_loss: 2.0552 - val_accuracy: 0.2829\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1525 - accuracy: 0.2060\n",
      "Epoch 00040: val_loss improved from 2.05516 to 2.03845, saving model to ./data/cvision/model\\model_1040-2.0384.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1525 - accuracy: 0.2060 - val_loss: 2.0384 - val_accuracy: 0.2878\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1307 - accuracy: 0.2218\n",
      "Epoch 00041: val_loss did not improve from 2.03845\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 2.1307 - accuracy: 0.2218 - val_loss: 2.0467 - val_accuracy: 0.2878\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1163 - accuracy: 0.2212\n",
      "Epoch 00042: val_loss improved from 2.03845 to 2.03185, saving model to ./data/cvision/model\\model_1042-2.0318.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.1163 - accuracy: 0.2212 - val_loss: 2.0318 - val_accuracy: 0.2829\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1217 - accuracy: 0.2098\n",
      "Epoch 00043: val_loss did not improve from 2.03185\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 2.1217 - accuracy: 0.2098 - val_loss: 2.0343 - val_accuracy: 0.2829\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1249 - accuracy: 0.2243\n",
      "Epoch 00044: val_loss improved from 2.03185 to 2.02771, saving model to ./data/cvision/model\\model_1044-2.0277.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1249 - accuracy: 0.2243 - val_loss: 2.0277 - val_accuracy: 0.2829\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1104 - accuracy: 0.2187\n",
      "Epoch 00045: val_loss improved from 2.02771 to 2.02451, saving model to ./data/cvision/model\\model_1045-2.0245.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 2.1104 - accuracy: 0.2187 - val_loss: 2.0245 - val_accuracy: 0.2585\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.1013 - accuracy: 0.2413\n",
      "Epoch 00046: val_loss did not improve from 2.02451\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 2.1013 - accuracy: 0.2413 - val_loss: 2.0551 - val_accuracy: 0.2488\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0998 - accuracy: 0.2357\n",
      "Epoch 00047: val_loss did not improve from 2.02451\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 2.0998 - accuracy: 0.2357 - val_loss: 2.0664 - val_accuracy: 0.2146\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0996 - accuracy: 0.2313\n",
      "Epoch 00048: val_loss did not improve from 2.02451\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 2.0996 - accuracy: 0.2313 - val_loss: 2.0913 - val_accuracy: 0.2341\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0513 - accuracy: 0.2552\n",
      "Epoch 00049: val_loss did not improve from 2.02451\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 2.0513 - accuracy: 0.2552 - val_loss: 2.0899 - val_accuracy: 0.2341\n",
      "Epoch 50/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.0447 - accuracy: 0.2552\n",
      "Epoch 00050: val_loss improved from 2.02451 to 2.01396, saving model to ./data/cvision/model\\model_1050-2.0140.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.0459 - accuracy: 0.2539 - val_loss: 2.0140 - val_accuracy: 0.2683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0582 - accuracy: 0.2552\n",
      "Epoch 00051: val_loss did not improve from 2.01396\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 2.0582 - accuracy: 0.2552 - val_loss: 2.0254 - val_accuracy: 0.2683\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0705 - accuracy: 0.2432\n",
      "Epoch 00052: val_loss improved from 2.01396 to 2.01193, saving model to ./data/cvision/model\\model_1052-2.0119.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.0705 - accuracy: 0.2432 - val_loss: 2.0119 - val_accuracy: 0.2780\n",
      "Epoch 53/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.0464 - accuracy: 0.2559\n",
      "Epoch 00053: val_loss improved from 2.01193 to 2.00144, saving model to ./data/cvision/model\\model_1053-2.0014.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.0479 - accuracy: 0.2520 - val_loss: 2.0014 - val_accuracy: 0.2780\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0169 - accuracy: 0.2773\n",
      "Epoch 00054: val_loss improved from 2.00144 to 1.99997, saving model to ./data/cvision/model\\model_1054-2.0000.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 2.0169 - accuracy: 0.2773 - val_loss: 2.0000 - val_accuracy: 0.2683\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0186 - accuracy: 0.2710\n",
      "Epoch 00055: val_loss improved from 1.99997 to 1.96491, saving model to ./data/cvision/model\\model_1055-1.9649.hdf5\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.0186 - accuracy: 0.2710 - val_loss: 1.9649 - val_accuracy: 0.2683\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0480 - accuracy: 0.2615\n",
      "Epoch 00056: val_loss did not improve from 1.96491\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 2.0480 - accuracy: 0.2615 - val_loss: 1.9871 - val_accuracy: 0.2439\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0344 - accuracy: 0.2710\n",
      "Epoch 00057: val_loss improved from 1.96491 to 1.94371, saving model to ./data/cvision/model\\model_1057-1.9437.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.0344 - accuracy: 0.2710 - val_loss: 1.9437 - val_accuracy: 0.2927\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.0035 - accuracy: 0.2640\n",
      "Epoch 00058: val_loss did not improve from 1.94371\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 2.0035 - accuracy: 0.2640 - val_loss: 1.9718 - val_accuracy: 0.2683\n",
      "Epoch 59/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.0001 - accuracy: 0.2780\n",
      "Epoch 00059: val_loss did not improve from 1.94371\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.9968 - accuracy: 0.2798 - val_loss: 1.9893 - val_accuracy: 0.2878\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9943 - accuracy: 0.2757\n",
      "Epoch 00060: val_loss improved from 1.94371 to 1.94125, saving model to ./data/cvision/model\\model_1060-1.9412.hdf5\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.9943 - accuracy: 0.2757 - val_loss: 1.9412 - val_accuracy: 0.2976\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9867 - accuracy: 0.2735\n",
      "Epoch 00061: val_loss improved from 1.94125 to 1.94089, saving model to ./data/cvision/model\\model_1061-1.9409.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.9867 - accuracy: 0.2735 - val_loss: 1.9409 - val_accuracy: 0.2780\n",
      "Epoch 62/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.9606 - accuracy: 0.2904\n",
      "Epoch 00062: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.9649 - accuracy: 0.2899 - val_loss: 2.0230 - val_accuracy: 0.2780\n",
      "Epoch 63/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.9812 - accuracy: 0.2754\n",
      "Epoch 00063: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.9764 - accuracy: 0.2810 - val_loss: 2.0153 - val_accuracy: 0.2683\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9830 - accuracy: 0.2703\n",
      "Epoch 00064: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.9830 - accuracy: 0.2703 - val_loss: 1.9987 - val_accuracy: 0.2585\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9700 - accuracy: 0.2798\n",
      "Epoch 00065: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.9700 - accuracy: 0.2798 - val_loss: 2.0061 - val_accuracy: 0.3073\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9677 - accuracy: 0.2823\n",
      "Epoch 00066: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.9677 - accuracy: 0.2823 - val_loss: 1.9788 - val_accuracy: 0.3024\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9402 - accuracy: 0.3163\n",
      "Epoch 00067: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.9402 - accuracy: 0.3163 - val_loss: 1.9566 - val_accuracy: 0.3073\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9535 - accuracy: 0.2891\n",
      "Epoch 00068: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.9535 - accuracy: 0.2891 - val_loss: 2.0109 - val_accuracy: 0.3024\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9280 - accuracy: 0.3062\n",
      "Epoch 00069: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.9280 - accuracy: 0.3062 - val_loss: 2.0492 - val_accuracy: 0.2878\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9296 - accuracy: 0.3031\n",
      "Epoch 00070: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.9296 - accuracy: 0.3031 - val_loss: 1.9933 - val_accuracy: 0.2927\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9412 - accuracy: 0.3043\n",
      "Epoch 00071: val_loss did not improve from 1.94089\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.9412 - accuracy: 0.3043 - val_loss: 1.9915 - val_accuracy: 0.2829\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9165 - accuracy: 0.3106\n",
      "Epoch 00072: val_loss improved from 1.94089 to 1.88103, saving model to ./data/cvision/model\\model_1072-1.8810.hdf5\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.9165 - accuracy: 0.3106 - val_loss: 1.8810 - val_accuracy: 0.3220\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9162 - accuracy: 0.3050\n",
      "Epoch 00073: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.9162 - accuracy: 0.3050 - val_loss: 1.9560 - val_accuracy: 0.2878\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8955 - accuracy: 0.3114\n",
      "Epoch 00074: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8955 - accuracy: 0.3114 - val_loss: 1.9519 - val_accuracy: 0.2780\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8961 - accuracy: 0.3245\n",
      "Epoch 00075: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.8961 - accuracy: 0.3245 - val_loss: 1.9446 - val_accuracy: 0.2878\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8774 - accuracy: 0.3106\n",
      "Epoch 00076: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8774 - accuracy: 0.3106 - val_loss: 1.9903 - val_accuracy: 0.2878\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.9018 - accuracy: 0.3157\n",
      "Epoch 00077: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.9018 - accuracy: 0.3157 - val_loss: 2.0068 - val_accuracy: 0.3122\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8987 - accuracy: 0.3258\n",
      "Epoch 00078: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.8987 - accuracy: 0.3258 - val_loss: 2.0245 - val_accuracy: 0.2878\n",
      "Epoch 79/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 1.8334 - accuracy: 0.3346\n",
      "Epoch 00079: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8334 - accuracy: 0.3346 - val_loss: 2.0100 - val_accuracy: 0.3122\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8634 - accuracy: 0.3346\n",
      "Epoch 00080: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8634 - accuracy: 0.3346 - val_loss: 1.9659 - val_accuracy: 0.3268\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8560 - accuracy: 0.3308\n",
      "Epoch 00081: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8560 - accuracy: 0.3308 - val_loss: 1.9315 - val_accuracy: 0.3317\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8781 - accuracy: 0.3283\n",
      "Epoch 00082: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8781 - accuracy: 0.3283 - val_loss: 1.9555 - val_accuracy: 0.2878\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8454 - accuracy: 0.3472\n",
      "Epoch 00083: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.8454 - accuracy: 0.3472 - val_loss: 1.8997 - val_accuracy: 0.3220\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8536 - accuracy: 0.3327 ETA: 0s - loss: 1.8401 - accuracy: \n",
      "Epoch 00084: val_loss did not improve from 1.88103\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8536 - accuracy: 0.3327 - val_loss: 1.9405 - val_accuracy: 0.3024\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8258 - accuracy: 0.3522\n",
      "Epoch 00085: val_loss improved from 1.88103 to 1.85972, saving model to ./data/cvision/model\\model_1085-1.8597.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.8258 - accuracy: 0.3522 - val_loss: 1.8597 - val_accuracy: 0.3463\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8552 - accuracy: 0.3422\n",
      "Epoch 00086: val_loss did not improve from 1.85972\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.8552 - accuracy: 0.3422 - val_loss: 1.9384 - val_accuracy: 0.3220\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8678 - accuracy: 0.3270\n",
      "Epoch 00087: val_loss did not improve from 1.85972\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8678 - accuracy: 0.3270 - val_loss: 1.9593 - val_accuracy: 0.3171\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8180 - accuracy: 0.3503\n",
      "Epoch 00088: val_loss did not improve from 1.85972\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.8180 - accuracy: 0.3503 - val_loss: 1.9864 - val_accuracy: 0.3171\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8283 - accuracy: 0.3359\n",
      "Epoch 00089: val_loss did not improve from 1.85972\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.8283 - accuracy: 0.3359 - val_loss: 1.8944 - val_accuracy: 0.3122\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8259 - accuracy: 0.3428\n",
      "Epoch 00090: val_loss improved from 1.85972 to 1.79899, saving model to ./data/cvision/model\\model_1090-1.7990.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.8259 - accuracy: 0.3428 - val_loss: 1.7990 - val_accuracy: 0.3659\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8092 - accuracy: 0.3516\n",
      "Epoch 00091: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.8092 - accuracy: 0.3516 - val_loss: 1.8566 - val_accuracy: 0.3610\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8216 - accuracy: 0.3403\n",
      "Epoch 00092: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8216 - accuracy: 0.3403 - val_loss: 1.9231 - val_accuracy: 0.3220\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7718 - accuracy: 0.3693\n",
      "Epoch 00093: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.7718 - accuracy: 0.3693 - val_loss: 1.8392 - val_accuracy: 0.3707\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7845 - accuracy: 0.3674\n",
      "Epoch 00094: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7845 - accuracy: 0.3674 - val_loss: 1.8787 - val_accuracy: 0.3463\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.8229 - accuracy: 0.3398\n",
      "Epoch 00095: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.8229 - accuracy: 0.3398 - val_loss: 1.8922 - val_accuracy: 0.3463\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7851 - accuracy: 0.3529\n",
      "Epoch 00096: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7851 - accuracy: 0.3529 - val_loss: 1.8685 - val_accuracy: 0.3610\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7687 - accuracy: 0.3661\n",
      "Epoch 00097: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.7687 - accuracy: 0.3661 - val_loss: 1.8178 - val_accuracy: 0.3659\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7681 - accuracy: 0.3667\n",
      "Epoch 00098: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7681 - accuracy: 0.3667 - val_loss: 1.8556 - val_accuracy: 0.3756\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7777 - accuracy: 0.3693\n",
      "Epoch 00099: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7777 - accuracy: 0.3693 - val_loss: 1.8208 - val_accuracy: 0.3610\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7849 - accuracy: 0.3648\n",
      "Epoch 00100: val_loss did not improve from 1.79899\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7849 - accuracy: 0.3648 - val_loss: 1.8842 - val_accuracy: 0.3415\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7498 - accuracy: 0.3890\n",
      "Epoch 00101: val_loss improved from 1.79899 to 1.74566, saving model to ./data/cvision/model\\model_10101-1.7457.hdf5\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.7498 - accuracy: 0.3890 - val_loss: 1.7457 - val_accuracy: 0.3707\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7625 - accuracy: 0.3819\n",
      "Epoch 00102: val_loss did not improve from 1.74566\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.7625 - accuracy: 0.3819 - val_loss: 1.8742 - val_accuracy: 0.3366\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7405 - accuracy: 0.3774\n",
      "Epoch 00103: val_loss improved from 1.74566 to 1.68584, saving model to ./data/cvision/model\\model_10103-1.6858.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.7405 - accuracy: 0.3774 - val_loss: 1.6858 - val_accuracy: 0.4390\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7693 - accuracy: 0.3800\n",
      "Epoch 00104: val_loss did not improve from 1.68584\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7693 - accuracy: 0.3800 - val_loss: 1.7913 - val_accuracy: 0.4000\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7582 - accuracy: 0.3667\n",
      "Epoch 00105: val_loss did not improve from 1.68584\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7582 - accuracy: 0.3667 - val_loss: 1.8245 - val_accuracy: 0.3854\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7316 - accuracy: 0.3705\n",
      "Epoch 00106: val_loss did not improve from 1.68584\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.7316 - accuracy: 0.3705 - val_loss: 1.7274 - val_accuracy: 0.4244\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6799 - accuracy: 0.4018\n",
      "Epoch 00107: val_loss improved from 1.68584 to 1.64022, saving model to ./data/cvision/model\\model_10107-1.6402.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.6799 - accuracy: 0.4018 - val_loss: 1.6402 - val_accuracy: 0.4439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.7138 - accuracy: 0.4030\n",
      "Epoch 00108: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7188 - accuracy: 0.3995 - val_loss: 1.7770 - val_accuracy: 0.3951\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7514 - accuracy: 0.3774\n",
      "Epoch 00109: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7514 - accuracy: 0.3774 - val_loss: 1.9248 - val_accuracy: 0.3415\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7302 - accuracy: 0.3823\n",
      "Epoch 00110: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.7302 - accuracy: 0.3823 - val_loss: 1.7180 - val_accuracy: 0.4000\n",
      "Epoch 111/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.7119 - accuracy: 0.3991\n",
      "Epoch 00111: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7087 - accuracy: 0.3976 - val_loss: 1.8291 - val_accuracy: 0.4049\n",
      "Epoch 112/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.7297 - accuracy: 0.3783\n",
      "Epoch 00112: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7255 - accuracy: 0.3800 - val_loss: 1.9095 - val_accuracy: 0.3854\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7686 - accuracy: 0.3787\n",
      "Epoch 00113: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.7686 - accuracy: 0.3787 - val_loss: 1.8193 - val_accuracy: 0.4000\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6952 - accuracy: 0.3973\n",
      "Epoch 00114: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.6952 - accuracy: 0.3973 - val_loss: 1.7318 - val_accuracy: 0.4098\n",
      "Epoch 115/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.6945 - accuracy: 0.3929\n",
      "Epoch 00115: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6915 - accuracy: 0.3982 - val_loss: 1.8189 - val_accuracy: 0.3951\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6798 - accuracy: 0.3976\n",
      "Epoch 00116: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6798 - accuracy: 0.3976 - val_loss: 1.8124 - val_accuracy: 0.3902\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6953 - accuracy: 0.4039\n",
      "Epoch 00117: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.6953 - accuracy: 0.4039 - val_loss: 1.7828 - val_accuracy: 0.3902\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6611 - accuracy: 0.4052\n",
      "Epoch 00118: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.6611 - accuracy: 0.4052 - val_loss: 1.7303 - val_accuracy: 0.4049\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.7113 - accuracy: 0.3913\n",
      "Epoch 00119: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.7113 - accuracy: 0.3913 - val_loss: 1.7034 - val_accuracy: 0.4244\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6579 - accuracy: 0.4140\n",
      "Epoch 00120: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6579 - accuracy: 0.4140 - val_loss: 1.7521 - val_accuracy: 0.4244\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6732 - accuracy: 0.4039\n",
      "Epoch 00121: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6732 - accuracy: 0.4039 - val_loss: 1.7345 - val_accuracy: 0.3951\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6658 - accuracy: 0.3963\n",
      "Epoch 00122: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6658 - accuracy: 0.3963 - val_loss: 1.7044 - val_accuracy: 0.4244\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6731 - accuracy: 0.4140\n",
      "Epoch 00123: val_loss did not improve from 1.64022\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.6731 - accuracy: 0.4140 - val_loss: 1.6896 - val_accuracy: 0.4390\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6717 - accuracy: 0.4083\n",
      "Epoch 00124: val_loss improved from 1.64022 to 1.59709, saving model to ./data/cvision/model\\model_10124-1.5971.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.6717 - accuracy: 0.4083 - val_loss: 1.5971 - val_accuracy: 0.4537\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6634 - accuracy: 0.4033\n",
      "Epoch 00125: val_loss did not improve from 1.59709\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.6634 - accuracy: 0.4033 - val_loss: 1.6206 - val_accuracy: 0.4488\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6999 - accuracy: 0.3951\n",
      "Epoch 00126: val_loss improved from 1.59709 to 1.54917, saving model to ./data/cvision/model\\model_10126-1.5492.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.6999 - accuracy: 0.3951 - val_loss: 1.5492 - val_accuracy: 0.4439\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6602 - accuracy: 0.4234\n",
      "Epoch 00127: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6602 - accuracy: 0.4234 - val_loss: 1.6888 - val_accuracy: 0.4488\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6642 - accuracy: 0.4115\n",
      "Epoch 00128: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6642 - accuracy: 0.4115 - val_loss: 1.7607 - val_accuracy: 0.4098\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6366 - accuracy: 0.4297\n",
      "Epoch 00129: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6366 - accuracy: 0.4297 - val_loss: 1.6193 - val_accuracy: 0.4732\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6534 - accuracy: 0.4228\n",
      "Epoch 00130: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6534 - accuracy: 0.4228 - val_loss: 1.6096 - val_accuracy: 0.4732\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6054 - accuracy: 0.4423\n",
      "Epoch 00131: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6054 - accuracy: 0.4423 - val_loss: 1.5524 - val_accuracy: 0.4585\n",
      "Epoch 132/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.6439 - accuracy: 0.4232\n",
      "Epoch 00132: val_loss did not improve from 1.54917\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6403 - accuracy: 0.4260 - val_loss: 1.6674 - val_accuracy: 0.4390\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6399 - accuracy: 0.4285\n",
      "Epoch 00133: val_loss improved from 1.54917 to 1.51607, saving model to ./data/cvision/model\\model_10133-1.5161.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.6399 - accuracy: 0.4285 - val_loss: 1.5161 - val_accuracy: 0.4683\n",
      "Epoch 134/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.6526 - accuracy: 0.4173\n",
      "Epoch 00134: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6457 - accuracy: 0.4216 - val_loss: 1.5775 - val_accuracy: 0.4732\n",
      "Epoch 135/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.6650 - accuracy: 0.3939 ETA: 0s - loss: 1.6554 - accuracy\n",
      "Epoch 00135: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6669 - accuracy: 0.3932 - val_loss: 1.7403 - val_accuracy: 0.4146\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6641 - accuracy: 0.4266\n",
      "Epoch 00136: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6641 - accuracy: 0.4266 - val_loss: 1.6276 - val_accuracy: 0.4341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6273 - accuracy: 0.4329\n",
      "Epoch 00137: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6273 - accuracy: 0.4329 - val_loss: 1.5272 - val_accuracy: 0.4634\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6456 - accuracy: 0.4171\n",
      "Epoch 00138: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6456 - accuracy: 0.4171 - val_loss: 1.7079 - val_accuracy: 0.4195\n",
      "Epoch 139/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5956 - accuracy: 0.4336\n",
      "Epoch 00139: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5978 - accuracy: 0.4329 - val_loss: 1.7009 - val_accuracy: 0.4244\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6191 - accuracy: 0.4304\n",
      "Epoch 00140: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6191 - accuracy: 0.4304 - val_loss: 1.6071 - val_accuracy: 0.4537\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6094 - accuracy: 0.4285\n",
      "Epoch 00141: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.6094 - accuracy: 0.4285 - val_loss: 1.6177 - val_accuracy: 0.4439\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5979 - accuracy: 0.4224\n",
      "Epoch 00142: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.5979 - accuracy: 0.4224 - val_loss: 1.5458 - val_accuracy: 0.4634\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6282 - accuracy: 0.4197\n",
      "Epoch 00143: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6282 - accuracy: 0.4197 - val_loss: 1.5512 - val_accuracy: 0.4585\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6167 - accuracy: 0.4291\n",
      "Epoch 00144: val_loss did not improve from 1.51607\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6167 - accuracy: 0.4291 - val_loss: 1.6091 - val_accuracy: 0.4634\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6219 - accuracy: 0.4241 ETA: 0s - loss: 1.6308 - accuracy\n",
      "Epoch 00145: val_loss improved from 1.51607 to 1.51584, saving model to ./data/cvision/model\\model_10145-1.5158.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.6219 - accuracy: 0.4241 - val_loss: 1.5158 - val_accuracy: 0.5122\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6170 - accuracy: 0.4216\n",
      "Epoch 00146: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6170 - accuracy: 0.4216 - val_loss: 1.5481 - val_accuracy: 0.5024\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6382 - accuracy: 0.4310\n",
      "Epoch 00147: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.6382 - accuracy: 0.4310 - val_loss: 1.5492 - val_accuracy: 0.4976\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5998 - accuracy: 0.4481\n",
      "Epoch 00148: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.5998 - accuracy: 0.4481 - val_loss: 1.5291 - val_accuracy: 0.5024\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6090 - accuracy: 0.4373\n",
      "Epoch 00149: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.6090 - accuracy: 0.4373 - val_loss: 1.5883 - val_accuracy: 0.4878\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5953 - accuracy: 0.4493\n",
      "Epoch 00150: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5953 - accuracy: 0.4493 - val_loss: 1.5552 - val_accuracy: 0.5122\n",
      "Epoch 151/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5876 - accuracy: 0.4531\n",
      "Epoch 00151: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5972 - accuracy: 0.4505 - val_loss: 1.5764 - val_accuracy: 0.5024\n",
      "Epoch 152/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5708 - accuracy: 0.4463\n",
      "Epoch 00152: val_loss did not improve from 1.51584\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5710 - accuracy: 0.4449 - val_loss: 1.5324 - val_accuracy: 0.5024\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5906 - accuracy: 0.4335\n",
      "Epoch 00153: val_loss improved from 1.51584 to 1.45082, saving model to ./data/cvision/model\\model_10153-1.4508.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.5906 - accuracy: 0.4335 - val_loss: 1.4508 - val_accuracy: 0.5024\n",
      "Epoch 154/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5798 - accuracy: 0.4486\n",
      "Epoch 00154: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5869 - accuracy: 0.4449 - val_loss: 1.5856 - val_accuracy: 0.4585\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5883 - accuracy: 0.4512\n",
      "Epoch 00155: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.5883 - accuracy: 0.4512 - val_loss: 1.6424 - val_accuracy: 0.4293\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.6006 - accuracy: 0.4430\n",
      "Epoch 00156: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.6006 - accuracy: 0.4430 - val_loss: 1.5030 - val_accuracy: 0.5073\n",
      "Epoch 157/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5740 - accuracy: 0.4518\n",
      "Epoch 00157: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5889 - accuracy: 0.4486 - val_loss: 1.4946 - val_accuracy: 0.4927\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5906 - accuracy: 0.4657\n",
      "Epoch 00158: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5906 - accuracy: 0.4657 - val_loss: 1.5647 - val_accuracy: 0.4780\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5522 - accuracy: 0.4474\n",
      "Epoch 00159: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5522 - accuracy: 0.4474 - val_loss: 1.6020 - val_accuracy: 0.4780\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5738 - accuracy: 0.4373\n",
      "Epoch 00160: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5738 - accuracy: 0.4373 - val_loss: 1.5422 - val_accuracy: 0.4732\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5940 - accuracy: 0.4493\n",
      "Epoch 00161: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5940 - accuracy: 0.4493 - val_loss: 1.4807 - val_accuracy: 0.5220\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5469 - accuracy: 0.4682\n",
      "Epoch 00162: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5469 - accuracy: 0.4682 - val_loss: 1.5337 - val_accuracy: 0.5073\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5718 - accuracy: 0.4562\n",
      "Epoch 00163: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5718 - accuracy: 0.4562 - val_loss: 1.5789 - val_accuracy: 0.4878\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5607 - accuracy: 0.4587\n",
      "Epoch 00164: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5607 - accuracy: 0.4587 - val_loss: 1.4541 - val_accuracy: 0.5268\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5460 - accuracy: 0.4348\n",
      "Epoch 00165: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5460 - accuracy: 0.4348 - val_loss: 1.4847 - val_accuracy: 0.5073\n",
      "Epoch 166/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5705 - accuracy: 0.4616\n",
      "Epoch 00166: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5712 - accuracy: 0.4600 - val_loss: 1.7745 - val_accuracy: 0.4244\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5729 - accuracy: 0.4373\n",
      "Epoch 00167: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5729 - accuracy: 0.4373 - val_loss: 1.5518 - val_accuracy: 0.4829\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5637 - accuracy: 0.4260\n",
      "Epoch 00168: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5637 - accuracy: 0.4260 - val_loss: 1.4634 - val_accuracy: 0.5317\n",
      "Epoch 169/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5641 - accuracy: 0.4648\n",
      "Epoch 00169: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5667 - accuracy: 0.4638 - val_loss: 1.4726 - val_accuracy: 0.5073\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5719 - accuracy: 0.4594\n",
      "Epoch 00170: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5719 - accuracy: 0.4594 - val_loss: 1.4605 - val_accuracy: 0.5122\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5551 - accuracy: 0.4505 ETA: 0s - loss: 1.5180 - accuracy\n",
      "Epoch 00171: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5551 - accuracy: 0.4505 - val_loss: 1.4701 - val_accuracy: 0.5122\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5373 - accuracy: 0.4694\n",
      "Epoch 00172: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5373 - accuracy: 0.4694 - val_loss: 1.6037 - val_accuracy: 0.5073\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5475 - accuracy: 0.4638\n",
      "Epoch 00173: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5475 - accuracy: 0.4638 - val_loss: 1.5774 - val_accuracy: 0.4634\n",
      "Epoch 174/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.5495 - accuracy: 0.4701\n",
      "Epoch 00174: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5482 - accuracy: 0.4701 - val_loss: 1.9447 - val_accuracy: 0.3415\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5430 - accuracy: 0.4682\n",
      "Epoch 00175: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5430 - accuracy: 0.4682 - val_loss: 1.8551 - val_accuracy: 0.4098\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5123 - accuracy: 0.4739\n",
      "Epoch 00176: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5123 - accuracy: 0.4739 - val_loss: 1.5270 - val_accuracy: 0.4683\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5712 - accuracy: 0.4543\n",
      "Epoch 00177: val_loss did not improve from 1.45082\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5712 - accuracy: 0.4543 - val_loss: 1.6278 - val_accuracy: 0.5073\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5446 - accuracy: 0.4448\n",
      "Epoch 00178: val_loss improved from 1.45082 to 1.43512, saving model to ./data/cvision/model\\model_10178-1.4351.hdf5\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.5446 - accuracy: 0.4448 - val_loss: 1.4351 - val_accuracy: 0.5317\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5261 - accuracy: 0.4625\n",
      "Epoch 00179: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5261 - accuracy: 0.4625 - val_loss: 1.4916 - val_accuracy: 0.5512\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5137 - accuracy: 0.4657\n",
      "Epoch 00180: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5137 - accuracy: 0.4657 - val_loss: 1.6591 - val_accuracy: 0.4732\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5105 - accuracy: 0.4663\n",
      "Epoch 00181: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.5105 - accuracy: 0.4663 - val_loss: 1.7641 - val_accuracy: 0.4439\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.4638\n",
      "Epoch 00182: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.5335 - accuracy: 0.4638 - val_loss: 1.5068 - val_accuracy: 0.5317\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5175 - accuracy: 0.4783\n",
      "Epoch 00183: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5175 - accuracy: 0.4783 - val_loss: 1.6360 - val_accuracy: 0.4683\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5011 - accuracy: 0.4745\n",
      "Epoch 00184: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5011 - accuracy: 0.4745 - val_loss: 1.4892 - val_accuracy: 0.5171\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5229 - accuracy: 0.4671\n",
      "Epoch 00185: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.5229 - accuracy: 0.4671 - val_loss: 1.5807 - val_accuracy: 0.4829\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5033 - accuracy: 0.4694\n",
      "Epoch 00186: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5033 - accuracy: 0.4694 - val_loss: 1.4747 - val_accuracy: 0.5220\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5078 - accuracy: 0.4581\n",
      "Epoch 00187: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5078 - accuracy: 0.4581 - val_loss: 1.6616 - val_accuracy: 0.4244\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5061 - accuracy: 0.4663\n",
      "Epoch 00188: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.5061 - accuracy: 0.4663 - val_loss: 1.5544 - val_accuracy: 0.4732\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4906 - accuracy: 0.4721\n",
      "Epoch 00189: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4906 - accuracy: 0.4721 - val_loss: 1.4760 - val_accuracy: 0.5415\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4828 - accuracy: 0.4844\n",
      "Epoch 00190: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.4828 - accuracy: 0.4844 - val_loss: 1.6395 - val_accuracy: 0.4634\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5147 - accuracy: 0.4704\n",
      "Epoch 00191: val_loss did not improve from 1.43512\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.5147 - accuracy: 0.4704 - val_loss: 1.6504 - val_accuracy: 0.4780\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5201 - accuracy: 0.4594\n",
      "Epoch 00192: val_loss improved from 1.43512 to 1.42427, saving model to ./data/cvision/model\\model_10192-1.4243.hdf5\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 1.5201 - accuracy: 0.4594 - val_loss: 1.4243 - val_accuracy: 0.5317\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5099 - accuracy: 0.4814\n",
      "Epoch 00193: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5099 - accuracy: 0.4814 - val_loss: 1.4872 - val_accuracy: 0.5220\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5061 - accuracy: 0.4694\n",
      "Epoch 00194: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.5061 - accuracy: 0.4694 - val_loss: 1.6931 - val_accuracy: 0.4732\n",
      "Epoch 195/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 1.5051 - accuracy: 0.4650\n",
      "Epoch 00195: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5051 - accuracy: 0.4650 - val_loss: 1.5233 - val_accuracy: 0.5171\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4940 - accuracy: 0.4701\n",
      "Epoch 00196: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4940 - accuracy: 0.4701 - val_loss: 1.5789 - val_accuracy: 0.4976\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4855 - accuracy: 0.4877\n",
      "Epoch 00197: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.4855 - accuracy: 0.4877 - val_loss: 1.5598 - val_accuracy: 0.4780\n",
      "Epoch 198/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4817 - accuracy: 0.4922\n",
      "Epoch 00198: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4812 - accuracy: 0.4915 - val_loss: 1.6989 - val_accuracy: 0.4488\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4950 - accuracy: 0.4852\n",
      "Epoch 00199: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4950 - accuracy: 0.4852 - val_loss: 1.4920 - val_accuracy: 0.5171\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5344 - accuracy: 0.4619\n",
      "Epoch 00200: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.5344 - accuracy: 0.4619 - val_loss: 1.5161 - val_accuracy: 0.5073\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4692 - accuracy: 0.4965\n",
      "Epoch 00201: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.4692 - accuracy: 0.4965 - val_loss: 1.4404 - val_accuracy: 0.5268\n",
      "Epoch 202/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4868 - accuracy: 0.4844\n",
      "Epoch 00202: val_loss did not improve from 1.42427\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4817 - accuracy: 0.4852 - val_loss: 1.4829 - val_accuracy: 0.5073\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4603 - accuracy: 0.4997\n",
      "Epoch 00203: val_loss improved from 1.42427 to 1.41369, saving model to ./data/cvision/model\\model_10203-1.4137.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.4603 - accuracy: 0.4997 - val_loss: 1.4137 - val_accuracy: 0.5171\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4657 - accuracy: 0.4833\n",
      "Epoch 00204: val_loss did not improve from 1.41369\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4657 - accuracy: 0.4833 - val_loss: 1.4303 - val_accuracy: 0.5317\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4763 - accuracy: 0.4865\n",
      "Epoch 00205: val_loss improved from 1.41369 to 1.41188, saving model to ./data/cvision/model\\model_10205-1.4119.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.4763 - accuracy: 0.4865 - val_loss: 1.4119 - val_accuracy: 0.4829\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4549 - accuracy: 0.5028\n",
      "Epoch 00206: val_loss did not improve from 1.41188\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4549 - accuracy: 0.5028 - val_loss: 1.4293 - val_accuracy: 0.5220\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4780 - accuracy: 0.4839\n",
      "Epoch 00207: val_loss improved from 1.41188 to 1.40788, saving model to ./data/cvision/model\\model_10207-1.4079.hdf5\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.4780 - accuracy: 0.4839 - val_loss: 1.4079 - val_accuracy: 0.5366\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4621 - accuracy: 0.4846\n",
      "Epoch 00208: val_loss improved from 1.40788 to 1.34715, saving model to ./data/cvision/model\\model_10208-1.3472.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.4621 - accuracy: 0.4846 - val_loss: 1.3472 - val_accuracy: 0.5317\n",
      "Epoch 209/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4456 - accuracy: 0.4785\n",
      "Epoch 00209: val_loss did not improve from 1.34715\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4446 - accuracy: 0.4783 - val_loss: 1.5581 - val_accuracy: 0.4878\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4826 - accuracy: 0.4846\n",
      "Epoch 00210: val_loss did not improve from 1.34715\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4826 - accuracy: 0.4846 - val_loss: 1.3708 - val_accuracy: 0.5366\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4622 - accuracy: 0.4909\n",
      "Epoch 00211: val_loss improved from 1.34715 to 1.32167, saving model to ./data/cvision/model\\model_10211-1.3217.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.4622 - accuracy: 0.4909 - val_loss: 1.3217 - val_accuracy: 0.5415\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4618 - accuracy: 0.4991\n",
      "Epoch 00212: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4618 - accuracy: 0.4991 - val_loss: 1.3858 - val_accuracy: 0.5463\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4244 - accuracy: 0.4865\n",
      "Epoch 00213: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4244 - accuracy: 0.4865 - val_loss: 1.3408 - val_accuracy: 0.5317\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4767 - accuracy: 0.4783\n",
      "Epoch 00214: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4767 - accuracy: 0.4783 - val_loss: 1.3835 - val_accuracy: 0.5415\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4367 - accuracy: 0.4972\n",
      "Epoch 00215: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4367 - accuracy: 0.4972 - val_loss: 1.3227 - val_accuracy: 0.5463\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4557 - accuracy: 0.4984\n",
      "Epoch 00216: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4557 - accuracy: 0.4984 - val_loss: 1.3477 - val_accuracy: 0.5561\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4514 - accuracy: 0.4833\n",
      "Epoch 00217: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4514 - accuracy: 0.4833 - val_loss: 1.3616 - val_accuracy: 0.5561\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4326 - accuracy: 0.4871\n",
      "Epoch 00218: val_loss did not improve from 1.32167\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4326 - accuracy: 0.4871 - val_loss: 1.3956 - val_accuracy: 0.5463\n",
      "Epoch 219/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4683 - accuracy: 0.5033\n",
      "Epoch 00219: val_loss improved from 1.32167 to 1.30855, saving model to ./data/cvision/model\\model_10219-1.3085.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.4695 - accuracy: 0.5035 - val_loss: 1.3085 - val_accuracy: 0.5561\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4744 - accuracy: 0.4802\n",
      "Epoch 00220: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4744 - accuracy: 0.4802 - val_loss: 1.4410 - val_accuracy: 0.5317\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4465 - accuracy: 0.4877\n",
      "Epoch 00221: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4465 - accuracy: 0.4877 - val_loss: 1.3231 - val_accuracy: 0.5561\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4278 - accuracy: 0.5028\n",
      "Epoch 00222: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.4278 - accuracy: 0.5028 - val_loss: 1.3532 - val_accuracy: 0.5512\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4427 - accuracy: 0.5041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00223: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4427 - accuracy: 0.5041 - val_loss: 1.5295 - val_accuracy: 0.4927\n",
      "Epoch 224/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4277 - accuracy: 0.5046\n",
      "Epoch 00224: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4216 - accuracy: 0.5079 - val_loss: 1.5911 - val_accuracy: 0.4780\n",
      "Epoch 225/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4960 - accuracy: 0.4756\n",
      "Epoch 00225: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.4685 - accuracy: 0.4871 - val_loss: 1.4606 - val_accuracy: 0.4829\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4227 - accuracy: 0.5167 ETA: 0s - loss: 1.3935 - accuracy: \n",
      "Epoch 00226: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4227 - accuracy: 0.5167 - val_loss: 1.5007 - val_accuracy: 0.5122\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4494 - accuracy: 0.5003\n",
      "Epoch 00227: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4494 - accuracy: 0.5003 - val_loss: 1.5129 - val_accuracy: 0.4732\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4326 - accuracy: 0.5060\n",
      "Epoch 00228: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4326 - accuracy: 0.5060 - val_loss: 1.4775 - val_accuracy: 0.4976\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4554 - accuracy: 0.5060\n",
      "Epoch 00229: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4554 - accuracy: 0.5060 - val_loss: 1.6309 - val_accuracy: 0.4634\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4387 - accuracy: 0.4997\n",
      "Epoch 00230: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4387 - accuracy: 0.4997 - val_loss: 1.7720 - val_accuracy: 0.4488\n",
      "Epoch 231/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4460 - accuracy: 0.4993\n",
      "Epoch 00231: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.4378 - accuracy: 0.5022 - val_loss: 1.3596 - val_accuracy: 0.5415\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4638 - accuracy: 0.4946\n",
      "Epoch 00232: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4638 - accuracy: 0.4946 - val_loss: 1.3905 - val_accuracy: 0.5415\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4000 - accuracy: 0.5011\n",
      "Epoch 00233: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4000 - accuracy: 0.5011 - val_loss: 1.4350 - val_accuracy: 0.5366\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4237 - accuracy: 0.5079\n",
      "Epoch 00234: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4237 - accuracy: 0.5079 - val_loss: 1.4111 - val_accuracy: 0.5463\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4158 - accuracy: 0.5151\n",
      "Epoch 00235: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4158 - accuracy: 0.5151 - val_loss: 1.4538 - val_accuracy: 0.5220\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4106 - accuracy: 0.5035\n",
      "Epoch 00236: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4106 - accuracy: 0.5035 - val_loss: 1.3425 - val_accuracy: 0.5561\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4358 - accuracy: 0.5050\n",
      "Epoch 00237: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.4358 - accuracy: 0.5050 - val_loss: 1.3156 - val_accuracy: 0.5707\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4022 - accuracy: 0.5009\n",
      "Epoch 00238: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.4022 - accuracy: 0.5009 - val_loss: 1.5151 - val_accuracy: 0.4927\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3780 - accuracy: 0.5167\n",
      "Epoch 00239: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3780 - accuracy: 0.5167 - val_loss: 1.3409 - val_accuracy: 0.5854\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4125 - accuracy: 0.5180\n",
      "Epoch 00240: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4125 - accuracy: 0.5180 - val_loss: 1.5276 - val_accuracy: 0.4634\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4362 - accuracy: 0.4946\n",
      "Epoch 00241: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.4362 - accuracy: 0.4946 - val_loss: 1.4351 - val_accuracy: 0.5024\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4253 - accuracy: 0.5161\n",
      "Epoch 00242: val_loss did not improve from 1.30855\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4253 - accuracy: 0.5161 - val_loss: 1.4761 - val_accuracy: 0.4683\n",
      "Epoch 243/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.4122 - accuracy: 0.4961\n",
      "Epoch 00243: val_loss improved from 1.30855 to 1.23992, saving model to ./data/cvision/model\\model_10243-1.2399.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.4006 - accuracy: 0.5022 - val_loss: 1.2399 - val_accuracy: 0.5951\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3727 - accuracy: 0.5279\n",
      "Epoch 00244: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.3727 - accuracy: 0.5279 - val_loss: 1.4258 - val_accuracy: 0.5073\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3996 - accuracy: 0.4991\n",
      "Epoch 00245: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3996 - accuracy: 0.4991 - val_loss: 1.4753 - val_accuracy: 0.5268\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4287 - accuracy: 0.4934\n",
      "Epoch 00246: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4287 - accuracy: 0.4934 - val_loss: 1.4870 - val_accuracy: 0.5366\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3901 - accuracy: 0.5009\n",
      "Epoch 00247: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3901 - accuracy: 0.5009 - val_loss: 1.5681 - val_accuracy: 0.4537\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3689 - accuracy: 0.5337\n",
      "Epoch 00248: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3689 - accuracy: 0.5337 - val_loss: 1.4674 - val_accuracy: 0.5268\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4474 - accuracy: 0.5161\n",
      "Epoch 00249: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4474 - accuracy: 0.5161 - val_loss: 1.5588 - val_accuracy: 0.4927\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3643 - accuracy: 0.5110\n",
      "Epoch 00250: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3643 - accuracy: 0.5110 - val_loss: 1.5305 - val_accuracy: 0.4732\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3746 - accuracy: 0.5362\n",
      "Epoch 00251: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3746 - accuracy: 0.5362 - val_loss: 1.5591 - val_accuracy: 0.4732\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4093 - accuracy: 0.5173\n",
      "Epoch 00252: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.4093 - accuracy: 0.5173 - val_loss: 1.5851 - val_accuracy: 0.4927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3858 - accuracy: 0.5154\n",
      "Epoch 00253: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3858 - accuracy: 0.5154 - val_loss: 1.4868 - val_accuracy: 0.5317\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4026 - accuracy: 0.5104\n",
      "Epoch 00254: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4026 - accuracy: 0.5104 - val_loss: 1.3951 - val_accuracy: 0.5366\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4005 - accuracy: 0.5173\n",
      "Epoch 00255: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4005 - accuracy: 0.5173 - val_loss: 1.3812 - val_accuracy: 0.5463\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.5091\n",
      "Epoch 00256: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.4075 - accuracy: 0.5091 - val_loss: 1.5582 - val_accuracy: 0.4829\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3948 - accuracy: 0.4972\n",
      "Epoch 00257: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3948 - accuracy: 0.4972 - val_loss: 1.4318 - val_accuracy: 0.5610\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3855 - accuracy: 0.5135\n",
      "Epoch 00258: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3855 - accuracy: 0.5135 - val_loss: 1.4403 - val_accuracy: 0.5415\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3511 - accuracy: 0.5325\n",
      "Epoch 00259: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3511 - accuracy: 0.5325 - val_loss: 1.5539 - val_accuracy: 0.4732\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3662 - accuracy: 0.5268\n",
      "Epoch 00260: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3662 - accuracy: 0.5268 - val_loss: 1.4034 - val_accuracy: 0.5366\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3938 - accuracy: 0.5280\n",
      "Epoch 00261: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3938 - accuracy: 0.5280 - val_loss: 1.4703 - val_accuracy: 0.5024\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3586 - accuracy: 0.5186\n",
      "Epoch 00262: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3586 - accuracy: 0.5186 - val_loss: 1.3536 - val_accuracy: 0.5512\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3607 - accuracy: 0.5201\n",
      "Epoch 00263: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3607 - accuracy: 0.5201 - val_loss: 1.3479 - val_accuracy: 0.5463\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3940 - accuracy: 0.5223\n",
      "Epoch 00264: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3940 - accuracy: 0.5223 - val_loss: 1.4346 - val_accuracy: 0.5171\n",
      "Epoch 265/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3841 - accuracy: 0.5215\n",
      "Epoch 00265: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3791 - accuracy: 0.5236 - val_loss: 1.4132 - val_accuracy: 0.5073\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3513 - accuracy: 0.5268\n",
      "Epoch 00266: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3513 - accuracy: 0.5268 - val_loss: 1.3689 - val_accuracy: 0.5463\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.4059 - accuracy: 0.5186\n",
      "Epoch 00267: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.4059 - accuracy: 0.5186 - val_loss: 1.2662 - val_accuracy: 0.5805\n",
      "Epoch 268/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3753 - accuracy: 0.5319\n",
      "Epoch 00268: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.3570 - accuracy: 0.5375 - val_loss: 1.3011 - val_accuracy: 0.5854\n",
      "Epoch 269/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3566 - accuracy: 0.5349\n",
      "Epoch 00269: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3485 - accuracy: 0.5362 - val_loss: 1.3054 - val_accuracy: 0.5561\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3326 - accuracy: 0.5285\n",
      "Epoch 00270: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3326 - accuracy: 0.5285 - val_loss: 1.3068 - val_accuracy: 0.5463\n",
      "Epoch 271/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3740 - accuracy: 0.5319\n",
      "Epoch 00271: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.3630 - accuracy: 0.5279 - val_loss: 1.4520 - val_accuracy: 0.5317\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3941 - accuracy: 0.5261\n",
      "Epoch 00272: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3941 - accuracy: 0.5261 - val_loss: 1.4304 - val_accuracy: 0.5415\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3498 - accuracy: 0.5482\n",
      "Epoch 00273: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3498 - accuracy: 0.5482 - val_loss: 1.3127 - val_accuracy: 0.5707\n",
      "Epoch 274/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3561 - accuracy: 0.5299\n",
      "Epoch 00274: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3498 - accuracy: 0.5318 - val_loss: 1.2966 - val_accuracy: 0.5756\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3915 - accuracy: 0.5369\n",
      "Epoch 00275: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3915 - accuracy: 0.5369 - val_loss: 1.3819 - val_accuracy: 0.5610\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3589 - accuracy: 0.5318\n",
      "Epoch 00276: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3589 - accuracy: 0.5318 - val_loss: 1.3851 - val_accuracy: 0.5561\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3327 - accuracy: 0.5381\n",
      "Epoch 00277: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3327 - accuracy: 0.5381 - val_loss: 1.4378 - val_accuracy: 0.5122\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3601 - accuracy: 0.5173\n",
      "Epoch 00278: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3601 - accuracy: 0.5173 - val_loss: 1.3941 - val_accuracy: 0.5317\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3434 - accuracy: 0.5318\n",
      "Epoch 00279: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3434 - accuracy: 0.5318 - val_loss: 1.2855 - val_accuracy: 0.5463\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3815 - accuracy: 0.5274\n",
      "Epoch 00280: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.3815 - accuracy: 0.5274 - val_loss: 1.3023 - val_accuracy: 0.5951\n",
      "Epoch 281/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3125 - accuracy: 0.5352\n",
      "Epoch 00281: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.3187 - accuracy: 0.5331 - val_loss: 1.2626 - val_accuracy: 0.5951\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3588 - accuracy: 0.5211\n",
      "Epoch 00282: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3588 - accuracy: 0.5211 - val_loss: 1.2734 - val_accuracy: 0.6049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3432 - accuracy: 0.5435\n",
      "Epoch 00283: val_loss did not improve from 1.23992\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3432 - accuracy: 0.5435 - val_loss: 1.2887 - val_accuracy: 0.6049\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3474 - accuracy: 0.5451\n",
      "Epoch 00284: val_loss improved from 1.23992 to 1.21366, saving model to ./data/cvision/model\\model_10284-1.2137.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.3474 - accuracy: 0.5451 - val_loss: 1.2137 - val_accuracy: 0.5854\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3576 - accuracy: 0.5249\n",
      "Epoch 00285: val_loss improved from 1.21366 to 1.19958, saving model to ./data/cvision/model\\model_10285-1.1996.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.3576 - accuracy: 0.5249 - val_loss: 1.1996 - val_accuracy: 0.5951\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.5446\n",
      "Epoch 00286: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.3050 - accuracy: 0.5446 - val_loss: 1.2152 - val_accuracy: 0.5951\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3406 - accuracy: 0.5362\n",
      "Epoch 00287: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3406 - accuracy: 0.5362 - val_loss: 1.2955 - val_accuracy: 0.6000\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3360 - accuracy: 0.5318\n",
      "Epoch 00288: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3360 - accuracy: 0.5318 - val_loss: 1.6041 - val_accuracy: 0.4829\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2916 - accuracy: 0.5545\n",
      "Epoch 00289: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2916 - accuracy: 0.5545 - val_loss: 1.2922 - val_accuracy: 0.6049\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3503 - accuracy: 0.5195\n",
      "Epoch 00290: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3503 - accuracy: 0.5195 - val_loss: 1.2735 - val_accuracy: 0.5756\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3698 - accuracy: 0.5154\n",
      "Epoch 00291: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3698 - accuracy: 0.5154 - val_loss: 1.2538 - val_accuracy: 0.5756\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3104 - accuracy: 0.5400\n",
      "Epoch 00292: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3104 - accuracy: 0.5400 - val_loss: 1.3043 - val_accuracy: 0.5659\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3553 - accuracy: 0.5375\n",
      "Epoch 00293: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3553 - accuracy: 0.5375 - val_loss: 1.2238 - val_accuracy: 0.5854\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3672 - accuracy: 0.5123\n",
      "Epoch 00294: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3672 - accuracy: 0.5123 - val_loss: 1.2605 - val_accuracy: 0.5610\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3334 - accuracy: 0.5362\n",
      "Epoch 00295: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3334 - accuracy: 0.5362 - val_loss: 1.2536 - val_accuracy: 0.5854\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3616 - accuracy: 0.5356\n",
      "Epoch 00296: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3616 - accuracy: 0.5356 - val_loss: 1.3258 - val_accuracy: 0.5854\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3345 - accuracy: 0.5331\n",
      "Epoch 00297: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.3345 - accuracy: 0.5331 - val_loss: 1.2434 - val_accuracy: 0.5805\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3408 - accuracy: 0.5482\n",
      "Epoch 00298: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3408 - accuracy: 0.5482 - val_loss: 1.3717 - val_accuracy: 0.5366\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3357 - accuracy: 0.5413\n",
      "Epoch 00299: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3357 - accuracy: 0.5413 - val_loss: 1.2934 - val_accuracy: 0.5512\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3174 - accuracy: 0.5476\n",
      "Epoch 00300: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3174 - accuracy: 0.5476 - val_loss: 1.3160 - val_accuracy: 0.5561\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2898 - accuracy: 0.5570\n",
      "Epoch 00301: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2898 - accuracy: 0.5570 - val_loss: 1.2717 - val_accuracy: 0.5317\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2886 - accuracy: 0.5501\n",
      "Epoch 00302: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2886 - accuracy: 0.5501 - val_loss: 1.2430 - val_accuracy: 0.5805\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3188 - accuracy: 0.5375\n",
      "Epoch 00303: val_loss did not improve from 1.19958\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3188 - accuracy: 0.5375 - val_loss: 1.2758 - val_accuracy: 0.5415\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3461 - accuracy: 0.5495\n",
      "Epoch 00304: val_loss improved from 1.19958 to 1.18665, saving model to ./data/cvision/model\\model_10304-1.1866.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.3461 - accuracy: 0.5495 - val_loss: 1.1866 - val_accuracy: 0.6049\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3262 - accuracy: 0.5539\n",
      "Epoch 00305: val_loss improved from 1.18665 to 1.18193, saving model to ./data/cvision/model\\model_10305-1.1819.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.3262 - accuracy: 0.5539 - val_loss: 1.1819 - val_accuracy: 0.6244\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3079 - accuracy: 0.5474\n",
      "Epoch 00306: val_loss did not improve from 1.18193\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3079 - accuracy: 0.5474 - val_loss: 1.2695 - val_accuracy: 0.6049\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3221 - accuracy: 0.5400\n",
      "Epoch 00307: val_loss did not improve from 1.18193\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3221 - accuracy: 0.5400 - val_loss: 1.2550 - val_accuracy: 0.6000\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3130 - accuracy: 0.5350\n",
      "Epoch 00308: val_loss improved from 1.18193 to 1.16604, saving model to ./data/cvision/model\\model_10308-1.1660.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.3130 - accuracy: 0.5350 - val_loss: 1.1660 - val_accuracy: 0.6098\n",
      "Epoch 309/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2696 - accuracy: 0.5566\n",
      "Epoch 00309: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2722 - accuracy: 0.5558 - val_loss: 1.2692 - val_accuracy: 0.5805\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2899 - accuracy: 0.5507\n",
      "Epoch 00310: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2899 - accuracy: 0.5507 - val_loss: 1.3057 - val_accuracy: 0.5707\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3124 - accuracy: 0.5539\n",
      "Epoch 00311: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3124 - accuracy: 0.5539 - val_loss: 1.3264 - val_accuracy: 0.5805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3054 - accuracy: 0.5329\n",
      "Epoch 00312: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.3054 - accuracy: 0.5329 - val_loss: 1.4053 - val_accuracy: 0.5317\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.5633\n",
      "Epoch 00313: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3148 - accuracy: 0.5633 - val_loss: 1.3118 - val_accuracy: 0.5512\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3410 - accuracy: 0.5514\n",
      "Epoch 00314: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3410 - accuracy: 0.5514 - val_loss: 1.2122 - val_accuracy: 0.5902\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2806 - accuracy: 0.5595\n",
      "Epoch 00315: val_loss did not improve from 1.16604\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2806 - accuracy: 0.5595 - val_loss: 1.2840 - val_accuracy: 0.5659\n",
      "Epoch 316/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2818 - accuracy: 0.5547\n",
      "Epoch 00316: val_loss improved from 1.16604 to 1.16377, saving model to ./data/cvision/model\\model_10316-1.1638.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.2761 - accuracy: 0.5577 - val_loss: 1.1638 - val_accuracy: 0.6049\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3152 - accuracy: 0.5545\n",
      "Epoch 00317: val_loss did not improve from 1.16377\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3152 - accuracy: 0.5545 - val_loss: 1.2437 - val_accuracy: 0.5756\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2682 - accuracy: 0.5577\n",
      "Epoch 00318: val_loss did not improve from 1.16377\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2682 - accuracy: 0.5577 - val_loss: 1.1819 - val_accuracy: 0.6000\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3288 - accuracy: 0.5514\n",
      "Epoch 00319: val_loss did not improve from 1.16377\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3288 - accuracy: 0.5514 - val_loss: 1.2068 - val_accuracy: 0.5902\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3376 - accuracy: 0.5520\n",
      "Epoch 00320: val_loss did not improve from 1.16377\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.3376 - accuracy: 0.5520 - val_loss: 1.1778 - val_accuracy: 0.6098\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2523 - accuracy: 0.5734 ETA: 0s - loss: 1.2394 - accuracy: \n",
      "Epoch 00321: val_loss improved from 1.16377 to 1.15024, saving model to ./data/cvision/model\\model_10321-1.1502.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.2523 - accuracy: 0.5734 - val_loss: 1.1502 - val_accuracy: 0.5902\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3168 - accuracy: 0.5501\n",
      "Epoch 00322: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3168 - accuracy: 0.5501 - val_loss: 1.1882 - val_accuracy: 0.6049\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2929 - accuracy: 0.5457\n",
      "Epoch 00323: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2929 - accuracy: 0.5457 - val_loss: 1.2766 - val_accuracy: 0.5805\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2693 - accuracy: 0.5703\n",
      "Epoch 00324: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2693 - accuracy: 0.5703 - val_loss: 1.1504 - val_accuracy: 0.6049\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3178 - accuracy: 0.5558\n",
      "Epoch 00325: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3178 - accuracy: 0.5558 - val_loss: 1.1759 - val_accuracy: 0.6244\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2996 - accuracy: 0.5400\n",
      "Epoch 00326: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2996 - accuracy: 0.5400 - val_loss: 1.2088 - val_accuracy: 0.6049\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3061 - accuracy: 0.5444\n",
      "Epoch 00327: val_loss did not improve from 1.15024\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.3061 - accuracy: 0.5444 - val_loss: 1.1606 - val_accuracy: 0.6098\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2768 - accuracy: 0.5583\n",
      "Epoch 00328: val_loss improved from 1.15024 to 1.14113, saving model to ./data/cvision/model\\model_10328-1.1411.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.2768 - accuracy: 0.5583 - val_loss: 1.1411 - val_accuracy: 0.5951\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2645 - accuracy: 0.5507\n",
      "Epoch 00329: val_loss did not improve from 1.14113\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2645 - accuracy: 0.5507 - val_loss: 1.2111 - val_accuracy: 0.6000\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3006 - accuracy: 0.5491\n",
      "Epoch 00330: val_loss improved from 1.14113 to 1.12108, saving model to ./data/cvision/model\\model_10330-1.1211.hdf5\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.3006 - accuracy: 0.5491 - val_loss: 1.1211 - val_accuracy: 0.6244\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2902 - accuracy: 0.5451\n",
      "Epoch 00331: val_loss did not improve from 1.12108\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2902 - accuracy: 0.5451 - val_loss: 1.1594 - val_accuracy: 0.6195\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2642 - accuracy: 0.5539\n",
      "Epoch 00332: val_loss did not improve from 1.12108\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2642 - accuracy: 0.5539 - val_loss: 1.1464 - val_accuracy: 0.6293\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2805 - accuracy: 0.5595\n",
      "Epoch 00333: val_loss did not improve from 1.12108\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2805 - accuracy: 0.5595 - val_loss: 1.1311 - val_accuracy: 0.6244\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2717 - accuracy: 0.5640\n",
      "Epoch 00334: val_loss did not improve from 1.12108\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2717 - accuracy: 0.5640 - val_loss: 1.1465 - val_accuracy: 0.6341\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2693 - accuracy: 0.5671\n",
      "Epoch 00335: val_loss improved from 1.12108 to 1.10638, saving model to ./data/cvision/model\\model_10335-1.1064.hdf5\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 1.2693 - accuracy: 0.5671 - val_loss: 1.1064 - val_accuracy: 0.6439\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2837 - accuracy: 0.5621\n",
      "Epoch 00336: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2837 - accuracy: 0.5621 - val_loss: 1.1488 - val_accuracy: 0.6049\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2557 - accuracy: 0.5602\n",
      "Epoch 00337: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2557 - accuracy: 0.5602 - val_loss: 1.2350 - val_accuracy: 0.5902\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2686 - accuracy: 0.5671\n",
      "Epoch 00338: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2686 - accuracy: 0.5671 - val_loss: 1.1721 - val_accuracy: 0.6195\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2772 - accuracy: 0.5476\n",
      "Epoch 00339: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2772 - accuracy: 0.5476 - val_loss: 1.2136 - val_accuracy: 0.6000\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2699 - accuracy: 0.5753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00340: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2699 - accuracy: 0.5753 - val_loss: 1.1316 - val_accuracy: 0.6195\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2779 - accuracy: 0.5737\n",
      "Epoch 00341: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2779 - accuracy: 0.5737 - val_loss: 1.1971 - val_accuracy: 0.6293\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2372 - accuracy: 0.5715\n",
      "Epoch 00342: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2372 - accuracy: 0.5715 - val_loss: 1.3202 - val_accuracy: 0.6146\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2721 - accuracy: 0.5564\n",
      "Epoch 00343: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2721 - accuracy: 0.5564 - val_loss: 1.1266 - val_accuracy: 0.6390\n",
      "Epoch 344/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2723 - accuracy: 0.5677\n",
      "Epoch 00344: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2689 - accuracy: 0.5677 - val_loss: 1.2798 - val_accuracy: 0.5951\n",
      "Epoch 345/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2653 - accuracy: 0.5664\n",
      "Epoch 00345: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2719 - accuracy: 0.5640 - val_loss: 1.1834 - val_accuracy: 0.6098\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2591 - accuracy: 0.5627\n",
      "Epoch 00346: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2591 - accuracy: 0.5627 - val_loss: 1.2401 - val_accuracy: 0.5951\n",
      "Epoch 347/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2791 - accuracy: 0.5560\n",
      "Epoch 00347: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2786 - accuracy: 0.5564 - val_loss: 1.1326 - val_accuracy: 0.6293\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2583 - accuracy: 0.5721\n",
      "Epoch 00348: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2583 - accuracy: 0.5721 - val_loss: 1.2079 - val_accuracy: 0.6146\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2875 - accuracy: 0.5451\n",
      "Epoch 00349: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2875 - accuracy: 0.5451 - val_loss: 1.1180 - val_accuracy: 0.6585\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2268 - accuracy: 0.5740\n",
      "Epoch 00350: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2268 - accuracy: 0.5740 - val_loss: 1.2114 - val_accuracy: 0.6195\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3031 - accuracy: 0.5665\n",
      "Epoch 00351: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.3031 - accuracy: 0.5665 - val_loss: 1.1891 - val_accuracy: 0.6244\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2580 - accuracy: 0.5564\n",
      "Epoch 00352: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2580 - accuracy: 0.5564 - val_loss: 1.1630 - val_accuracy: 0.6195\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2688 - accuracy: 0.5715\n",
      "Epoch 00353: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2688 - accuracy: 0.5715 - val_loss: 1.1706 - val_accuracy: 0.6049\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2812 - accuracy: 0.5570\n",
      "Epoch 00354: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2812 - accuracy: 0.5570 - val_loss: 1.1653 - val_accuracy: 0.6244\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2432 - accuracy: 0.5640\n",
      "Epoch 00355: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2432 - accuracy: 0.5640 - val_loss: 1.2094 - val_accuracy: 0.5951\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2279 - accuracy: 0.5715\n",
      "Epoch 00356: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2279 - accuracy: 0.5715 - val_loss: 1.2567 - val_accuracy: 0.6000\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2443 - accuracy: 0.5829\n",
      "Epoch 00357: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2443 - accuracy: 0.5829 - val_loss: 1.3496 - val_accuracy: 0.5415\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2486 - accuracy: 0.5816\n",
      "Epoch 00358: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2486 - accuracy: 0.5816 - val_loss: 1.1786 - val_accuracy: 0.5805\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2156 - accuracy: 0.5742\n",
      "Epoch 00359: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2156 - accuracy: 0.5742 - val_loss: 1.2363 - val_accuracy: 0.5805\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2752 - accuracy: 0.5577\n",
      "Epoch 00360: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2752 - accuracy: 0.5577 - val_loss: 1.3320 - val_accuracy: 0.5707\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2376 - accuracy: 0.5822\n",
      "Epoch 00361: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2376 - accuracy: 0.5822 - val_loss: 1.3043 - val_accuracy: 0.5805\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2229 - accuracy: 0.5658\n",
      "Epoch 00362: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2229 - accuracy: 0.5658 - val_loss: 1.3093 - val_accuracy: 0.5366\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1941 - accuracy: 0.5703\n",
      "Epoch 00363: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.1941 - accuracy: 0.5703 - val_loss: 1.2983 - val_accuracy: 0.5902\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2712 - accuracy: 0.5766\n",
      "Epoch 00364: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2712 - accuracy: 0.5766 - val_loss: 1.2472 - val_accuracy: 0.6049\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2743 - accuracy: 0.5709\n",
      "Epoch 00365: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2743 - accuracy: 0.5709 - val_loss: 1.1509 - val_accuracy: 0.6195\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2587 - accuracy: 0.5696\n",
      "Epoch 00366: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2587 - accuracy: 0.5696 - val_loss: 1.1708 - val_accuracy: 0.6293\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2456 - accuracy: 0.5658 ETA: 0s - loss: 1.2992 - accuracy\n",
      "Epoch 00367: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2456 - accuracy: 0.5658 - val_loss: 1.2033 - val_accuracy: 0.5756\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2568 - accuracy: 0.5810\n",
      "Epoch 00368: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2568 - accuracy: 0.5810 - val_loss: 1.2778 - val_accuracy: 0.5756\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2520 - accuracy: 0.5614\n",
      "Epoch 00369: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2520 - accuracy: 0.5614 - val_loss: 1.2516 - val_accuracy: 0.5707\n",
      "Epoch 370/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 1.2274 - accuracy: 0.5721\n",
      "Epoch 00370: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2274 - accuracy: 0.5721 - val_loss: 1.2724 - val_accuracy: 0.5756\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2213 - accuracy: 0.5671\n",
      "Epoch 00371: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2213 - accuracy: 0.5671 - val_loss: 1.2745 - val_accuracy: 0.5756\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1990 - accuracy: 0.5873\n",
      "Epoch 00372: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1990 - accuracy: 0.5873 - val_loss: 1.2781 - val_accuracy: 0.5659\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2704 - accuracy: 0.5495\n",
      "Epoch 00373: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2704 - accuracy: 0.5495 - val_loss: 1.4066 - val_accuracy: 0.5415\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2222 - accuracy: 0.5784\n",
      "Epoch 00374: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2222 - accuracy: 0.5784 - val_loss: 1.3651 - val_accuracy: 0.5415\n",
      "Epoch 375/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2634 - accuracy: 0.5560\n",
      "Epoch 00375: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2418 - accuracy: 0.5614 - val_loss: 1.2622 - val_accuracy: 0.5707\n",
      "Epoch 376/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2254 - accuracy: 0.5794\n",
      "Epoch 00376: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2280 - accuracy: 0.5791 - val_loss: 1.5126 - val_accuracy: 0.5268\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2167 - accuracy: 0.5820\n",
      "Epoch 00377: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2167 - accuracy: 0.5820 - val_loss: 1.3009 - val_accuracy: 0.5610\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2301 - accuracy: 0.5583\n",
      "Epoch 00378: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2301 - accuracy: 0.5583 - val_loss: 1.2974 - val_accuracy: 0.5659\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1949 - accuracy: 0.5910\n",
      "Epoch 00379: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.1949 - accuracy: 0.5910 - val_loss: 1.2759 - val_accuracy: 0.5659\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2237 - accuracy: 0.5804\n",
      "Epoch 00380: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2237 - accuracy: 0.5804 - val_loss: 1.2663 - val_accuracy: 0.5805\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2157 - accuracy: 0.5980\n",
      "Epoch 00381: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2157 - accuracy: 0.5980 - val_loss: 1.3629 - val_accuracy: 0.5756\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2354 - accuracy: 0.5810\n",
      "Epoch 00382: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2354 - accuracy: 0.5810 - val_loss: 1.4532 - val_accuracy: 0.5561\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2748 - accuracy: 0.5646\n",
      "Epoch 00383: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2748 - accuracy: 0.5646 - val_loss: 1.2478 - val_accuracy: 0.5854\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2099 - accuracy: 0.5873\n",
      "Epoch 00384: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2099 - accuracy: 0.5873 - val_loss: 1.3506 - val_accuracy: 0.5512\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2746 - accuracy: 0.5602\n",
      "Epoch 00385: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2746 - accuracy: 0.5602 - val_loss: 1.3272 - val_accuracy: 0.5659\n",
      "Epoch 386/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2108 - accuracy: 0.5944\n",
      "Epoch 00386: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2056 - accuracy: 0.5948 - val_loss: 1.3632 - val_accuracy: 0.5415\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2124 - accuracy: 0.5860\n",
      "Epoch 00387: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2124 - accuracy: 0.5860 - val_loss: 1.4169 - val_accuracy: 0.5415\n",
      "Epoch 388/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2109 - accuracy: 0.5794\n",
      "Epoch 00388: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2162 - accuracy: 0.5778 - val_loss: 1.1872 - val_accuracy: 0.6244\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1970 - accuracy: 0.5974\n",
      "Epoch 00389: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1970 - accuracy: 0.5974 - val_loss: 1.2612 - val_accuracy: 0.5756\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1911 - accuracy: 0.5992\n",
      "Epoch 00390: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.1911 - accuracy: 0.5992 - val_loss: 1.2371 - val_accuracy: 0.5854\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1843 - accuracy: 0.6011\n",
      "Epoch 00391: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1843 - accuracy: 0.6011 - val_loss: 1.1518 - val_accuracy: 0.6146\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2344 - accuracy: 0.5904\n",
      "Epoch 00392: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2344 - accuracy: 0.5904 - val_loss: 1.1183 - val_accuracy: 0.6244\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2372 - accuracy: 0.5866\n",
      "Epoch 00393: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2372 - accuracy: 0.5866 - val_loss: 1.1823 - val_accuracy: 0.6146\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1878 - accuracy: 0.5892\n",
      "Epoch 00394: val_loss did not improve from 1.10638\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1878 - accuracy: 0.5892 - val_loss: 1.1621 - val_accuracy: 0.6000\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2552 - accuracy: 0.5665\n",
      "Epoch 00395: val_loss improved from 1.10638 to 1.08109, saving model to ./data/cvision/model\\model_10395-1.0811.hdf5\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.2552 - accuracy: 0.5665 - val_loss: 1.0811 - val_accuracy: 0.6244\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2184 - accuracy: 0.5740\n",
      "Epoch 00396: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2184 - accuracy: 0.5740 - val_loss: 1.1132 - val_accuracy: 0.6390\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2197 - accuracy: 0.5866\n",
      "Epoch 00397: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2197 - accuracy: 0.5866 - val_loss: 1.1257 - val_accuracy: 0.6341\n",
      "Epoch 398/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2234 - accuracy: 0.5883\n",
      "Epoch 00398: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2331 - accuracy: 0.5822 - val_loss: 1.2771 - val_accuracy: 0.5902\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2213 - accuracy: 0.5893\n",
      "Epoch 00399: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.2213 - accuracy: 0.5893 - val_loss: 1.0881 - val_accuracy: 0.6488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1820 - accuracy: 0.5904\n",
      "Epoch 00400: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1820 - accuracy: 0.5904 - val_loss: 1.1783 - val_accuracy: 0.6244\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1873 - accuracy: 0.5798\n",
      "Epoch 00401: val_loss did not improve from 1.08109\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 1.1873 - accuracy: 0.5798 - val_loss: 1.1455 - val_accuracy: 0.6293\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2004 - accuracy: 0.5942\n",
      "Epoch 00402: val_loss improved from 1.08109 to 1.06971, saving model to ./data/cvision/model\\model_10402-1.0697.hdf5\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 1.2004 - accuracy: 0.5942 - val_loss: 1.0697 - val_accuracy: 0.6585\n",
      "Epoch 403/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2006 - accuracy: 0.5951\n",
      "Epoch 00403: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1958 - accuracy: 0.5980 - val_loss: 1.1969 - val_accuracy: 0.6146\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2014 - accuracy: 0.5921 ETA: 0s - loss: 1.2208 - accuracy\n",
      "Epoch 00404: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.2014 - accuracy: 0.5921 - val_loss: 1.1479 - val_accuracy: 0.6488\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2166 - accuracy: 0.5709\n",
      "Epoch 00405: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2166 - accuracy: 0.5709 - val_loss: 1.1278 - val_accuracy: 0.6195\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1922 - accuracy: 0.5999\n",
      "Epoch 00406: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1922 - accuracy: 0.5999 - val_loss: 1.1181 - val_accuracy: 0.6390\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2166 - accuracy: 0.5887\n",
      "Epoch 00407: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2166 - accuracy: 0.5887 - val_loss: 1.1299 - val_accuracy: 0.6390\n",
      "Epoch 408/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.2293 - accuracy: 0.5875\n",
      "Epoch 00408: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2174 - accuracy: 0.5854 - val_loss: 1.1933 - val_accuracy: 0.6049\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2161 - accuracy: 0.5866\n",
      "Epoch 00409: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2161 - accuracy: 0.5866 - val_loss: 1.1175 - val_accuracy: 0.6146\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2539 - accuracy: 0.5822\n",
      "Epoch 00410: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.2539 - accuracy: 0.5822 - val_loss: 1.1460 - val_accuracy: 0.6293\n",
      "Epoch 411/2000\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.1574 - accuracy: 0.6022\n",
      "Epoch 00411: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.1563 - accuracy: 0.6005 - val_loss: 1.2647 - val_accuracy: 0.5756\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2418 - accuracy: 0.5810\n",
      "Epoch 00412: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.2418 - accuracy: 0.5810 - val_loss: 1.2289 - val_accuracy: 0.5902\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1691 - accuracy: 0.5982\n",
      "Epoch 00413: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 1.1691 - accuracy: 0.5982 - val_loss: 1.2733 - val_accuracy: 0.6146\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2012 - accuracy: 0.5740\n",
      "Epoch 00414: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.2012 - accuracy: 0.5740 - val_loss: 1.1843 - val_accuracy: 0.6098\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1936 - accuracy: 0.5917\n",
      "Epoch 00415: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.1936 - accuracy: 0.5917 - val_loss: 1.1740 - val_accuracy: 0.6244\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2088 - accuracy: 0.5898\n",
      "Epoch 00416: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 1.2088 - accuracy: 0.5898 - val_loss: 1.2565 - val_accuracy: 0.5951\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1880 - accuracy: 0.5936\n",
      "Epoch 00417: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 1.1880 - accuracy: 0.5936 - val_loss: 1.4006 - val_accuracy: 0.5366\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2253 - accuracy: 0.5778\n",
      "Epoch 00418: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2253 - accuracy: 0.5778 - val_loss: 1.2375 - val_accuracy: 0.5951\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1961 - accuracy: 0.5684\n",
      "Epoch 00419: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.1961 - accuracy: 0.5684 - val_loss: 1.2134 - val_accuracy: 0.5951\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1858 - accuracy: 0.5835\n",
      "Epoch 00420: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.1858 - accuracy: 0.5835 - val_loss: 1.2662 - val_accuracy: 0.5854\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1848 - accuracy: 0.5967\n",
      "Epoch 00421: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 1.1848 - accuracy: 0.5967 - val_loss: 1.2089 - val_accuracy: 0.6146\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1943 - accuracy: 0.5860\n",
      "Epoch 00422: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.1943 - accuracy: 0.5860 - val_loss: 1.2596 - val_accuracy: 0.5854\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2175 - accuracy: 0.5829\n",
      "Epoch 00423: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2175 - accuracy: 0.5829 - val_loss: 1.1737 - val_accuracy: 0.6195\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.2091 - accuracy: 0.5892\n",
      "Epoch 00424: val_loss did not improve from 1.06971\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 1.2091 - accuracy: 0.5892 - val_loss: 1.0984 - val_accuracy: 0.6244\n",
      "Epoch 00424: early stopping\n",
      "CNN: Epochs=2000, Train accuracy=0.60113, Validation accuracy=0.65854\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[es, cp]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1598339021678,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "QTPZASsAHFg3",
    "outputId": "2f0741de-2815-4be5-d0ae-fa77588688d4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EQCvPGZks9v"
   },
   "outputs": [],
   "source": [
    "#모델 구조 저장\n",
    "model_json = model.to_json()\n",
    "with open(f'data/cvision/model_10.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-8nb5jokyny"
   },
   "outputs": [],
   "source": [
    "#예측 진행\n",
    "res = model.predict_classes(test_X)\n",
    "\n",
    "submission.digit = res\n",
    "submission.to_csv('data/cvision/my_subm_9-6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54T6v8cCHFg5"
   },
   "outputs": [],
   "source": [
    "#model_10: ResNet - residual block 구성\n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
    ")\n",
    "\n",
    "def conv1(x):\n",
    "    x = Conv2D(64, (7,7), strides=(2,2), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv2(x, filter_in=64, filter_out=256):\n",
    "    x = MaxPooling2D((3,3), 2)(x)\n",
    "    \n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(1):#3):\n",
    "        if (i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "        \n",
    "    return x\n",
    "\n",
    "def conv3(x, filter_in=128, filter_out=512):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(2):#4):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv4(x, filter_in=256, filter_out=1024):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(1):#6):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x\n",
    "\n",
    "def conv5(x, filter_in=512, filter_out=2048):\n",
    "    shortcut = x\n",
    "    \n",
    "    for i in range(1):#3):\n",
    "        if(i == 0):\n",
    "            x = Conv2D(filter_in, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            shortcut = Conv2D(filter_out, (1,1), strides=(2,2), padding='valid', kernel_initializer='he_normal')(shortcut) #각 layer의 첫 block에서는 dimension 증가 필요\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "            \n",
    "        else:\n",
    "            x = Conv2D(filter_in, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_in, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            x = Conv2D(filter_out, (1,1), strides=(1,1), padding='valid', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "            x = Add()([x, shortcut])\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "            \n",
    "            shortcut = x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr98KwNSHFg7",
    "outputId": "3a0dd29f-d4a9-4b93-e734-e9304c3c55de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_55\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1110 (Conv2D)            (None, 14, 14, 64)   3200        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1110 (Batch (None, 14, 14, 64)   256         conv2d_1110[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_545 (LeakyReLU)     (None, 14, 14, 64)   0           batch_normalization_1110[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 6, 6, 64)     0           leaky_re_lu_545[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1111 (Conv2D)            (None, 6, 6, 32)     2080        max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1111 (Batch (None, 6, 6, 32)     128         conv2d_1111[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_546 (LeakyReLU)     (None, 6, 6, 32)     0           batch_normalization_1111[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1112 (Conv2D)            (None, 6, 6, 32)     9248        leaky_re_lu_546[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1112 (Batch (None, 6, 6, 32)     128         conv2d_1112[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_547 (LeakyReLU)     (None, 6, 6, 32)     0           batch_normalization_1112[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1113 (Conv2D)            (None, 6, 6, 64)     2112        leaky_re_lu_547[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1114 (Conv2D)            (None, 6, 6, 64)     4160        max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1113 (Batch (None, 6, 6, 64)     256         conv2d_1113[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1114 (Batch (None, 6, 6, 64)     256         conv2d_1114[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_326 (Add)                   (None, 6, 6, 64)     0           batch_normalization_1113[0][0]   \n",
      "                                                                 batch_normalization_1114[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_548 (LeakyReLU)     (None, 6, 6, 64)     0           add_326[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 6, 6, 64)     0           leaky_re_lu_548[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1115 (Conv2D)            (None, 3, 3, 32)     2080        dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1115 (Batch (None, 3, 3, 32)     128         conv2d_1115[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_549 (LeakyReLU)     (None, 3, 3, 32)     0           batch_normalization_1115[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1116 (Conv2D)            (None, 3, 3, 32)     9248        leaky_re_lu_549[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1116 (Batch (None, 3, 3, 32)     128         conv2d_1116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_550 (LeakyReLU)     (None, 3, 3, 32)     0           batch_normalization_1116[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1117 (Conv2D)            (None, 3, 3, 64)     2112        leaky_re_lu_550[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1118 (Conv2D)            (None, 3, 3, 64)     4160        dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1117 (Batch (None, 3, 3, 64)     256         conv2d_1117[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1118 (Batch (None, 3, 3, 64)     256         conv2d_1118[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_327 (Add)                   (None, 3, 3, 64)     0           batch_normalization_1117[0][0]   \n",
      "                                                                 batch_normalization_1118[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_551 (LeakyReLU)     (None, 3, 3, 64)     0           add_327[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1119 (Conv2D)            (None, 3, 3, 32)     2080        leaky_re_lu_551[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1119 (Batch (None, 3, 3, 32)     128         conv2d_1119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_552 (LeakyReLU)     (None, 3, 3, 32)     0           batch_normalization_1119[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1120 (Conv2D)            (None, 3, 3, 32)     9248        leaky_re_lu_552[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1120 (Batch (None, 3, 3, 32)     128         conv2d_1120[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_553 (LeakyReLU)     (None, 3, 3, 32)     0           batch_normalization_1120[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1121 (Conv2D)            (None, 3, 3, 64)     2112        leaky_re_lu_553[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1121 (Batch (None, 3, 3, 64)     256         conv2d_1121[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_328 (Add)                   (None, 3, 3, 64)     0           batch_normalization_1121[0][0]   \n",
      "                                                                 leaky_re_lu_551[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_554 (LeakyReLU)     (None, 3, 3, 64)     0           add_328[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 3, 3, 64)     0           leaky_re_lu_554[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1122 (Conv2D)            (None, 2, 2, 64)     4160        dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1122 (Batch (None, 2, 2, 64)     256         conv2d_1122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_555 (LeakyReLU)     (None, 2, 2, 64)     0           batch_normalization_1122[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1123 (Conv2D)            (None, 2, 2, 64)     36928       leaky_re_lu_555[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1123 (Batch (None, 2, 2, 64)     256         conv2d_1123[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_556 (LeakyReLU)     (None, 2, 2, 64)     0           batch_normalization_1123[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1124 (Conv2D)            (None, 2, 2, 128)    8320        leaky_re_lu_556[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1125 (Conv2D)            (None, 2, 2, 128)    8320        dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1124 (Batch (None, 2, 2, 128)    512         conv2d_1124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1125 (Batch (None, 2, 2, 128)    512         conv2d_1125[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_329 (Add)                   (None, 2, 2, 128)    0           batch_normalization_1124[0][0]   \n",
      "                                                                 batch_normalization_1125[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_557 (LeakyReLU)     (None, 2, 2, 128)    0           add_329[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 2, 2, 128)    0           leaky_re_lu_557[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1126 (Conv2D)            (None, 1, 1, 64)     8256        dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1126 (Batch (None, 1, 1, 64)     256         conv2d_1126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_558 (LeakyReLU)     (None, 1, 1, 64)     0           batch_normalization_1126[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1127 (Conv2D)            (None, 1, 1, 64)     36928       leaky_re_lu_558[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1127 (Batch (None, 1, 1, 64)     256         conv2d_1127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_559 (LeakyReLU)     (None, 1, 1, 64)     0           batch_normalization_1127[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1128 (Conv2D)            (None, 1, 1, 128)    8320        leaky_re_lu_559[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1129 (Conv2D)            (None, 1, 1, 128)    16512       dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1128 (Batch (None, 1, 1, 128)    512         conv2d_1128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1129 (Batch (None, 1, 1, 128)    512         conv2d_1129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_330 (Add)                   (None, 1, 1, 128)    0           batch_normalization_1128[0][0]   \n",
      "                                                                 batch_normalization_1129[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_560 (LeakyReLU)     (None, 1, 1, 128)    0           add_330[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 1, 1, 128)    0           leaky_re_lu_560[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_27 (Gl (None, 128)          0           dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 10)           1290        global_average_pooling2d_27[0][0]\n",
      "==================================================================================================\n",
      "Total params: 186,250\n",
      "Trainable params: 183,562\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "#모델링\n",
    "classes = 10\n",
    "tensor_in = Input(shape= train_X.shape[1:], dtype='float32', name='input')\n",
    "\n",
    "x = conv1(tensor_in)\n",
    "x = conv2(x, 32, 64)\n",
    "x = Dropout(0.4)(x)\n",
    "x = conv3(x, 32, 64)\n",
    "x = Dropout(0.4)(x)\n",
    "x = conv4(x, 64, 128)\n",
    "x = Dropout(0.4)(x)\n",
    "x = conv5(x, 64, 128)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "tensor_out = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(tensor_in, tensor_out)\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#Adam(0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19652,
     "status": "error",
     "timestamp": 1598178350080,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "fPtBXYtwHFg-",
    "outputId": "cb892981-08ed-4ba9-af29-c492b265fb8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 3.3559 - accuracy: 0.0964 - val_loss: 2.9831 - val_accuracy: 0.0877\n",
      "Epoch 2/4000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.3912 - accuracy: 0.1006 - val_loss: 2.8385 - val_accuracy: 0.0909\n",
      "Epoch 3/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.3649 - accuracy: 0.0992 - val_loss: 2.7397 - val_accuracy: 0.0974\n",
      "Epoch 4/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1698 - accuracy: 0.1159 - val_loss: 2.6587 - val_accuracy: 0.0942\n",
      "Epoch 5/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2307 - accuracy: 0.1103 - val_loss: 2.5902 - val_accuracy: 0.0974\n",
      "Epoch 6/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1670 - accuracy: 0.1045 - val_loss: 2.5337 - val_accuracy: 0.0877\n",
      "Epoch 7/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1895 - accuracy: 0.1034 - val_loss: 2.4875 - val_accuracy: 0.0877\n",
      "Epoch 8/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1855 - accuracy: 0.0918 - val_loss: 2.4508 - val_accuracy: 0.0877\n",
      "Epoch 9/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1438 - accuracy: 0.1084 - val_loss: 2.4196 - val_accuracy: 0.0877\n",
      "Epoch 10/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1243 - accuracy: 0.0922 - val_loss: 2.3934 - val_accuracy: 0.0877\n",
      "Epoch 11/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9914 - accuracy: 0.1229 - val_loss: 2.3719 - val_accuracy: 0.0942\n",
      "Epoch 12/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0302 - accuracy: 0.1159 - val_loss: 2.3548 - val_accuracy: 0.1006\n",
      "Epoch 13/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9630 - accuracy: 0.1075 - val_loss: 2.3416 - val_accuracy: 0.0974\n",
      "Epoch 14/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9993 - accuracy: 0.0978 - val_loss: 2.3315 - val_accuracy: 0.0942\n",
      "Epoch 15/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9481 - accuracy: 0.1201 - val_loss: 2.3232 - val_accuracy: 0.0942\n",
      "Epoch 16/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9802 - accuracy: 0.1064 - val_loss: 2.3178 - val_accuracy: 0.0844\n",
      "Epoch 17/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9274 - accuracy: 0.0967 - val_loss: 2.3124 - val_accuracy: 0.0877\n",
      "Epoch 18/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9336 - accuracy: 0.1045 - val_loss: 2.3080 - val_accuracy: 0.0779\n",
      "Epoch 19/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9374 - accuracy: 0.1094 - val_loss: 2.3037 - val_accuracy: 0.0714\n",
      "Epoch 20/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9103 - accuracy: 0.1084 - val_loss: 2.2997 - val_accuracy: 0.0844\n",
      "Epoch 21/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8997 - accuracy: 0.1094 - val_loss: 2.2950 - val_accuracy: 0.0779\n",
      "Epoch 22/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9298 - accuracy: 0.1006 - val_loss: 2.2913 - val_accuracy: 0.0877\n",
      "Epoch 23/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8956 - accuracy: 0.1104 - val_loss: 2.2893 - val_accuracy: 0.1234\n",
      "Epoch 24/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8087 - accuracy: 0.1173 - val_loss: 2.2888 - val_accuracy: 0.1234\n",
      "Epoch 25/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8729 - accuracy: 0.0938 - val_loss: 2.2878 - val_accuracy: 0.1299\n",
      "Epoch 26/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8045 - accuracy: 0.1035 - val_loss: 2.2858 - val_accuracy: 0.1461\n",
      "Epoch 27/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8440 - accuracy: 0.1123 - val_loss: 2.2831 - val_accuracy: 0.1461\n",
      "Epoch 28/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7647 - accuracy: 0.1075 - val_loss: 2.2809 - val_accuracy: 0.1494\n",
      "Epoch 29/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7985 - accuracy: 0.1182 - val_loss: 2.2792 - val_accuracy: 0.1623\n",
      "Epoch 30/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7777 - accuracy: 0.1123 - val_loss: 2.2777 - val_accuracy: 0.1623\n",
      "Epoch 31/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7981 - accuracy: 0.1103 - val_loss: 2.2766 - val_accuracy: 0.1688\n",
      "Epoch 32/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7751 - accuracy: 0.1341 - val_loss: 2.2756 - val_accuracy: 0.1591\n",
      "Epoch 33/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7998 - accuracy: 0.1034 - val_loss: 2.2743 - val_accuracy: 0.1494\n",
      "Epoch 34/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7351 - accuracy: 0.1117 - val_loss: 2.2738 - val_accuracy: 0.1494\n",
      "Epoch 35/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7202 - accuracy: 0.1016 - val_loss: 2.2736 - val_accuracy: 0.1494\n",
      "Epoch 36/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7096 - accuracy: 0.0977 - val_loss: 2.2730 - val_accuracy: 0.1526\n",
      "Epoch 37/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6793 - accuracy: 0.1211 - val_loss: 2.2721 - val_accuracy: 0.1558\n",
      "Epoch 38/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6852 - accuracy: 0.1201 - val_loss: 2.2715 - val_accuracy: 0.1623\n",
      "Epoch 39/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6476 - accuracy: 0.1173 - val_loss: 2.2712 - val_accuracy: 0.1494\n",
      "Epoch 40/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6726 - accuracy: 0.1201 - val_loss: 2.2709 - val_accuracy: 0.1429\n",
      "Epoch 41/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7411 - accuracy: 0.0964 - val_loss: 2.2704 - val_accuracy: 0.1396\n",
      "Epoch 42/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6740 - accuracy: 0.1103 - val_loss: 2.2700 - val_accuracy: 0.1461\n",
      "Epoch 43/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6002 - accuracy: 0.1201 - val_loss: 2.2693 - val_accuracy: 0.1461\n",
      "Epoch 44/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6609 - accuracy: 0.1182 - val_loss: 2.2677 - val_accuracy: 0.1461\n",
      "Epoch 45/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6416 - accuracy: 0.1187 - val_loss: 2.2665 - val_accuracy: 0.1331\n",
      "Epoch 46/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5942 - accuracy: 0.1327 - val_loss: 2.2659 - val_accuracy: 0.1331\n",
      "Epoch 47/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6864 - accuracy: 0.1201 - val_loss: 2.2653 - val_accuracy: 0.1331\n",
      "Epoch 48/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6283 - accuracy: 0.1173 - val_loss: 2.2650 - val_accuracy: 0.1266\n",
      "Epoch 49/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6614 - accuracy: 0.1113 - val_loss: 2.2652 - val_accuracy: 0.1266\n",
      "Epoch 50/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6383 - accuracy: 0.1143 - val_loss: 2.2658 - val_accuracy: 0.1039\n",
      "Epoch 51/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5500 - accuracy: 0.1230 - val_loss: 2.2660 - val_accuracy: 0.0909\n",
      "Epoch 52/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5997 - accuracy: 0.1229 - val_loss: 2.2663 - val_accuracy: 0.0942\n",
      "Epoch 53/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5509 - accuracy: 0.1117 - val_loss: 2.2665 - val_accuracy: 0.1039\n",
      "Epoch 54/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5918 - accuracy: 0.1035 - val_loss: 2.2666 - val_accuracy: 0.1006\n",
      "Epoch 55/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6295 - accuracy: 0.1075 - val_loss: 2.2662 - val_accuracy: 0.1136\n",
      "Epoch 56/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5616 - accuracy: 0.1299 - val_loss: 2.2654 - val_accuracy: 0.1136\n",
      "Epoch 57/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5678 - accuracy: 0.1240 - val_loss: 2.2648 - val_accuracy: 0.1201\n",
      "Epoch 58/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5844 - accuracy: 0.1113 - val_loss: 2.2640 - val_accuracy: 0.1266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5125 - accuracy: 0.1260 - val_loss: 2.2632 - val_accuracy: 0.1266\n",
      "Epoch 60/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5432 - accuracy: 0.1089 - val_loss: 2.2628 - val_accuracy: 0.1396\n",
      "Epoch 61/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5274 - accuracy: 0.1397 - val_loss: 2.2621 - val_accuracy: 0.1429\n",
      "Epoch 62/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4883 - accuracy: 0.1221 - val_loss: 2.2610 - val_accuracy: 0.1429\n",
      "Epoch 63/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5257 - accuracy: 0.1074 - val_loss: 2.2594 - val_accuracy: 0.1396\n",
      "Epoch 64/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5400 - accuracy: 0.1172 - val_loss: 2.2577 - val_accuracy: 0.1494\n",
      "Epoch 65/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4593 - accuracy: 0.1397 - val_loss: 2.2554 - val_accuracy: 0.1526\n",
      "Epoch 66/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5329 - accuracy: 0.0978 - val_loss: 2.2535 - val_accuracy: 0.1558\n",
      "Epoch 67/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5395 - accuracy: 0.1104 - val_loss: 2.2516 - val_accuracy: 0.1591\n",
      "Epoch 68/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.4761 - accuracy: 0.1369 - val_loss: 2.2500 - val_accuracy: 0.1526\n",
      "Epoch 69/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.4866 - accuracy: 0.1143 - val_loss: 2.2481 - val_accuracy: 0.1591\n",
      "Epoch 70/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.4883 - accuracy: 0.1240 - val_loss: 2.2463 - val_accuracy: 0.1656\n",
      "Epoch 71/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5064 - accuracy: 0.0978 - val_loss: 2.2451 - val_accuracy: 0.1656\n",
      "Epoch 72/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4676 - accuracy: 0.1243 - val_loss: 2.2438 - val_accuracy: 0.1753\n",
      "Epoch 73/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4555 - accuracy: 0.1299 - val_loss: 2.2427 - val_accuracy: 0.1753\n",
      "Epoch 74/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.4463 - accuracy: 0.1162 - val_loss: 2.2417 - val_accuracy: 0.1753\n",
      "Epoch 75/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5065 - accuracy: 0.1117 - val_loss: 2.2411 - val_accuracy: 0.1786\n",
      "Epoch 76/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.4623 - accuracy: 0.1215 - val_loss: 2.2408 - val_accuracy: 0.1851\n",
      "Epoch 77/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.4452 - accuracy: 0.1279 - val_loss: 2.2406 - val_accuracy: 0.1916\n",
      "Epoch 78/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4190 - accuracy: 0.1397 - val_loss: 2.2399 - val_accuracy: 0.1948\n",
      "Epoch 79/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.3879 - accuracy: 0.1465 - val_loss: 2.2393 - val_accuracy: 0.1981\n",
      "Epoch 80/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.4164 - accuracy: 0.1439 - val_loss: 2.2384 - val_accuracy: 0.2045\n",
      "Epoch 81/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4606 - accuracy: 0.1279 - val_loss: 2.2370 - val_accuracy: 0.2013\n",
      "Epoch 82/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.4072 - accuracy: 0.1215 - val_loss: 2.2357 - val_accuracy: 0.2078\n",
      "Epoch 83/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4046 - accuracy: 0.1285 - val_loss: 2.2349 - val_accuracy: 0.2143\n",
      "Epoch 84/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4548 - accuracy: 0.1182 - val_loss: 2.2342 - val_accuracy: 0.2143\n",
      "Epoch 85/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.4058 - accuracy: 0.1229 - val_loss: 2.2339 - val_accuracy: 0.2175\n",
      "Epoch 86/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.3797 - accuracy: 0.1383 - val_loss: 2.2337 - val_accuracy: 0.2175\n",
      "Epoch 87/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.4110 - accuracy: 0.1152 - val_loss: 2.2335 - val_accuracy: 0.2143\n",
      "Epoch 88/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.3521 - accuracy: 0.1387 - val_loss: 2.2331 - val_accuracy: 0.2110\n",
      "Epoch 89/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.3638 - accuracy: 0.1387 - val_loss: 2.2329 - val_accuracy: 0.2143\n",
      "Epoch 90/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.3966 - accuracy: 0.1318 - val_loss: 2.2322 - val_accuracy: 0.2110\n",
      "Epoch 91/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.4077 - accuracy: 0.1411 - val_loss: 2.2315 - val_accuracy: 0.2110\n",
      "Epoch 92/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.4299 - accuracy: 0.1162 - val_loss: 2.2309 - val_accuracy: 0.2078\n",
      "Epoch 93/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.4087 - accuracy: 0.1173 - val_loss: 2.2301 - val_accuracy: 0.2045\n",
      "Epoch 94/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.3998 - accuracy: 0.1201 - val_loss: 2.2298 - val_accuracy: 0.2045\n",
      "Epoch 95/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.3701 - accuracy: 0.1318 - val_loss: 2.2299 - val_accuracy: 0.1916\n",
      "Epoch 96/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.3499 - accuracy: 0.1523 - val_loss: 2.2298 - val_accuracy: 0.1981\n",
      "Epoch 97/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3331 - accuracy: 0.1397 - val_loss: 2.2297 - val_accuracy: 0.1948\n",
      "Epoch 98/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.3632 - accuracy: 0.1396 - val_loss: 2.2293 - val_accuracy: 0.1851\n",
      "Epoch 99/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.3237 - accuracy: 0.1377 - val_loss: 2.2291 - val_accuracy: 0.1851\n",
      "Epoch 100/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3340 - accuracy: 0.1522 - val_loss: 2.2291 - val_accuracy: 0.1753\n",
      "Epoch 101/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.3520 - accuracy: 0.1328 - val_loss: 2.2291 - val_accuracy: 0.1721\n",
      "Epoch 102/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.3483 - accuracy: 0.1436 - val_loss: 2.2288 - val_accuracy: 0.1721\n",
      "Epoch 103/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.3411 - accuracy: 0.1397 - val_loss: 2.2284 - val_accuracy: 0.1753\n",
      "Epoch 104/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.3522 - accuracy: 0.1230 - val_loss: 2.2280 - val_accuracy: 0.1786\n",
      "Epoch 105/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.3097 - accuracy: 0.1522 - val_loss: 2.2270 - val_accuracy: 0.1786\n",
      "Epoch 106/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.3151 - accuracy: 0.1383 - val_loss: 2.2258 - val_accuracy: 0.1753\n",
      "Epoch 107/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.3008 - accuracy: 0.1383 - val_loss: 2.2246 - val_accuracy: 0.1786\n",
      "Epoch 108/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.3459 - accuracy: 0.1670 - val_loss: 2.2234 - val_accuracy: 0.1786\n",
      "Epoch 109/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.3234 - accuracy: 0.1425 - val_loss: 2.2220 - val_accuracy: 0.1721\n",
      "Epoch 110/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3314 - accuracy: 0.1327 - val_loss: 2.2209 - val_accuracy: 0.1753\n",
      "Epoch 111/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.3012 - accuracy: 0.1445 - val_loss: 2.2200 - val_accuracy: 0.1753\n",
      "Epoch 112/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.2940 - accuracy: 0.1611 - val_loss: 2.2197 - val_accuracy: 0.1753\n",
      "Epoch 113/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.3067 - accuracy: 0.1494 - val_loss: 2.2197 - val_accuracy: 0.1786\n",
      "Epoch 114/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.3240 - accuracy: 0.1475 - val_loss: 2.2198 - val_accuracy: 0.1721\n",
      "Epoch 115/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.3044 - accuracy: 0.1536 - val_loss: 2.2196 - val_accuracy: 0.1721\n",
      "Epoch 116/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.3216 - accuracy: 0.1426 - val_loss: 2.2196 - val_accuracy: 0.1688\n",
      "Epoch 117/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2946 - accuracy: 0.1582 - val_loss: 2.2195 - val_accuracy: 0.1623\n",
      "Epoch 118/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.3182 - accuracy: 0.1494 - val_loss: 2.2194 - val_accuracy: 0.1591\n",
      "Epoch 119/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2904 - accuracy: 0.1522 - val_loss: 2.2200 - val_accuracy: 0.1591\n",
      "Epoch 120/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.3126 - accuracy: 0.1411 - val_loss: 2.2204 - val_accuracy: 0.1623\n",
      "Epoch 121/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2908 - accuracy: 0.1425 - val_loss: 2.2208 - val_accuracy: 0.1623\n",
      "Epoch 122/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2812 - accuracy: 0.1494 - val_loss: 2.2213 - val_accuracy: 0.1591\n",
      "Epoch 123/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2891 - accuracy: 0.1611 - val_loss: 2.2217 - val_accuracy: 0.1623\n",
      "Epoch 124/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2812 - accuracy: 0.1582 - val_loss: 2.2223 - val_accuracy: 0.1591\n",
      "Epoch 125/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.2875 - accuracy: 0.1620 - val_loss: 2.2226 - val_accuracy: 0.1656\n",
      "Epoch 126/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.2675 - accuracy: 0.1480 - val_loss: 2.2232 - val_accuracy: 0.1656\n",
      "Epoch 127/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.2595 - accuracy: 0.1582 - val_loss: 2.2241 - val_accuracy: 0.1688\n",
      "Epoch 128/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2503 - accuracy: 0.1533 - val_loss: 2.2248 - val_accuracy: 0.1656\n",
      "Epoch 129/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2835 - accuracy: 0.1550 - val_loss: 2.2254 - val_accuracy: 0.1623\n",
      "Epoch 130/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2858 - accuracy: 0.1357 - val_loss: 2.2258 - val_accuracy: 0.1623\n",
      "Epoch 131/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2739 - accuracy: 0.1453 - val_loss: 2.2260 - val_accuracy: 0.1656\n",
      "Epoch 132/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2702 - accuracy: 0.1466 - val_loss: 2.2265 - val_accuracy: 0.1656\n",
      "Epoch 133/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.3002 - accuracy: 0.1676 - val_loss: 2.2271 - val_accuracy: 0.1623\n",
      "Epoch 134/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.2528 - accuracy: 0.1718 - val_loss: 2.2274 - val_accuracy: 0.1656\n",
      "Epoch 135/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2330 - accuracy: 0.1777 - val_loss: 2.2279 - val_accuracy: 0.1591\n",
      "Epoch 136/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.2760 - accuracy: 0.1670 - val_loss: 2.2280 - val_accuracy: 0.1656\n",
      "Epoch 137/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2546 - accuracy: 0.1927 - val_loss: 2.2277 - val_accuracy: 0.1721\n",
      "Epoch 138/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2613 - accuracy: 0.1718 - val_loss: 2.2272 - val_accuracy: 0.1721\n",
      "Epoch 139/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2706 - accuracy: 0.1718 - val_loss: 2.2270 - val_accuracy: 0.1786\n",
      "Epoch 140/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2356 - accuracy: 0.1983 - val_loss: 2.2266 - val_accuracy: 0.1786\n",
      "Epoch 141/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2645 - accuracy: 0.1718 - val_loss: 2.2263 - val_accuracy: 0.1721\n",
      "Epoch 142/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.2663 - accuracy: 0.1578 - val_loss: 2.2260 - val_accuracy: 0.1721\n",
      "Epoch 143/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2167 - accuracy: 0.1606 - val_loss: 2.2256 - val_accuracy: 0.1721\n",
      "Epoch 144/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2437 - accuracy: 0.1718 - val_loss: 2.2249 - val_accuracy: 0.1688\n",
      "Epoch 145/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.2428 - accuracy: 0.1855 - val_loss: 2.2244 - val_accuracy: 0.1688\n",
      "Epoch 146/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.2283 - accuracy: 0.1787 - val_loss: 2.2238 - val_accuracy: 0.1688\n",
      "Epoch 147/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2133 - accuracy: 0.1774 - val_loss: 2.2234 - val_accuracy: 0.1688\n",
      "Epoch 148/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.2701 - accuracy: 0.1572 - val_loss: 2.2229 - val_accuracy: 0.1688\n",
      "Epoch 149/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2627 - accuracy: 0.1453 - val_loss: 2.2220 - val_accuracy: 0.1656\n",
      "Epoch 150/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2380 - accuracy: 0.1899 - val_loss: 2.2210 - val_accuracy: 0.1656\n",
      "Epoch 151/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2115 - accuracy: 0.1788 - val_loss: 2.2202 - val_accuracy: 0.1656\n",
      "Epoch 152/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2306 - accuracy: 0.1758 - val_loss: 2.2194 - val_accuracy: 0.1656\n",
      "Epoch 153/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2532 - accuracy: 0.1620 - val_loss: 2.2186 - val_accuracy: 0.1656\n",
      "Epoch 154/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2219 - accuracy: 0.1738 - val_loss: 2.2178 - val_accuracy: 0.1656\n",
      "Epoch 155/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.2022 - accuracy: 0.1904 - val_loss: 2.2168 - val_accuracy: 0.1688\n",
      "Epoch 156/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.2022 - accuracy: 0.2053 - val_loss: 2.2159 - val_accuracy: 0.1688\n",
      "Epoch 157/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.2579 - accuracy: 0.1550 - val_loss: 2.2148 - val_accuracy: 0.1688\n",
      "Epoch 158/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2149 - accuracy: 0.1746 - val_loss: 2.2137 - val_accuracy: 0.1656\n",
      "Epoch 159/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1968 - accuracy: 0.1983 - val_loss: 2.2126 - val_accuracy: 0.1656\n",
      "Epoch 160/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2303 - accuracy: 0.1760 - val_loss: 2.2119 - val_accuracy: 0.1688\n",
      "Epoch 161/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.2400 - accuracy: 0.1718 - val_loss: 2.2108 - val_accuracy: 0.1688\n",
      "Epoch 162/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.2168 - accuracy: 0.2025 - val_loss: 2.2101 - val_accuracy: 0.1688\n",
      "Epoch 163/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.2140 - accuracy: 0.1758 - val_loss: 2.2098 - val_accuracy: 0.1656\n",
      "Epoch 164/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2132 - accuracy: 0.1816 - val_loss: 2.2095 - val_accuracy: 0.1656\n",
      "Epoch 165/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1809 - accuracy: 0.1895 - val_loss: 2.2088 - val_accuracy: 0.1688\n",
      "Epoch 166/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.2010 - accuracy: 0.2039 - val_loss: 2.2081 - val_accuracy: 0.1688\n",
      "Epoch 167/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.1956 - accuracy: 0.1758 - val_loss: 2.2073 - val_accuracy: 0.1721\n",
      "Epoch 168/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1959 - accuracy: 0.2080 - val_loss: 2.2070 - val_accuracy: 0.1721\n",
      "Epoch 169/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1950 - accuracy: 0.1816 - val_loss: 2.2066 - val_accuracy: 0.1721\n",
      "Epoch 170/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1816 - accuracy: 0.1634 - val_loss: 2.2061 - val_accuracy: 0.1721\n",
      "Epoch 171/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1798 - accuracy: 0.1963 - val_loss: 2.2057 - val_accuracy: 0.1721\n",
      "Epoch 172/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.1617 - accuracy: 0.2039 - val_loss: 2.2055 - val_accuracy: 0.1721\n",
      "Epoch 173/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1894 - accuracy: 0.1983 - val_loss: 2.2052 - val_accuracy: 0.1721\n",
      "Epoch 174/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1680 - accuracy: 0.1982 - val_loss: 2.2050 - val_accuracy: 0.1721\n",
      "Epoch 175/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.1786 - accuracy: 0.1802 - val_loss: 2.2041 - val_accuracy: 0.1721\n",
      "Epoch 176/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.2019 - accuracy: 0.1738 - val_loss: 2.2034 - val_accuracy: 0.1721\n",
      "Epoch 177/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1919 - accuracy: 0.1992 - val_loss: 2.2026 - val_accuracy: 0.1721\n",
      "Epoch 178/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2139 - accuracy: 0.2053 - val_loss: 2.2020 - val_accuracy: 0.1721\n",
      "Epoch 179/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2114 - accuracy: 0.1865 - val_loss: 2.2013 - val_accuracy: 0.1656\n",
      "Epoch 180/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1730 - accuracy: 0.2179 - val_loss: 2.2006 - val_accuracy: 0.1656\n",
      "Epoch 181/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2014 - accuracy: 0.1992 - val_loss: 2.2000 - val_accuracy: 0.1721\n",
      "Epoch 182/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.1690 - accuracy: 0.1875 - val_loss: 2.1996 - val_accuracy: 0.1721\n",
      "Epoch 183/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.2087 - accuracy: 0.1969 - val_loss: 2.1992 - val_accuracy: 0.1721\n",
      "Epoch 184/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1676 - accuracy: 0.2053 - val_loss: 2.1983 - val_accuracy: 0.1721\n",
      "Epoch 185/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.1885 - accuracy: 0.2041 - val_loss: 2.1969 - val_accuracy: 0.1721\n",
      "Epoch 186/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.2122 - accuracy: 0.1858 - val_loss: 2.1953 - val_accuracy: 0.1753\n",
      "Epoch 187/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.1820 - accuracy: 0.1982 - val_loss: 2.1940 - val_accuracy: 0.1786\n",
      "Epoch 188/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1766 - accuracy: 0.2011 - val_loss: 2.1923 - val_accuracy: 0.1786\n",
      "Epoch 189/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2107 - accuracy: 0.1885 - val_loss: 2.1909 - val_accuracy: 0.1786\n",
      "Epoch 190/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.2146 - accuracy: 0.1760 - val_loss: 2.1890 - val_accuracy: 0.1818\n",
      "Epoch 191/4000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.1718 - accuracy: 0.1855 - val_loss: 2.1872 - val_accuracy: 0.1818\n",
      "Epoch 192/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.1746 - accuracy: 0.1982 - val_loss: 2.1854 - val_accuracy: 0.1818\n",
      "Epoch 193/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.2023 - accuracy: 0.1777 - val_loss: 2.1829 - val_accuracy: 0.1818\n",
      "Epoch 194/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.2024 - accuracy: 0.2100 - val_loss: 2.1802 - val_accuracy: 0.1818\n",
      "Epoch 195/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.1585 - accuracy: 0.2080 - val_loss: 2.1775 - val_accuracy: 0.1851\n",
      "Epoch 196/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1401 - accuracy: 0.2217 - val_loss: 2.1745 - val_accuracy: 0.1883\n",
      "Epoch 197/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.1467 - accuracy: 0.1875 - val_loss: 2.1723 - val_accuracy: 0.1981\n",
      "Epoch 198/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1732 - accuracy: 0.2051 - val_loss: 2.1703 - val_accuracy: 0.1981\n",
      "Epoch 199/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1484 - accuracy: 0.2188 - val_loss: 2.1687 - val_accuracy: 0.2013\n",
      "Epoch 200/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.1584 - accuracy: 0.2021 - val_loss: 2.1670 - val_accuracy: 0.2013\n",
      "Epoch 201/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1931 - accuracy: 0.1797 - val_loss: 2.1653 - val_accuracy: 0.2013\n",
      "Epoch 202/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1412 - accuracy: 0.2137 - val_loss: 2.1633 - val_accuracy: 0.2013\n",
      "Epoch 203/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1323 - accuracy: 0.2095 - val_loss: 2.1615 - val_accuracy: 0.1981\n",
      "Epoch 204/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1471 - accuracy: 0.2039 - val_loss: 2.1597 - val_accuracy: 0.2013\n",
      "Epoch 205/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.1368 - accuracy: 0.1955 - val_loss: 2.1572 - val_accuracy: 0.2013\n",
      "Epoch 206/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1311 - accuracy: 0.2129 - val_loss: 2.1551 - val_accuracy: 0.2013\n",
      "Epoch 207/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.1406 - accuracy: 0.1899 - val_loss: 2.1531 - val_accuracy: 0.2013\n",
      "Epoch 208/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1558 - accuracy: 0.2053 - val_loss: 2.1514 - val_accuracy: 0.2045\n",
      "Epoch 209/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1261 - accuracy: 0.2137 - val_loss: 2.1493 - val_accuracy: 0.2045\n",
      "Epoch 210/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1715 - accuracy: 0.1885 - val_loss: 2.1473 - val_accuracy: 0.2045\n",
      "Epoch 211/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.1577 - accuracy: 0.2053 - val_loss: 2.1458 - val_accuracy: 0.2078\n",
      "Epoch 212/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1351 - accuracy: 0.2165 - val_loss: 2.1449 - val_accuracy: 0.2045\n",
      "Epoch 213/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.1383 - accuracy: 0.2178 - val_loss: 2.1439 - val_accuracy: 0.2045\n",
      "Epoch 214/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1622 - accuracy: 0.2291 - val_loss: 2.1430 - val_accuracy: 0.2078\n",
      "Epoch 215/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1480 - accuracy: 0.2285 - val_loss: 2.1421 - val_accuracy: 0.2078\n",
      "Epoch 216/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1390 - accuracy: 0.2011 - val_loss: 2.1399 - val_accuracy: 0.2078\n",
      "Epoch 217/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1444 - accuracy: 0.2053 - val_loss: 2.1376 - val_accuracy: 0.2078\n",
      "Epoch 218/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1850 - accuracy: 0.2137 - val_loss: 2.1349 - val_accuracy: 0.2110\n",
      "Epoch 219/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.1292 - accuracy: 0.2179 - val_loss: 2.1321 - val_accuracy: 0.2110\n",
      "Epoch 220/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1204 - accuracy: 0.2165 - val_loss: 2.1286 - val_accuracy: 0.2110\n",
      "Epoch 221/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.1258 - accuracy: 0.2263 - val_loss: 2.1254 - val_accuracy: 0.2143\n",
      "Epoch 222/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.1010 - accuracy: 0.2227 - val_loss: 2.1222 - val_accuracy: 0.2175\n",
      "Epoch 223/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1552 - accuracy: 0.2025 - val_loss: 2.1190 - val_accuracy: 0.2143\n",
      "Epoch 224/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1127 - accuracy: 0.2275 - val_loss: 2.1162 - val_accuracy: 0.2175\n",
      "Epoch 225/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1418 - accuracy: 0.2119 - val_loss: 2.1131 - val_accuracy: 0.2175\n",
      "Epoch 226/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0871 - accuracy: 0.2363 - val_loss: 2.1098 - val_accuracy: 0.2208\n",
      "Epoch 227/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.1232 - accuracy: 0.2179 - val_loss: 2.1059 - val_accuracy: 0.2240\n",
      "Epoch 228/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1206 - accuracy: 0.2139 - val_loss: 2.1022 - val_accuracy: 0.2338\n",
      "Epoch 229/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1185 - accuracy: 0.2197 - val_loss: 2.0987 - val_accuracy: 0.2403\n",
      "Epoch 230/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1185 - accuracy: 0.2129 - val_loss: 2.0950 - val_accuracy: 0.2435\n",
      "Epoch 231/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1206 - accuracy: 0.2324 - val_loss: 2.0918 - val_accuracy: 0.2532\n",
      "Epoch 232/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0932 - accuracy: 0.2295 - val_loss: 2.0886 - val_accuracy: 0.2565\n",
      "Epoch 233/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.0808 - accuracy: 0.2444 - val_loss: 2.0855 - val_accuracy: 0.2532\n",
      "Epoch 234/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1255 - accuracy: 0.2129 - val_loss: 2.0821 - val_accuracy: 0.2565\n",
      "Epoch 235/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1120 - accuracy: 0.2067 - val_loss: 2.0785 - val_accuracy: 0.2532\n",
      "Epoch 236/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0946 - accuracy: 0.2412 - val_loss: 2.0746 - val_accuracy: 0.2532\n",
      "Epoch 237/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.1240 - accuracy: 0.2129 - val_loss: 2.0713 - val_accuracy: 0.2565\n",
      "Epoch 238/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0851 - accuracy: 0.2472 - val_loss: 2.0675 - val_accuracy: 0.2597\n",
      "Epoch 239/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.1002 - accuracy: 0.2486 - val_loss: 2.0642 - val_accuracy: 0.2630\n",
      "Epoch 240/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.1133 - accuracy: 0.2168 - val_loss: 2.0607 - val_accuracy: 0.2630\n",
      "Epoch 241/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1023 - accuracy: 0.2304 - val_loss: 2.0572 - val_accuracy: 0.2630\n",
      "Epoch 242/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.0934 - accuracy: 0.2277 - val_loss: 2.0543 - val_accuracy: 0.2662\n",
      "Epoch 243/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.1203 - accuracy: 0.2236 - val_loss: 2.0513 - val_accuracy: 0.2727\n",
      "Epoch 244/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.1033 - accuracy: 0.2430 - val_loss: 2.0483 - val_accuracy: 0.2727\n",
      "Epoch 245/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.1030 - accuracy: 0.2275 - val_loss: 2.0453 - val_accuracy: 0.2727\n",
      "Epoch 246/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0593 - accuracy: 0.2486 - val_loss: 2.0426 - val_accuracy: 0.2727\n",
      "Epoch 247/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.0693 - accuracy: 0.2514 - val_loss: 2.0396 - val_accuracy: 0.2727\n",
      "Epoch 248/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.1035 - accuracy: 0.2246 - val_loss: 2.0363 - val_accuracy: 0.2792\n",
      "Epoch 249/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0828 - accuracy: 0.2402 - val_loss: 2.0325 - val_accuracy: 0.2825\n",
      "Epoch 250/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.1161 - accuracy: 0.2165 - val_loss: 2.0288 - val_accuracy: 0.2792\n",
      "Epoch 251/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0988 - accuracy: 0.2263 - val_loss: 2.0252 - val_accuracy: 0.2825\n",
      "Epoch 252/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.1203 - accuracy: 0.2221 - val_loss: 2.0225 - val_accuracy: 0.2857\n",
      "Epoch 253/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0827 - accuracy: 0.2458 - val_loss: 2.0201 - val_accuracy: 0.2890\n",
      "Epoch 254/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0977 - accuracy: 0.2277 - val_loss: 2.0172 - val_accuracy: 0.2857\n",
      "Epoch 255/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0797 - accuracy: 0.2556 - val_loss: 2.0153 - val_accuracy: 0.2857\n",
      "Epoch 256/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0725 - accuracy: 0.2193 - val_loss: 2.0129 - val_accuracy: 0.2857\n",
      "Epoch 257/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0562 - accuracy: 0.2332 - val_loss: 2.0105 - val_accuracy: 0.2890\n",
      "Epoch 258/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0791 - accuracy: 0.2500 - val_loss: 2.0086 - val_accuracy: 0.2890\n",
      "Epoch 259/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0801 - accuracy: 0.2598 - val_loss: 2.0059 - val_accuracy: 0.2922\n",
      "Epoch 260/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.1156 - accuracy: 0.2119 - val_loss: 2.0036 - val_accuracy: 0.2955\n",
      "Epoch 261/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0819 - accuracy: 0.2383 - val_loss: 2.0012 - val_accuracy: 0.3052\n",
      "Epoch 262/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0728 - accuracy: 0.2461 - val_loss: 1.9988 - val_accuracy: 0.3052\n",
      "Epoch 263/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0481 - accuracy: 0.2388 - val_loss: 1.9970 - val_accuracy: 0.3084\n",
      "Epoch 264/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.0779 - accuracy: 0.2314 - val_loss: 1.9959 - val_accuracy: 0.3052\n",
      "Epoch 265/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0924 - accuracy: 0.2318 - val_loss: 1.9948 - val_accuracy: 0.3084\n",
      "Epoch 266/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0777 - accuracy: 0.2458 - val_loss: 1.9940 - val_accuracy: 0.3084\n",
      "Epoch 267/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.0806 - accuracy: 0.2373 - val_loss: 1.9937 - val_accuracy: 0.3052\n",
      "Epoch 268/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0797 - accuracy: 0.2588 - val_loss: 1.9935 - val_accuracy: 0.3052\n",
      "Epoch 269/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0805 - accuracy: 0.2412 - val_loss: 1.9931 - val_accuracy: 0.3052\n",
      "Epoch 270/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0862 - accuracy: 0.2346 - val_loss: 1.9932 - val_accuracy: 0.3084\n",
      "Epoch 271/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0440 - accuracy: 0.2422 - val_loss: 1.9935 - val_accuracy: 0.3084\n",
      "Epoch 272/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0698 - accuracy: 0.2668 - val_loss: 1.9933 - val_accuracy: 0.3052\n",
      "Epoch 273/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0571 - accuracy: 0.2346 - val_loss: 1.9933 - val_accuracy: 0.2987\n",
      "Epoch 274/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0629 - accuracy: 0.2373 - val_loss: 1.9932 - val_accuracy: 0.2987\n",
      "Epoch 275/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.0600 - accuracy: 0.2451 - val_loss: 1.9924 - val_accuracy: 0.2987\n",
      "Epoch 276/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0086 - accuracy: 0.2654 - val_loss: 1.9911 - val_accuracy: 0.3019\n",
      "Epoch 277/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0556 - accuracy: 0.2556 - val_loss: 1.9896 - val_accuracy: 0.2987\n",
      "Epoch 278/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0520 - accuracy: 0.2471 - val_loss: 1.9873 - val_accuracy: 0.3019\n",
      "Epoch 279/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0358 - accuracy: 0.2626 - val_loss: 1.9848 - val_accuracy: 0.3084\n",
      "Epoch 280/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0671 - accuracy: 0.2480 - val_loss: 1.9824 - val_accuracy: 0.3084\n",
      "Epoch 281/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0628 - accuracy: 0.2542 - val_loss: 1.9794 - val_accuracy: 0.3084\n",
      "Epoch 282/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0584 - accuracy: 0.2373 - val_loss: 1.9767 - val_accuracy: 0.3084\n",
      "Epoch 283/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0437 - accuracy: 0.2416 - val_loss: 1.9743 - val_accuracy: 0.3084\n",
      "Epoch 284/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0222 - accuracy: 0.2549 - val_loss: 1.9717 - val_accuracy: 0.3084\n",
      "Epoch 285/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0259 - accuracy: 0.2578 - val_loss: 1.9696 - val_accuracy: 0.3117\n",
      "Epoch 286/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0739 - accuracy: 0.2291 - val_loss: 1.9677 - val_accuracy: 0.3117\n",
      "Epoch 287/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0455 - accuracy: 0.2334 - val_loss: 1.9660 - val_accuracy: 0.3149\n",
      "Epoch 288/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.0462 - accuracy: 0.2461 - val_loss: 1.9639 - val_accuracy: 0.3149\n",
      "Epoch 289/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0128 - accuracy: 0.2640 - val_loss: 1.9619 - val_accuracy: 0.3149\n",
      "Epoch 290/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.0888 - accuracy: 0.2275 - val_loss: 1.9600 - val_accuracy: 0.3149\n",
      "Epoch 291/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0489 - accuracy: 0.2612 - val_loss: 1.9577 - val_accuracy: 0.3214\n",
      "Epoch 292/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0396 - accuracy: 0.2263 - val_loss: 1.9554 - val_accuracy: 0.3279\n",
      "Epoch 293/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0554 - accuracy: 0.2432 - val_loss: 1.9534 - val_accuracy: 0.3247\n",
      "Epoch 294/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0396 - accuracy: 0.2451 - val_loss: 1.9514 - val_accuracy: 0.3247\n",
      "Epoch 295/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0494 - accuracy: 0.2151 - val_loss: 1.9489 - val_accuracy: 0.3247\n",
      "Epoch 296/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0098 - accuracy: 0.2500 - val_loss: 1.9462 - val_accuracy: 0.3214\n",
      "Epoch 297/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0277 - accuracy: 0.2588 - val_loss: 1.9442 - val_accuracy: 0.3214\n",
      "Epoch 298/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0255 - accuracy: 0.2568 - val_loss: 1.9419 - val_accuracy: 0.3247\n",
      "Epoch 299/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.0248 - accuracy: 0.2627 - val_loss: 1.9392 - val_accuracy: 0.3214\n",
      "Epoch 300/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9828 - accuracy: 0.2835 - val_loss: 1.9362 - val_accuracy: 0.3247\n",
      "Epoch 301/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0449 - accuracy: 0.2490 - val_loss: 1.9333 - val_accuracy: 0.3279\n",
      "Epoch 302/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0045 - accuracy: 0.2626 - val_loss: 1.9300 - val_accuracy: 0.3312\n",
      "Epoch 303/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0000 - accuracy: 0.2709 - val_loss: 1.9270 - val_accuracy: 0.3312\n",
      "Epoch 304/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0594 - accuracy: 0.2451 - val_loss: 1.9240 - val_accuracy: 0.3377\n",
      "Epoch 305/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0447 - accuracy: 0.2451 - val_loss: 1.9209 - val_accuracy: 0.3312\n",
      "Epoch 306/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0293 - accuracy: 0.2570 - val_loss: 1.9182 - val_accuracy: 0.3377\n",
      "Epoch 307/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0244 - accuracy: 0.2607 - val_loss: 1.9156 - val_accuracy: 0.3377\n",
      "Epoch 308/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0381 - accuracy: 0.2626 - val_loss: 1.9125 - val_accuracy: 0.3344\n",
      "Epoch 309/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.0239 - accuracy: 0.2444 - val_loss: 1.9098 - val_accuracy: 0.3279\n",
      "Epoch 310/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0252 - accuracy: 0.2598 - val_loss: 1.9074 - val_accuracy: 0.3312\n",
      "Epoch 311/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0314 - accuracy: 0.2598 - val_loss: 1.9052 - val_accuracy: 0.3344\n",
      "Epoch 312/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0347 - accuracy: 0.2480 - val_loss: 1.9032 - val_accuracy: 0.3377\n",
      "Epoch 313/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0213 - accuracy: 0.2500 - val_loss: 1.9013 - val_accuracy: 0.3377\n",
      "Epoch 314/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0423 - accuracy: 0.2416 - val_loss: 1.8996 - val_accuracy: 0.3377\n",
      "Epoch 315/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0011 - accuracy: 0.2705 - val_loss: 1.8977 - val_accuracy: 0.3409\n",
      "Epoch 316/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9923 - accuracy: 0.2835 - val_loss: 1.8956 - val_accuracy: 0.3409\n",
      "Epoch 317/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0386 - accuracy: 0.2676 - val_loss: 1.8937 - val_accuracy: 0.3442\n",
      "Epoch 318/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9954 - accuracy: 0.2696 - val_loss: 1.8925 - val_accuracy: 0.3409\n",
      "Epoch 319/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0152 - accuracy: 0.2542 - val_loss: 1.8908 - val_accuracy: 0.3409\n",
      "Epoch 320/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0153 - accuracy: 0.2812 - val_loss: 1.8889 - val_accuracy: 0.3409\n",
      "Epoch 321/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0059 - accuracy: 0.2500 - val_loss: 1.8870 - val_accuracy: 0.3442\n",
      "Epoch 322/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9839 - accuracy: 0.2773 - val_loss: 1.8851 - val_accuracy: 0.3474\n",
      "Epoch 323/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9993 - accuracy: 0.2654 - val_loss: 1.8835 - val_accuracy: 0.3474\n",
      "Epoch 324/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9535 - accuracy: 0.2696 - val_loss: 1.8817 - val_accuracy: 0.3474\n",
      "Epoch 325/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.9963 - accuracy: 0.2646 - val_loss: 1.8803 - val_accuracy: 0.3539\n",
      "Epoch 326/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9773 - accuracy: 0.2765 - val_loss: 1.8785 - val_accuracy: 0.3539\n",
      "Epoch 327/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9544 - accuracy: 0.2765 - val_loss: 1.8763 - val_accuracy: 0.3539\n",
      "Epoch 328/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0005 - accuracy: 0.2852 - val_loss: 1.8741 - val_accuracy: 0.3506\n",
      "Epoch 329/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9589 - accuracy: 0.2765 - val_loss: 1.8719 - val_accuracy: 0.3474\n",
      "Epoch 330/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.9891 - accuracy: 0.2812 - val_loss: 1.8698 - val_accuracy: 0.3442\n",
      "Epoch 331/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.9549 - accuracy: 0.2835 - val_loss: 1.8680 - val_accuracy: 0.3442\n",
      "Epoch 332/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9890 - accuracy: 0.2793 - val_loss: 1.8667 - val_accuracy: 0.3442\n",
      "Epoch 333/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0520 - accuracy: 0.2640 - val_loss: 1.8654 - val_accuracy: 0.3474\n",
      "Epoch 334/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0096 - accuracy: 0.2668 - val_loss: 1.8640 - val_accuracy: 0.3571\n",
      "Epoch 335/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0059 - accuracy: 0.2709 - val_loss: 1.8626 - val_accuracy: 0.3604\n",
      "Epoch 336/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9571 - accuracy: 0.2910 - val_loss: 1.8614 - val_accuracy: 0.3669\n",
      "Epoch 337/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9605 - accuracy: 0.2863 - val_loss: 1.8607 - val_accuracy: 0.3669\n",
      "Epoch 338/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9971 - accuracy: 0.2754 - val_loss: 1.8600 - val_accuracy: 0.3669\n",
      "Epoch 339/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9862 - accuracy: 0.2715 - val_loss: 1.8593 - val_accuracy: 0.3669\n",
      "Epoch 340/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9800 - accuracy: 0.2588 - val_loss: 1.8584 - val_accuracy: 0.3669\n",
      "Epoch 341/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9975 - accuracy: 0.2764 - val_loss: 1.8575 - val_accuracy: 0.3701\n",
      "Epoch 342/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9911 - accuracy: 0.2779 - val_loss: 1.8572 - val_accuracy: 0.3669\n",
      "Epoch 343/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9413 - accuracy: 0.2793 - val_loss: 1.8574 - val_accuracy: 0.3701\n",
      "Epoch 344/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9650 - accuracy: 0.2920 - val_loss: 1.8581 - val_accuracy: 0.3701\n",
      "Epoch 345/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9999 - accuracy: 0.2891 - val_loss: 1.8589 - val_accuracy: 0.3539\n",
      "Epoch 346/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9990 - accuracy: 0.2773 - val_loss: 1.8587 - val_accuracy: 0.3539\n",
      "Epoch 347/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9737 - accuracy: 0.2705 - val_loss: 1.8586 - val_accuracy: 0.3506\n",
      "Epoch 348/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9434 - accuracy: 0.2807 - val_loss: 1.8587 - val_accuracy: 0.3474\n",
      "Epoch 349/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9447 - accuracy: 0.2947 - val_loss: 1.8589 - val_accuracy: 0.3506\n",
      "Epoch 350/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9582 - accuracy: 0.3037 - val_loss: 1.8585 - val_accuracy: 0.3506\n",
      "Epoch 351/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9674 - accuracy: 0.2988 - val_loss: 1.8582 - val_accuracy: 0.3539\n",
      "Epoch 352/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9510 - accuracy: 0.2891 - val_loss: 1.8579 - val_accuracy: 0.3506\n",
      "Epoch 353/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9726 - accuracy: 0.2686 - val_loss: 1.8577 - val_accuracy: 0.3506\n",
      "Epoch 354/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9484 - accuracy: 0.2877 - val_loss: 1.8561 - val_accuracy: 0.3474\n",
      "Epoch 355/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.9809 - accuracy: 0.2751 - val_loss: 1.8544 - val_accuracy: 0.3474\n",
      "Epoch 356/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9939 - accuracy: 0.2695 - val_loss: 1.8524 - val_accuracy: 0.3539\n",
      "Epoch 357/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9397 - accuracy: 0.2821 - val_loss: 1.8506 - val_accuracy: 0.3571\n",
      "Epoch 358/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9535 - accuracy: 0.2975 - val_loss: 1.8487 - val_accuracy: 0.3571\n",
      "Epoch 359/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9550 - accuracy: 0.2861 - val_loss: 1.8468 - val_accuracy: 0.3539\n",
      "Epoch 360/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9286 - accuracy: 0.2959 - val_loss: 1.8452 - val_accuracy: 0.3539\n",
      "Epoch 361/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9754 - accuracy: 0.2737 - val_loss: 1.8439 - val_accuracy: 0.3539\n",
      "Epoch 362/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9472 - accuracy: 0.2930 - val_loss: 1.8429 - val_accuracy: 0.3506\n",
      "Epoch 363/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.9797 - accuracy: 0.2822 - val_loss: 1.8422 - val_accuracy: 0.3539\n",
      "Epoch 364/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9927 - accuracy: 0.2696 - val_loss: 1.8425 - val_accuracy: 0.3539\n",
      "Epoch 365/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.9455 - accuracy: 0.2877 - val_loss: 1.8420 - val_accuracy: 0.3442\n",
      "Epoch 366/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9382 - accuracy: 0.3047 - val_loss: 1.8423 - val_accuracy: 0.3506\n",
      "Epoch 367/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9277 - accuracy: 0.2877 - val_loss: 1.8427 - val_accuracy: 0.3474\n",
      "Epoch 368/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9952 - accuracy: 0.2961 - val_loss: 1.8434 - val_accuracy: 0.3506\n",
      "Epoch 369/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9678 - accuracy: 0.3073 - val_loss: 1.8433 - val_accuracy: 0.3506\n",
      "Epoch 370/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9726 - accuracy: 0.2849 - val_loss: 1.8433 - val_accuracy: 0.3474\n",
      "Epoch 371/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9246 - accuracy: 0.2812 - val_loss: 1.8429 - val_accuracy: 0.3474\n",
      "Epoch 372/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9261 - accuracy: 0.2919 - val_loss: 1.8427 - val_accuracy: 0.3506\n",
      "Epoch 373/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9329 - accuracy: 0.3212 - val_loss: 1.8423 - val_accuracy: 0.3506\n",
      "Epoch 374/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8883 - accuracy: 0.3142 - val_loss: 1.8417 - val_accuracy: 0.3571\n",
      "Epoch 375/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9271 - accuracy: 0.2822 - val_loss: 1.8421 - val_accuracy: 0.3539\n",
      "Epoch 376/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9685 - accuracy: 0.2773 - val_loss: 1.8424 - val_accuracy: 0.3474\n",
      "Epoch 377/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9721 - accuracy: 0.2695 - val_loss: 1.8417 - val_accuracy: 0.3377\n",
      "Epoch 378/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.9083 - accuracy: 0.3008 - val_loss: 1.8408 - val_accuracy: 0.3442\n",
      "Epoch 379/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9955 - accuracy: 0.2490 - val_loss: 1.8402 - val_accuracy: 0.3474\n",
      "Epoch 380/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.9398 - accuracy: 0.2910 - val_loss: 1.8398 - val_accuracy: 0.3539\n",
      "Epoch 381/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9353 - accuracy: 0.2598 - val_loss: 1.8387 - val_accuracy: 0.3442\n",
      "Epoch 382/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9358 - accuracy: 0.2849 - val_loss: 1.8369 - val_accuracy: 0.3474\n",
      "Epoch 383/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9614 - accuracy: 0.2988 - val_loss: 1.8341 - val_accuracy: 0.3604\n",
      "Epoch 384/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9227 - accuracy: 0.3125 - val_loss: 1.8326 - val_accuracy: 0.3636\n",
      "Epoch 385/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9161 - accuracy: 0.2947 - val_loss: 1.8311 - val_accuracy: 0.3604\n",
      "Epoch 386/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9448 - accuracy: 0.2919 - val_loss: 1.8293 - val_accuracy: 0.3571\n",
      "Epoch 387/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9263 - accuracy: 0.3128 - val_loss: 1.8270 - val_accuracy: 0.3571\n",
      "Epoch 388/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9316 - accuracy: 0.2919 - val_loss: 1.8235 - val_accuracy: 0.3539\n",
      "Epoch 389/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9325 - accuracy: 0.2765 - val_loss: 1.8206 - val_accuracy: 0.3506\n",
      "Epoch 390/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9410 - accuracy: 0.3003 - val_loss: 1.8192 - val_accuracy: 0.3506\n",
      "Epoch 391/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9161 - accuracy: 0.3115 - val_loss: 1.8181 - val_accuracy: 0.3474\n",
      "Epoch 392/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9065 - accuracy: 0.3073 - val_loss: 1.8180 - val_accuracy: 0.3442\n",
      "Epoch 393/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9596 - accuracy: 0.2861 - val_loss: 1.8176 - val_accuracy: 0.3474\n",
      "Epoch 394/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9255 - accuracy: 0.3031 - val_loss: 1.8173 - val_accuracy: 0.3442\n",
      "Epoch 395/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8801 - accuracy: 0.3268 - val_loss: 1.8183 - val_accuracy: 0.3474\n",
      "Epoch 396/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9085 - accuracy: 0.2807 - val_loss: 1.8201 - val_accuracy: 0.3474\n",
      "Epoch 397/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9141 - accuracy: 0.3057 - val_loss: 1.8218 - val_accuracy: 0.3442\n",
      "Epoch 398/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8977 - accuracy: 0.3174 - val_loss: 1.8225 - val_accuracy: 0.3442\n",
      "Epoch 399/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8875 - accuracy: 0.3096 - val_loss: 1.8223 - val_accuracy: 0.3377\n",
      "Epoch 400/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9010 - accuracy: 0.2989 - val_loss: 1.8206 - val_accuracy: 0.3377\n",
      "Epoch 401/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9063 - accuracy: 0.3076 - val_loss: 1.8186 - val_accuracy: 0.3377\n",
      "Epoch 402/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9480 - accuracy: 0.3008 - val_loss: 1.8158 - val_accuracy: 0.3344\n",
      "Epoch 403/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9268 - accuracy: 0.2793 - val_loss: 1.8129 - val_accuracy: 0.3377\n",
      "Epoch 404/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.8918 - accuracy: 0.3252 - val_loss: 1.8107 - val_accuracy: 0.3344\n",
      "Epoch 405/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9472 - accuracy: 0.2696 - val_loss: 1.8092 - val_accuracy: 0.3312\n",
      "Epoch 406/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.8684 - accuracy: 0.3213 - val_loss: 1.8075 - val_accuracy: 0.3344\n",
      "Epoch 407/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9073 - accuracy: 0.2891 - val_loss: 1.8057 - val_accuracy: 0.3344\n",
      "Epoch 408/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8962 - accuracy: 0.2919 - val_loss: 1.8036 - val_accuracy: 0.3409\n",
      "Epoch 409/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.9189 - accuracy: 0.3128 - val_loss: 1.8019 - val_accuracy: 0.3409\n",
      "Epoch 410/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8658 - accuracy: 0.3296 - val_loss: 1.8021 - val_accuracy: 0.3442\n",
      "Epoch 411/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9183 - accuracy: 0.3066 - val_loss: 1.8034 - val_accuracy: 0.3442\n",
      "Epoch 412/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8913 - accuracy: 0.3203 - val_loss: 1.8046 - val_accuracy: 0.3442\n",
      "Epoch 413/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8446 - accuracy: 0.3422 - val_loss: 1.8044 - val_accuracy: 0.3409\n",
      "Epoch 414/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9138 - accuracy: 0.3115 - val_loss: 1.8053 - val_accuracy: 0.3474\n",
      "Epoch 415/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8883 - accuracy: 0.3254 - val_loss: 1.8054 - val_accuracy: 0.3506\n",
      "Epoch 416/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8969 - accuracy: 0.3018 - val_loss: 1.8058 - val_accuracy: 0.3539\n",
      "Epoch 417/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8922 - accuracy: 0.3174 - val_loss: 1.8056 - val_accuracy: 0.3571\n",
      "Epoch 418/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9302 - accuracy: 0.2979 - val_loss: 1.8052 - val_accuracy: 0.3571\n",
      "Epoch 419/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9076 - accuracy: 0.3045 - val_loss: 1.8053 - val_accuracy: 0.3604\n",
      "Epoch 420/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8810 - accuracy: 0.3115 - val_loss: 1.8071 - val_accuracy: 0.3474\n",
      "Epoch 421/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8740 - accuracy: 0.3154 - val_loss: 1.8124 - val_accuracy: 0.3409\n",
      "Epoch 422/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8824 - accuracy: 0.3066 - val_loss: 1.8188 - val_accuracy: 0.3312\n",
      "Epoch 423/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8842 - accuracy: 0.3184 - val_loss: 1.8254 - val_accuracy: 0.3409\n",
      "Epoch 424/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8933 - accuracy: 0.3128 - val_loss: 1.8320 - val_accuracy: 0.3442\n",
      "Epoch 425/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8834 - accuracy: 0.2863 - val_loss: 1.8393 - val_accuracy: 0.3377\n",
      "Epoch 426/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8840 - accuracy: 0.3296 - val_loss: 1.8454 - val_accuracy: 0.3409\n",
      "Epoch 427/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8881 - accuracy: 0.2975 - val_loss: 1.8513 - val_accuracy: 0.3377\n",
      "Epoch 428/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8968 - accuracy: 0.3254 - val_loss: 1.8560 - val_accuracy: 0.3344\n",
      "Epoch 429/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8623 - accuracy: 0.3240 - val_loss: 1.8570 - val_accuracy: 0.3344\n",
      "Epoch 430/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8365 - accuracy: 0.3394 - val_loss: 1.8578 - val_accuracy: 0.3344\n",
      "Epoch 431/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8770 - accuracy: 0.3017 - val_loss: 1.8606 - val_accuracy: 0.3442\n",
      "Epoch 432/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8551 - accuracy: 0.3223 - val_loss: 1.8626 - val_accuracy: 0.3409\n",
      "Epoch 433/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9068 - accuracy: 0.2975 - val_loss: 1.8629 - val_accuracy: 0.3377\n",
      "Epoch 434/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8600 - accuracy: 0.2961 - val_loss: 1.8640 - val_accuracy: 0.3442\n",
      "Epoch 435/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8897 - accuracy: 0.3045 - val_loss: 1.8639 - val_accuracy: 0.3409\n",
      "Epoch 436/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8355 - accuracy: 0.3226 - val_loss: 1.8623 - val_accuracy: 0.3506\n",
      "Epoch 437/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8581 - accuracy: 0.3242 - val_loss: 1.8601 - val_accuracy: 0.3506\n",
      "Epoch 438/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8511 - accuracy: 0.3492 - val_loss: 1.8577 - val_accuracy: 0.3506\n",
      "Epoch 439/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8709 - accuracy: 0.3268 - val_loss: 1.8535 - val_accuracy: 0.3539\n",
      "Epoch 440/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8920 - accuracy: 0.3340 - val_loss: 1.8518 - val_accuracy: 0.3571\n",
      "Epoch 441/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.8658 - accuracy: 0.3125 - val_loss: 1.8503 - val_accuracy: 0.3474\n",
      "Epoch 442/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8687 - accuracy: 0.3184 - val_loss: 1.8494 - val_accuracy: 0.3442\n",
      "Epoch 443/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8975 - accuracy: 0.3156 - val_loss: 1.8475 - val_accuracy: 0.3442\n",
      "Epoch 444/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8763 - accuracy: 0.3184 - val_loss: 1.8466 - val_accuracy: 0.3442\n",
      "Epoch 445/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8470 - accuracy: 0.3311 - val_loss: 1.8444 - val_accuracy: 0.3409\n",
      "Epoch 446/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8536 - accuracy: 0.3320 - val_loss: 1.8414 - val_accuracy: 0.3409\n",
      "Epoch 447/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8940 - accuracy: 0.3057 - val_loss: 1.8354 - val_accuracy: 0.3377\n",
      "Epoch 448/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8440 - accuracy: 0.3436 - val_loss: 1.8288 - val_accuracy: 0.3474\n",
      "Epoch 449/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.9002 - accuracy: 0.3008 - val_loss: 1.8236 - val_accuracy: 0.3506\n",
      "Epoch 450/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7917 - accuracy: 0.3447 - val_loss: 1.8187 - val_accuracy: 0.3539\n",
      "Epoch 451/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8829 - accuracy: 0.3226 - val_loss: 1.8188 - val_accuracy: 0.3604\n",
      "Epoch 452/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8578 - accuracy: 0.3324 - val_loss: 1.8170 - val_accuracy: 0.3636\n",
      "Epoch 453/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8481 - accuracy: 0.3350 - val_loss: 1.8149 - val_accuracy: 0.3604\n",
      "Epoch 454/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8577 - accuracy: 0.3203 - val_loss: 1.8114 - val_accuracy: 0.3604\n",
      "Epoch 455/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8583 - accuracy: 0.3589 - val_loss: 1.8093 - val_accuracy: 0.3669\n",
      "Epoch 456/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8264 - accuracy: 0.3296 - val_loss: 1.8053 - val_accuracy: 0.3604\n",
      "Epoch 457/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8083 - accuracy: 0.3359 - val_loss: 1.8047 - val_accuracy: 0.3636\n",
      "Epoch 458/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8524 - accuracy: 0.3394 - val_loss: 1.8032 - val_accuracy: 0.3604\n",
      "Epoch 459/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8776 - accuracy: 0.3212 - val_loss: 1.8046 - val_accuracy: 0.3604\n",
      "Epoch 460/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8629 - accuracy: 0.3301 - val_loss: 1.8085 - val_accuracy: 0.3604\n",
      "Epoch 461/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8264 - accuracy: 0.3379 - val_loss: 1.8104 - val_accuracy: 0.3571\n",
      "Epoch 462/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8559 - accuracy: 0.3073 - val_loss: 1.8134 - val_accuracy: 0.3604\n",
      "Epoch 463/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8730 - accuracy: 0.3301 - val_loss: 1.8128 - val_accuracy: 0.3669\n",
      "Epoch 464/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8597 - accuracy: 0.3115 - val_loss: 1.8094 - val_accuracy: 0.3734\n",
      "Epoch 465/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.8641 - accuracy: 0.3232 - val_loss: 1.8037 - val_accuracy: 0.3701\n",
      "Epoch 466/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.8560 - accuracy: 0.3310 - val_loss: 1.7978 - val_accuracy: 0.3734\n",
      "Epoch 467/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8111 - accuracy: 0.3389 - val_loss: 1.7955 - val_accuracy: 0.3734\n",
      "Epoch 468/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.8004 - accuracy: 0.3643 - val_loss: 1.7922 - val_accuracy: 0.3734\n",
      "Epoch 469/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8318 - accuracy: 0.3366 - val_loss: 1.7886 - val_accuracy: 0.3734\n",
      "Epoch 470/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8958 - accuracy: 0.3142 - val_loss: 1.7882 - val_accuracy: 0.3799\n",
      "Epoch 471/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8146 - accuracy: 0.3535 - val_loss: 1.7869 - val_accuracy: 0.3766\n",
      "Epoch 472/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8420 - accuracy: 0.3359 - val_loss: 1.7819 - val_accuracy: 0.3831\n",
      "Epoch 473/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8736 - accuracy: 0.3223 - val_loss: 1.7812 - val_accuracy: 0.3831\n",
      "Epoch 474/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8617 - accuracy: 0.3154 - val_loss: 1.7799 - val_accuracy: 0.3896\n",
      "Epoch 475/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8009 - accuracy: 0.3525 - val_loss: 1.7772 - val_accuracy: 0.3864\n",
      "Epoch 476/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7664 - accuracy: 0.3855 - val_loss: 1.7766 - val_accuracy: 0.3831\n",
      "Epoch 477/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8544 - accuracy: 0.3418 - val_loss: 1.7804 - val_accuracy: 0.3831\n",
      "Epoch 478/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8332 - accuracy: 0.3394 - val_loss: 1.7867 - val_accuracy: 0.3734\n",
      "Epoch 479/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8566 - accuracy: 0.3164 - val_loss: 1.7923 - val_accuracy: 0.3669\n",
      "Epoch 480/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7948 - accuracy: 0.3506 - val_loss: 1.7968 - val_accuracy: 0.3669\n",
      "Epoch 481/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.8510 - accuracy: 0.3324 - val_loss: 1.7996 - val_accuracy: 0.3636\n",
      "Epoch 482/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8282 - accuracy: 0.3226 - val_loss: 1.7990 - val_accuracy: 0.3539\n",
      "Epoch 483/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8268 - accuracy: 0.3340 - val_loss: 1.7980 - val_accuracy: 0.3506\n",
      "Epoch 484/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.8206 - accuracy: 0.3380 - val_loss: 1.7956 - val_accuracy: 0.3539\n",
      "Epoch 485/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8415 - accuracy: 0.3226 - val_loss: 1.7881 - val_accuracy: 0.3506\n",
      "Epoch 486/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8145 - accuracy: 0.3398 - val_loss: 1.7828 - val_accuracy: 0.3539\n",
      "Epoch 487/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8073 - accuracy: 0.3450 - val_loss: 1.7773 - val_accuracy: 0.3539\n",
      "Epoch 488/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.8636 - accuracy: 0.3296 - val_loss: 1.7735 - val_accuracy: 0.3604\n",
      "Epoch 489/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8207 - accuracy: 0.3478 - val_loss: 1.7697 - val_accuracy: 0.3701\n",
      "Epoch 490/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.7944 - accuracy: 0.3352 - val_loss: 1.7680 - val_accuracy: 0.3734\n",
      "Epoch 491/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8106 - accuracy: 0.3545 - val_loss: 1.7699 - val_accuracy: 0.3636\n",
      "Epoch 492/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7820 - accuracy: 0.3645 - val_loss: 1.7717 - val_accuracy: 0.3636\n",
      "Epoch 493/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7965 - accuracy: 0.3545 - val_loss: 1.7724 - val_accuracy: 0.3669\n",
      "Epoch 494/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8038 - accuracy: 0.3545 - val_loss: 1.7743 - val_accuracy: 0.3604\n",
      "Epoch 495/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.8015 - accuracy: 0.3311 - val_loss: 1.7737 - val_accuracy: 0.3669\n",
      "Epoch 496/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.8159 - accuracy: 0.3645 - val_loss: 1.7705 - val_accuracy: 0.3701\n",
      "Epoch 497/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7747 - accuracy: 0.3506 - val_loss: 1.7701 - val_accuracy: 0.3701\n",
      "Epoch 498/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7997 - accuracy: 0.3366 - val_loss: 1.7712 - val_accuracy: 0.3766\n",
      "Epoch 499/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8252 - accuracy: 0.3408 - val_loss: 1.7723 - val_accuracy: 0.3766\n",
      "Epoch 500/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7924 - accuracy: 0.3603 - val_loss: 1.7711 - val_accuracy: 0.3701\n",
      "Epoch 501/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7998 - accuracy: 0.3623 - val_loss: 1.7697 - val_accuracy: 0.3701\n",
      "Epoch 502/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7461 - accuracy: 0.3673 - val_loss: 1.7696 - val_accuracy: 0.3766\n",
      "Epoch 503/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8202 - accuracy: 0.3428 - val_loss: 1.7699 - val_accuracy: 0.3799\n",
      "Epoch 504/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7757 - accuracy: 0.3575 - val_loss: 1.7747 - val_accuracy: 0.3799\n",
      "Epoch 505/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7889 - accuracy: 0.3438 - val_loss: 1.7793 - val_accuracy: 0.3929\n",
      "Epoch 506/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8505 - accuracy: 0.3457 - val_loss: 1.7849 - val_accuracy: 0.3929\n",
      "Epoch 507/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7454 - accuracy: 0.3613 - val_loss: 1.7914 - val_accuracy: 0.3864\n",
      "Epoch 508/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7862 - accuracy: 0.3659 - val_loss: 1.7917 - val_accuracy: 0.3766\n",
      "Epoch 509/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7821 - accuracy: 0.3574 - val_loss: 1.7896 - val_accuracy: 0.3799\n",
      "Epoch 510/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7747 - accuracy: 0.3827 - val_loss: 1.7916 - val_accuracy: 0.3831\n",
      "Epoch 511/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7877 - accuracy: 0.3394 - val_loss: 1.7981 - val_accuracy: 0.3831\n",
      "Epoch 512/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7930 - accuracy: 0.3301 - val_loss: 1.7987 - val_accuracy: 0.3864\n",
      "Epoch 513/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7948 - accuracy: 0.3589 - val_loss: 1.7977 - val_accuracy: 0.3799\n",
      "Epoch 514/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7508 - accuracy: 0.3841 - val_loss: 1.7888 - val_accuracy: 0.3799\n",
      "Epoch 515/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8231 - accuracy: 0.3633 - val_loss: 1.7782 - val_accuracy: 0.3799\n",
      "Epoch 516/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7667 - accuracy: 0.3613 - val_loss: 1.7692 - val_accuracy: 0.3864\n",
      "Epoch 517/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7579 - accuracy: 0.3662 - val_loss: 1.7585 - val_accuracy: 0.3929\n",
      "Epoch 518/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.7807 - accuracy: 0.3877 - val_loss: 1.7494 - val_accuracy: 0.3994\n",
      "Epoch 519/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8183 - accuracy: 0.3575 - val_loss: 1.7454 - val_accuracy: 0.3961\n",
      "Epoch 520/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7888 - accuracy: 0.3506 - val_loss: 1.7421 - val_accuracy: 0.4026\n",
      "Epoch 521/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7857 - accuracy: 0.3545 - val_loss: 1.7419 - val_accuracy: 0.4058\n",
      "Epoch 522/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7937 - accuracy: 0.3564 - val_loss: 1.7420 - val_accuracy: 0.4058\n",
      "Epoch 523/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7209 - accuracy: 0.3760 - val_loss: 1.7473 - val_accuracy: 0.4058\n",
      "Epoch 524/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7500 - accuracy: 0.3715 - val_loss: 1.7538 - val_accuracy: 0.4026\n",
      "Epoch 525/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7649 - accuracy: 0.3623 - val_loss: 1.7567 - val_accuracy: 0.3929\n",
      "Epoch 526/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7615 - accuracy: 0.3701 - val_loss: 1.7585 - val_accuracy: 0.3961\n",
      "Epoch 527/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7256 - accuracy: 0.3827 - val_loss: 1.7574 - val_accuracy: 0.3896\n",
      "Epoch 528/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.7641 - accuracy: 0.3561 - val_loss: 1.7580 - val_accuracy: 0.3896\n",
      "Epoch 529/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8166 - accuracy: 0.3324 - val_loss: 1.7620 - val_accuracy: 0.3831\n",
      "Epoch 530/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7592 - accuracy: 0.3643 - val_loss: 1.7665 - val_accuracy: 0.3896\n",
      "Epoch 531/4000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.7779 - accuracy: 0.3691 - val_loss: 1.7733 - val_accuracy: 0.3929\n",
      "Epoch 532/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7917 - accuracy: 0.3547 - val_loss: 1.7799 - val_accuracy: 0.3766\n",
      "Epoch 533/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7314 - accuracy: 0.3623 - val_loss: 1.7833 - val_accuracy: 0.3734\n",
      "Epoch 534/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.7458 - accuracy: 0.3545 - val_loss: 1.7897 - val_accuracy: 0.3701\n",
      "Epoch 535/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7813 - accuracy: 0.3682 - val_loss: 1.7934 - val_accuracy: 0.3571\n",
      "Epoch 536/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7974 - accuracy: 0.3545 - val_loss: 1.7980 - val_accuracy: 0.3571\n",
      "Epoch 537/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7621 - accuracy: 0.3687 - val_loss: 1.7912 - val_accuracy: 0.3539\n",
      "Epoch 538/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7576 - accuracy: 0.3623 - val_loss: 1.7842 - val_accuracy: 0.3571\n",
      "Epoch 539/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7553 - accuracy: 0.3603 - val_loss: 1.7714 - val_accuracy: 0.3604\n",
      "Epoch 540/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7355 - accuracy: 0.3785 - val_loss: 1.7638 - val_accuracy: 0.3571\n",
      "Epoch 541/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7588 - accuracy: 0.3603 - val_loss: 1.7515 - val_accuracy: 0.3669\n",
      "Epoch 542/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7852 - accuracy: 0.3506 - val_loss: 1.7394 - val_accuracy: 0.3701\n",
      "Epoch 543/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7214 - accuracy: 0.3818 - val_loss: 1.7276 - val_accuracy: 0.3701\n",
      "Epoch 544/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7153 - accuracy: 0.3771 - val_loss: 1.7180 - val_accuracy: 0.3734\n",
      "Epoch 545/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.7467 - accuracy: 0.3687 - val_loss: 1.7093 - val_accuracy: 0.3766\n",
      "Epoch 546/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.7987 - accuracy: 0.3715 - val_loss: 1.7081 - val_accuracy: 0.3864\n",
      "Epoch 547/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.7348 - accuracy: 0.3827 - val_loss: 1.7091 - val_accuracy: 0.3896\n",
      "Epoch 548/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7409 - accuracy: 0.3750 - val_loss: 1.7157 - val_accuracy: 0.3864\n",
      "Epoch 549/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7478 - accuracy: 0.3662 - val_loss: 1.7222 - val_accuracy: 0.3929\n",
      "Epoch 550/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7613 - accuracy: 0.3883 - val_loss: 1.7329 - val_accuracy: 0.3831\n",
      "Epoch 551/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7600 - accuracy: 0.3926 - val_loss: 1.7500 - val_accuracy: 0.3669\n",
      "Epoch 552/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7130 - accuracy: 0.3799 - val_loss: 1.7676 - val_accuracy: 0.3636\n",
      "Epoch 553/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.7134 - accuracy: 0.3779 - val_loss: 1.7839 - val_accuracy: 0.3506\n",
      "Epoch 554/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7428 - accuracy: 0.3711 - val_loss: 1.7935 - val_accuracy: 0.3442\n",
      "Epoch 555/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7343 - accuracy: 0.3613 - val_loss: 1.7928 - val_accuracy: 0.3506\n",
      "Epoch 556/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7138 - accuracy: 0.3883 - val_loss: 1.7862 - val_accuracy: 0.3539\n",
      "Epoch 557/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.7365 - accuracy: 0.3828 - val_loss: 1.7790 - val_accuracy: 0.3669\n",
      "Epoch 558/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7590 - accuracy: 0.3869 - val_loss: 1.7659 - val_accuracy: 0.3701\n",
      "Epoch 559/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7145 - accuracy: 0.3799 - val_loss: 1.7487 - val_accuracy: 0.3766\n",
      "Epoch 560/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7828 - accuracy: 0.3561 - val_loss: 1.7287 - val_accuracy: 0.3766\n",
      "Epoch 561/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7297 - accuracy: 0.3841 - val_loss: 1.7048 - val_accuracy: 0.3994\n",
      "Epoch 562/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7589 - accuracy: 0.3564 - val_loss: 1.6893 - val_accuracy: 0.3961\n",
      "Epoch 563/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6991 - accuracy: 0.4064 - val_loss: 1.6819 - val_accuracy: 0.3961\n",
      "Epoch 564/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7228 - accuracy: 0.3701 - val_loss: 1.6746 - val_accuracy: 0.4026\n",
      "Epoch 565/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.7471 - accuracy: 0.3799 - val_loss: 1.6689 - val_accuracy: 0.4091\n",
      "Epoch 566/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7440 - accuracy: 0.3740 - val_loss: 1.6683 - val_accuracy: 0.4188\n",
      "Epoch 567/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7205 - accuracy: 0.3897 - val_loss: 1.6759 - val_accuracy: 0.4058\n",
      "Epoch 568/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7970 - accuracy: 0.3575 - val_loss: 1.6833 - val_accuracy: 0.3994\n",
      "Epoch 569/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7093 - accuracy: 0.3818 - val_loss: 1.6889 - val_accuracy: 0.3961\n",
      "Epoch 570/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7370 - accuracy: 0.3809 - val_loss: 1.6954 - val_accuracy: 0.3929\n",
      "Epoch 571/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7134 - accuracy: 0.3848 - val_loss: 1.7006 - val_accuracy: 0.3961\n",
      "Epoch 572/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7228 - accuracy: 0.3730 - val_loss: 1.7113 - val_accuracy: 0.3961\n",
      "Epoch 573/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7321 - accuracy: 0.3682 - val_loss: 1.7224 - val_accuracy: 0.3929\n",
      "Epoch 574/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.6684 - accuracy: 0.3841 - val_loss: 1.7364 - val_accuracy: 0.3929\n",
      "Epoch 575/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7624 - accuracy: 0.3855 - val_loss: 1.7542 - val_accuracy: 0.3734\n",
      "Epoch 576/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7189 - accuracy: 0.3838 - val_loss: 1.7671 - val_accuracy: 0.3669\n",
      "Epoch 577/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7412 - accuracy: 0.3925 - val_loss: 1.7671 - val_accuracy: 0.3701\n",
      "Epoch 578/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7068 - accuracy: 0.4008 - val_loss: 1.7639 - val_accuracy: 0.3604\n",
      "Epoch 579/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.7091 - accuracy: 0.3883 - val_loss: 1.7634 - val_accuracy: 0.3636\n",
      "Epoch 580/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7340 - accuracy: 0.3757 - val_loss: 1.7624 - val_accuracy: 0.3734\n",
      "Epoch 581/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6879 - accuracy: 0.3877 - val_loss: 1.7544 - val_accuracy: 0.3864\n",
      "Epoch 582/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6881 - accuracy: 0.3945 - val_loss: 1.7478 - val_accuracy: 0.3864\n",
      "Epoch 583/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.7315 - accuracy: 0.3789 - val_loss: 1.7498 - val_accuracy: 0.3766\n",
      "Epoch 584/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6798 - accuracy: 0.3682 - val_loss: 1.7493 - val_accuracy: 0.3701\n",
      "Epoch 585/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7261 - accuracy: 0.3799 - val_loss: 1.7448 - val_accuracy: 0.3669\n",
      "Epoch 586/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.7184 - accuracy: 0.3799 - val_loss: 1.7394 - val_accuracy: 0.3734\n",
      "Epoch 587/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6941 - accuracy: 0.3911 - val_loss: 1.7376 - val_accuracy: 0.3864\n",
      "Epoch 588/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7525 - accuracy: 0.4050 - val_loss: 1.7329 - val_accuracy: 0.3994\n",
      "Epoch 589/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.7219 - accuracy: 0.3799 - val_loss: 1.7374 - val_accuracy: 0.3961\n",
      "Epoch 590/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7002 - accuracy: 0.3799 - val_loss: 1.7442 - val_accuracy: 0.3929\n",
      "Epoch 591/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7481 - accuracy: 0.3939 - val_loss: 1.7491 - val_accuracy: 0.3929\n",
      "Epoch 592/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7025 - accuracy: 0.3955 - val_loss: 1.7480 - val_accuracy: 0.3896\n",
      "Epoch 593/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6981 - accuracy: 0.3965 - val_loss: 1.7434 - val_accuracy: 0.3994\n",
      "Epoch 594/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6890 - accuracy: 0.3715 - val_loss: 1.7278 - val_accuracy: 0.4123\n",
      "Epoch 595/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6764 - accuracy: 0.3848 - val_loss: 1.7149 - val_accuracy: 0.4351\n",
      "Epoch 596/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6944 - accuracy: 0.3984 - val_loss: 1.7054 - val_accuracy: 0.4383\n",
      "Epoch 597/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.7222 - accuracy: 0.3855 - val_loss: 1.7011 - val_accuracy: 0.4448\n",
      "Epoch 598/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6766 - accuracy: 0.4008 - val_loss: 1.6940 - val_accuracy: 0.4318\n",
      "Epoch 599/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.6734 - accuracy: 0.3925 - val_loss: 1.6840 - val_accuracy: 0.4156\n",
      "Epoch 600/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6755 - accuracy: 0.4014 - val_loss: 1.6777 - val_accuracy: 0.4156\n",
      "Epoch 601/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6798 - accuracy: 0.4053 - val_loss: 1.6746 - val_accuracy: 0.4221\n",
      "Epoch 602/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6707 - accuracy: 0.3994 - val_loss: 1.6743 - val_accuracy: 0.4123\n",
      "Epoch 603/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6608 - accuracy: 0.3715 - val_loss: 1.6758 - val_accuracy: 0.4123\n",
      "Epoch 604/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6829 - accuracy: 0.4092 - val_loss: 1.6696 - val_accuracy: 0.4156\n",
      "Epoch 605/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6842 - accuracy: 0.3740 - val_loss: 1.6661 - val_accuracy: 0.4156\n",
      "Epoch 606/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7011 - accuracy: 0.3785 - val_loss: 1.6641 - val_accuracy: 0.4156\n",
      "Epoch 607/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7208 - accuracy: 0.3994 - val_loss: 1.6600 - val_accuracy: 0.4156\n",
      "Epoch 608/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6888 - accuracy: 0.3939 - val_loss: 1.6521 - val_accuracy: 0.4188\n",
      "Epoch 609/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6762 - accuracy: 0.3955 - val_loss: 1.6456 - val_accuracy: 0.4156\n",
      "Epoch 610/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6581 - accuracy: 0.3771 - val_loss: 1.6384 - val_accuracy: 0.4123\n",
      "Epoch 611/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7205 - accuracy: 0.3869 - val_loss: 1.6354 - val_accuracy: 0.4156\n",
      "Epoch 612/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6656 - accuracy: 0.3925 - val_loss: 1.6401 - val_accuracy: 0.4123\n",
      "Epoch 613/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6885 - accuracy: 0.3855 - val_loss: 1.6445 - val_accuracy: 0.4058\n",
      "Epoch 614/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6498 - accuracy: 0.4131 - val_loss: 1.6476 - val_accuracy: 0.4058\n",
      "Epoch 615/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6637 - accuracy: 0.4092 - val_loss: 1.6487 - val_accuracy: 0.4156\n",
      "Epoch 616/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6697 - accuracy: 0.3975 - val_loss: 1.6455 - val_accuracy: 0.4123\n",
      "Epoch 617/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6582 - accuracy: 0.3945 - val_loss: 1.6469 - val_accuracy: 0.4091\n",
      "Epoch 618/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6783 - accuracy: 0.3729 - val_loss: 1.6429 - val_accuracy: 0.4091\n",
      "Epoch 619/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6634 - accuracy: 0.4082 - val_loss: 1.6411 - val_accuracy: 0.4091\n",
      "Epoch 620/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6352 - accuracy: 0.4268 - val_loss: 1.6461 - val_accuracy: 0.4058\n",
      "Epoch 621/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6208 - accuracy: 0.4274 - val_loss: 1.6506 - val_accuracy: 0.4026\n",
      "Epoch 622/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6067 - accuracy: 0.4248 - val_loss: 1.6506 - val_accuracy: 0.3961\n",
      "Epoch 623/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.6125 - accuracy: 0.4148 - val_loss: 1.6433 - val_accuracy: 0.4026\n",
      "Epoch 624/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6824 - accuracy: 0.3785 - val_loss: 1.6374 - val_accuracy: 0.4058\n",
      "Epoch 625/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6609 - accuracy: 0.3953 - val_loss: 1.6378 - val_accuracy: 0.4091\n",
      "Epoch 626/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6794 - accuracy: 0.3911 - val_loss: 1.6330 - val_accuracy: 0.4156\n",
      "Epoch 627/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6312 - accuracy: 0.4218 - val_loss: 1.6296 - val_accuracy: 0.4286\n",
      "Epoch 628/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6682 - accuracy: 0.3926 - val_loss: 1.6319 - val_accuracy: 0.4286\n",
      "Epoch 629/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6551 - accuracy: 0.4053 - val_loss: 1.6304 - val_accuracy: 0.4286\n",
      "Epoch 630/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6635 - accuracy: 0.4062 - val_loss: 1.6291 - val_accuracy: 0.4318\n",
      "Epoch 631/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.6157 - accuracy: 0.4358 - val_loss: 1.6192 - val_accuracy: 0.4286\n",
      "Epoch 632/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6572 - accuracy: 0.4150 - val_loss: 1.6079 - val_accuracy: 0.4253\n",
      "Epoch 633/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6116 - accuracy: 0.3841 - val_loss: 1.6030 - val_accuracy: 0.4188\n",
      "Epoch 634/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6371 - accuracy: 0.4092 - val_loss: 1.6054 - val_accuracy: 0.4123\n",
      "Epoch 635/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6415 - accuracy: 0.4180 - val_loss: 1.6094 - val_accuracy: 0.4123\n",
      "Epoch 636/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5665 - accuracy: 0.4623 - val_loss: 1.6132 - val_accuracy: 0.4123\n",
      "Epoch 637/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.6309 - accuracy: 0.4092 - val_loss: 1.6168 - val_accuracy: 0.4156\n",
      "Epoch 638/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.6288 - accuracy: 0.3966 - val_loss: 1.6328 - val_accuracy: 0.4026\n",
      "Epoch 639/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6444 - accuracy: 0.3966 - val_loss: 1.6602 - val_accuracy: 0.3961\n",
      "Epoch 640/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6324 - accuracy: 0.4326 - val_loss: 1.6767 - val_accuracy: 0.3864\n",
      "Epoch 641/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6478 - accuracy: 0.4092 - val_loss: 1.6879 - val_accuracy: 0.3799\n",
      "Epoch 642/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5976 - accuracy: 0.4404 - val_loss: 1.6852 - val_accuracy: 0.3734\n",
      "Epoch 643/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6436 - accuracy: 0.4078 - val_loss: 1.6834 - val_accuracy: 0.3799\n",
      "Epoch 644/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5841 - accuracy: 0.4413 - val_loss: 1.6736 - val_accuracy: 0.3864\n",
      "Epoch 645/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5913 - accuracy: 0.4375 - val_loss: 1.6684 - val_accuracy: 0.3929\n",
      "Epoch 646/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6228 - accuracy: 0.4302 - val_loss: 1.6474 - val_accuracy: 0.3994\n",
      "Epoch 647/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6160 - accuracy: 0.4232 - val_loss: 1.6307 - val_accuracy: 0.4123\n",
      "Epoch 648/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.6200 - accuracy: 0.4160 - val_loss: 1.6227 - val_accuracy: 0.4058\n",
      "Epoch 649/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.6305 - accuracy: 0.4204 - val_loss: 1.6278 - val_accuracy: 0.4026\n",
      "Epoch 650/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6452 - accuracy: 0.4062 - val_loss: 1.6333 - val_accuracy: 0.3994\n",
      "Epoch 651/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5975 - accuracy: 0.4287 - val_loss: 1.6406 - val_accuracy: 0.4026\n",
      "Epoch 652/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6645 - accuracy: 0.3994 - val_loss: 1.6591 - val_accuracy: 0.4026\n",
      "Epoch 653/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6095 - accuracy: 0.4344 - val_loss: 1.6669 - val_accuracy: 0.3994\n",
      "Epoch 654/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.6142 - accuracy: 0.4365 - val_loss: 1.6786 - val_accuracy: 0.3994\n",
      "Epoch 655/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6085 - accuracy: 0.4219 - val_loss: 1.7038 - val_accuracy: 0.3864\n",
      "Epoch 656/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5762 - accuracy: 0.4344 - val_loss: 1.7316 - val_accuracy: 0.3766\n",
      "Epoch 657/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6403 - accuracy: 0.3980 - val_loss: 1.7588 - val_accuracy: 0.3636\n",
      "Epoch 658/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6134 - accuracy: 0.4344 - val_loss: 1.7753 - val_accuracy: 0.3539\n",
      "Epoch 659/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5713 - accuracy: 0.4453 - val_loss: 1.7847 - val_accuracy: 0.3474\n",
      "Epoch 660/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6416 - accuracy: 0.4180 - val_loss: 1.7659 - val_accuracy: 0.3539\n",
      "Epoch 661/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6087 - accuracy: 0.4160 - val_loss: 1.7423 - val_accuracy: 0.3571\n",
      "Epoch 662/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6524 - accuracy: 0.4043 - val_loss: 1.7128 - val_accuracy: 0.3636\n",
      "Epoch 663/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5952 - accuracy: 0.4469 - val_loss: 1.6996 - val_accuracy: 0.3701\n",
      "Epoch 664/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6128 - accuracy: 0.4260 - val_loss: 1.6917 - val_accuracy: 0.3864\n",
      "Epoch 665/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5972 - accuracy: 0.4258 - val_loss: 1.6817 - val_accuracy: 0.3929\n",
      "Epoch 666/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6267 - accuracy: 0.4248 - val_loss: 1.6825 - val_accuracy: 0.3961\n",
      "Epoch 667/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6041 - accuracy: 0.4316 - val_loss: 1.7037 - val_accuracy: 0.3961\n",
      "Epoch 668/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5960 - accuracy: 0.4229 - val_loss: 1.7212 - val_accuracy: 0.3896\n",
      "Epoch 669/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5946 - accuracy: 0.4326 - val_loss: 1.7393 - val_accuracy: 0.3831\n",
      "Epoch 670/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5285 - accuracy: 0.4707 - val_loss: 1.7748 - val_accuracy: 0.3734\n",
      "Epoch 671/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.6249 - accuracy: 0.4277 - val_loss: 1.8000 - val_accuracy: 0.3799\n",
      "Epoch 672/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6198 - accuracy: 0.4078 - val_loss: 1.8248 - val_accuracy: 0.3539\n",
      "Epoch 673/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5998 - accuracy: 0.4473 - val_loss: 1.8460 - val_accuracy: 0.3636\n",
      "Epoch 674/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5328 - accuracy: 0.4525 - val_loss: 1.8757 - val_accuracy: 0.3539\n",
      "Epoch 675/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5719 - accuracy: 0.4246 - val_loss: 1.9096 - val_accuracy: 0.3506\n",
      "Epoch 676/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.5857 - accuracy: 0.4365 - val_loss: 1.9202 - val_accuracy: 0.3474\n",
      "Epoch 677/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5858 - accuracy: 0.4346 - val_loss: 1.9217 - val_accuracy: 0.3442\n",
      "Epoch 678/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6060 - accuracy: 0.4189 - val_loss: 1.9186 - val_accuracy: 0.3506\n",
      "Epoch 679/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5820 - accuracy: 0.4395 - val_loss: 1.9027 - val_accuracy: 0.3669\n",
      "Epoch 680/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5858 - accuracy: 0.4297 - val_loss: 1.8687 - val_accuracy: 0.3669\n",
      "Epoch 681/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.6138 - accuracy: 0.4246 - val_loss: 1.8310 - val_accuracy: 0.3766\n",
      "Epoch 682/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5906 - accuracy: 0.4274 - val_loss: 1.8187 - val_accuracy: 0.3734\n",
      "Epoch 683/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5511 - accuracy: 0.4482 - val_loss: 1.7984 - val_accuracy: 0.3799\n",
      "Epoch 684/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5453 - accuracy: 0.4805 - val_loss: 1.7833 - val_accuracy: 0.3831\n",
      "Epoch 685/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5992 - accuracy: 0.4372 - val_loss: 1.7586 - val_accuracy: 0.3961\n",
      "Epoch 686/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5111 - accuracy: 0.4679 - val_loss: 1.7241 - val_accuracy: 0.4188\n",
      "Epoch 687/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5403 - accuracy: 0.4497 - val_loss: 1.7022 - val_accuracy: 0.4221\n",
      "Epoch 688/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5863 - accuracy: 0.4434 - val_loss: 1.6880 - val_accuracy: 0.4253\n",
      "Epoch 689/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5563 - accuracy: 0.4463 - val_loss: 1.6771 - val_accuracy: 0.4221\n",
      "Epoch 690/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.6243 - accuracy: 0.4473 - val_loss: 1.6693 - val_accuracy: 0.4253\n",
      "Epoch 691/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.6311 - accuracy: 0.4258 - val_loss: 1.6727 - val_accuracy: 0.4188\n",
      "Epoch 692/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5840 - accuracy: 0.4473 - val_loss: 1.6878 - val_accuracy: 0.4156\n",
      "Epoch 693/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.6189 - accuracy: 0.4148 - val_loss: 1.6820 - val_accuracy: 0.4091\n",
      "Epoch 694/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5941 - accuracy: 0.4268 - val_loss: 1.6657 - val_accuracy: 0.4026\n",
      "Epoch 695/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5959 - accuracy: 0.4204 - val_loss: 1.6644 - val_accuracy: 0.4091\n",
      "Epoch 696/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5733 - accuracy: 0.4385 - val_loss: 1.6632 - val_accuracy: 0.4026\n",
      "Epoch 697/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.5681 - accuracy: 0.4623 - val_loss: 1.6741 - val_accuracy: 0.4058\n",
      "Epoch 698/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5561 - accuracy: 0.4455 - val_loss: 1.6878 - val_accuracy: 0.4058\n",
      "Epoch 699/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5873 - accuracy: 0.4232 - val_loss: 1.6955 - val_accuracy: 0.4058\n",
      "Epoch 700/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5446 - accuracy: 0.4511 - val_loss: 1.7057 - val_accuracy: 0.4123\n",
      "Epoch 701/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5872 - accuracy: 0.4229 - val_loss: 1.7195 - val_accuracy: 0.4156\n",
      "Epoch 702/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5166 - accuracy: 0.4648 - val_loss: 1.7370 - val_accuracy: 0.4058\n",
      "Epoch 703/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5653 - accuracy: 0.4502 - val_loss: 1.7330 - val_accuracy: 0.4091\n",
      "Epoch 704/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5344 - accuracy: 0.4721 - val_loss: 1.7153 - val_accuracy: 0.4026\n",
      "Epoch 705/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5869 - accuracy: 0.4427 - val_loss: 1.6833 - val_accuracy: 0.4091\n",
      "Epoch 706/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5709 - accuracy: 0.4427 - val_loss: 1.6447 - val_accuracy: 0.4318\n",
      "Epoch 707/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5777 - accuracy: 0.4232 - val_loss: 1.6179 - val_accuracy: 0.4416\n",
      "Epoch 708/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5407 - accuracy: 0.4473 - val_loss: 1.6053 - val_accuracy: 0.4253\n",
      "Epoch 709/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.4912 - accuracy: 0.4916 - val_loss: 1.6027 - val_accuracy: 0.4253\n",
      "Epoch 710/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5456 - accuracy: 0.4441 - val_loss: 1.6127 - val_accuracy: 0.4318\n",
      "Epoch 711/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5115 - accuracy: 0.4609 - val_loss: 1.6349 - val_accuracy: 0.4221\n",
      "Epoch 712/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5735 - accuracy: 0.4372 - val_loss: 1.6648 - val_accuracy: 0.4286\n",
      "Epoch 713/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5834 - accuracy: 0.4427 - val_loss: 1.6863 - val_accuracy: 0.4286\n",
      "Epoch 714/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5099 - accuracy: 0.4511 - val_loss: 1.6976 - val_accuracy: 0.4253\n",
      "Epoch 715/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5118 - accuracy: 0.4483 - val_loss: 1.7049 - val_accuracy: 0.4286\n",
      "Epoch 716/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5498 - accuracy: 0.4385 - val_loss: 1.7359 - val_accuracy: 0.4188\n",
      "Epoch 717/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5733 - accuracy: 0.4531 - val_loss: 1.7627 - val_accuracy: 0.4253\n",
      "Epoch 718/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.6073 - accuracy: 0.4455 - val_loss: 1.7659 - val_accuracy: 0.4156\n",
      "Epoch 719/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5623 - accuracy: 0.4512 - val_loss: 1.7584 - val_accuracy: 0.4091\n",
      "Epoch 720/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5585 - accuracy: 0.4497 - val_loss: 1.7295 - val_accuracy: 0.4091\n",
      "Epoch 721/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5287 - accuracy: 0.4648 - val_loss: 1.7050 - val_accuracy: 0.4123\n",
      "Epoch 722/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5233 - accuracy: 0.4539 - val_loss: 1.6762 - val_accuracy: 0.4286\n",
      "Epoch 723/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5720 - accuracy: 0.4570 - val_loss: 1.6618 - val_accuracy: 0.4351\n",
      "Epoch 724/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.5398 - accuracy: 0.4385 - val_loss: 1.6425 - val_accuracy: 0.4286\n",
      "Epoch 725/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5238 - accuracy: 0.4735 - val_loss: 1.6348 - val_accuracy: 0.4383\n",
      "Epoch 726/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5358 - accuracy: 0.4346 - val_loss: 1.6219 - val_accuracy: 0.4286\n",
      "Epoch 727/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5477 - accuracy: 0.4375 - val_loss: 1.6230 - val_accuracy: 0.4253\n",
      "Epoch 728/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5539 - accuracy: 0.4609 - val_loss: 1.6520 - val_accuracy: 0.4286\n",
      "Epoch 729/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5495 - accuracy: 0.4443 - val_loss: 1.6880 - val_accuracy: 0.4253\n",
      "Epoch 730/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5704 - accuracy: 0.4512 - val_loss: 1.7378 - val_accuracy: 0.4091\n",
      "Epoch 731/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5436 - accuracy: 0.4372 - val_loss: 1.7719 - val_accuracy: 0.3929\n",
      "Epoch 732/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5033 - accuracy: 0.4832 - val_loss: 1.8041 - val_accuracy: 0.3896\n",
      "Epoch 733/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4818 - accuracy: 0.4688 - val_loss: 1.8246 - val_accuracy: 0.3896\n",
      "Epoch 734/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5168 - accuracy: 0.4665 - val_loss: 1.8390 - val_accuracy: 0.3929\n",
      "Epoch 735/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4832 - accuracy: 0.4832 - val_loss: 1.8350 - val_accuracy: 0.3864\n",
      "Epoch 736/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5548 - accuracy: 0.4629 - val_loss: 1.8167 - val_accuracy: 0.3864\n",
      "Epoch 737/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5146 - accuracy: 0.4567 - val_loss: 1.7861 - val_accuracy: 0.4058\n",
      "Epoch 738/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5058 - accuracy: 0.4629 - val_loss: 1.7575 - val_accuracy: 0.4221\n",
      "Epoch 739/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5439 - accuracy: 0.4629 - val_loss: 1.7356 - val_accuracy: 0.4286\n",
      "Epoch 740/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5140 - accuracy: 0.4665 - val_loss: 1.7176 - val_accuracy: 0.4253\n",
      "Epoch 741/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5434 - accuracy: 0.4385 - val_loss: 1.6918 - val_accuracy: 0.4318\n",
      "Epoch 742/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.5144 - accuracy: 0.4561 - val_loss: 1.6837 - val_accuracy: 0.4416\n",
      "Epoch 743/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5436 - accuracy: 0.4424 - val_loss: 1.6720 - val_accuracy: 0.4416\n",
      "Epoch 744/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4480 - accuracy: 0.4863 - val_loss: 1.6591 - val_accuracy: 0.4578\n",
      "Epoch 745/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4985 - accuracy: 0.4658 - val_loss: 1.6400 - val_accuracy: 0.4740\n",
      "Epoch 746/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4812 - accuracy: 0.4930 - val_loss: 1.6397 - val_accuracy: 0.4740\n",
      "Epoch 747/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4869 - accuracy: 0.4777 - val_loss: 1.6422 - val_accuracy: 0.4643\n",
      "Epoch 748/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5227 - accuracy: 0.4619 - val_loss: 1.6581 - val_accuracy: 0.4643\n",
      "Epoch 749/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.5058 - accuracy: 0.4883 - val_loss: 1.6809 - val_accuracy: 0.4481\n",
      "Epoch 750/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4914 - accuracy: 0.4637 - val_loss: 1.7199 - val_accuracy: 0.4513\n",
      "Epoch 751/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4260 - accuracy: 0.4893 - val_loss: 1.7274 - val_accuracy: 0.4513\n",
      "Epoch 752/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5018 - accuracy: 0.4749 - val_loss: 1.7418 - val_accuracy: 0.4513\n",
      "Epoch 753/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5324 - accuracy: 0.4580 - val_loss: 1.7508 - val_accuracy: 0.4416\n",
      "Epoch 754/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4748 - accuracy: 0.4986 - val_loss: 1.7667 - val_accuracy: 0.4286\n",
      "Epoch 755/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4889 - accuracy: 0.4531 - val_loss: 1.8024 - val_accuracy: 0.4123\n",
      "Epoch 756/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4378 - accuracy: 0.4818 - val_loss: 1.8136 - val_accuracy: 0.4058\n",
      "Epoch 757/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5222 - accuracy: 0.4749 - val_loss: 1.8019 - val_accuracy: 0.4026\n",
      "Epoch 758/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4883 - accuracy: 0.4561 - val_loss: 1.7842 - val_accuracy: 0.4123\n",
      "Epoch 759/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4126 - accuracy: 0.5084 - val_loss: 1.7697 - val_accuracy: 0.4253\n",
      "Epoch 760/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5006 - accuracy: 0.4763 - val_loss: 1.7495 - val_accuracy: 0.4221\n",
      "Epoch 761/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4827 - accuracy: 0.4777 - val_loss: 1.7587 - val_accuracy: 0.4156\n",
      "Epoch 762/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5156 - accuracy: 0.4619 - val_loss: 1.7641 - val_accuracy: 0.4188\n",
      "Epoch 763/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5076 - accuracy: 0.4854 - val_loss: 1.7513 - val_accuracy: 0.4156\n",
      "Epoch 764/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5051 - accuracy: 0.4902 - val_loss: 1.7357 - val_accuracy: 0.4253\n",
      "Epoch 765/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5127 - accuracy: 0.4648 - val_loss: 1.7012 - val_accuracy: 0.4351\n",
      "Epoch 766/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4729 - accuracy: 0.4893 - val_loss: 1.6586 - val_accuracy: 0.4578\n",
      "Epoch 767/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5038 - accuracy: 0.4541 - val_loss: 1.6225 - val_accuracy: 0.4610\n",
      "Epoch 768/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4828 - accuracy: 0.4551 - val_loss: 1.6023 - val_accuracy: 0.4675\n",
      "Epoch 769/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5376 - accuracy: 0.4570 - val_loss: 1.6138 - val_accuracy: 0.4675\n",
      "Epoch 770/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5237 - accuracy: 0.4581 - val_loss: 1.6395 - val_accuracy: 0.4578\n",
      "Epoch 771/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4244 - accuracy: 0.4980 - val_loss: 1.6819 - val_accuracy: 0.4513\n",
      "Epoch 772/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4494 - accuracy: 0.5029 - val_loss: 1.7164 - val_accuracy: 0.4448\n",
      "Epoch 773/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4398 - accuracy: 0.5049 - val_loss: 1.7336 - val_accuracy: 0.4481\n",
      "Epoch 774/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4663 - accuracy: 0.4971 - val_loss: 1.7239 - val_accuracy: 0.4481\n",
      "Epoch 775/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4581 - accuracy: 0.4912 - val_loss: 1.6973 - val_accuracy: 0.4545\n",
      "Epoch 776/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4970 - accuracy: 0.4763 - val_loss: 1.6658 - val_accuracy: 0.4513\n",
      "Epoch 777/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4716 - accuracy: 0.4749 - val_loss: 1.6285 - val_accuracy: 0.4481\n",
      "Epoch 778/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4713 - accuracy: 0.4581 - val_loss: 1.6019 - val_accuracy: 0.4708\n",
      "Epoch 779/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4712 - accuracy: 0.4805 - val_loss: 1.5725 - val_accuracy: 0.4805\n",
      "Epoch 780/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4604 - accuracy: 0.4912 - val_loss: 1.5799 - val_accuracy: 0.4708\n",
      "Epoch 781/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.4386 - accuracy: 0.5000 - val_loss: 1.5996 - val_accuracy: 0.4610\n",
      "Epoch 782/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4999 - accuracy: 0.4521 - val_loss: 1.6234 - val_accuracy: 0.4513\n",
      "Epoch 783/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4763 - accuracy: 0.4846 - val_loss: 1.6433 - val_accuracy: 0.4481\n",
      "Epoch 784/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4370 - accuracy: 0.4844 - val_loss: 1.6817 - val_accuracy: 0.4416\n",
      "Epoch 785/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4838 - accuracy: 0.4814 - val_loss: 1.6983 - val_accuracy: 0.4221\n",
      "Epoch 786/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4875 - accuracy: 0.4648 - val_loss: 1.7065 - val_accuracy: 0.4188\n",
      "Epoch 787/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4166 - accuracy: 0.5195 - val_loss: 1.7167 - val_accuracy: 0.4188\n",
      "Epoch 788/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4795 - accuracy: 0.5056 - val_loss: 1.7298 - val_accuracy: 0.4253\n",
      "Epoch 789/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4437 - accuracy: 0.5000 - val_loss: 1.7600 - val_accuracy: 0.3994\n",
      "Epoch 790/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4772 - accuracy: 0.4600 - val_loss: 1.7997 - val_accuracy: 0.3864\n",
      "Epoch 791/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4200 - accuracy: 0.5070 - val_loss: 1.8403 - val_accuracy: 0.3734\n",
      "Epoch 792/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4505 - accuracy: 0.5020 - val_loss: 1.8690 - val_accuracy: 0.3636\n",
      "Epoch 793/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4551 - accuracy: 0.4958 - val_loss: 1.8537 - val_accuracy: 0.3734\n",
      "Epoch 794/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4530 - accuracy: 0.4922 - val_loss: 1.8092 - val_accuracy: 0.3864\n",
      "Epoch 795/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4218 - accuracy: 0.4961 - val_loss: 1.7451 - val_accuracy: 0.3961\n",
      "Epoch 796/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4171 - accuracy: 0.5000 - val_loss: 1.6662 - val_accuracy: 0.4481\n",
      "Epoch 797/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4841 - accuracy: 0.4860 - val_loss: 1.5957 - val_accuracy: 0.4578\n",
      "Epoch 798/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4419 - accuracy: 0.4971 - val_loss: 1.5350 - val_accuracy: 0.4773\n",
      "Epoch 799/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5279 - accuracy: 0.4791 - val_loss: 1.5053 - val_accuracy: 0.4903\n",
      "Epoch 800/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3891 - accuracy: 0.5127 - val_loss: 1.5096 - val_accuracy: 0.4805\n",
      "Epoch 801/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4008 - accuracy: 0.5196 - val_loss: 1.5345 - val_accuracy: 0.4740\n",
      "Epoch 802/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4908 - accuracy: 0.4912 - val_loss: 1.5697 - val_accuracy: 0.4610\n",
      "Epoch 803/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.4793 - accuracy: 0.4902 - val_loss: 1.6084 - val_accuracy: 0.4513\n",
      "Epoch 804/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4159 - accuracy: 0.5014 - val_loss: 1.6332 - val_accuracy: 0.4481\n",
      "Epoch 805/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4300 - accuracy: 0.5056 - val_loss: 1.6493 - val_accuracy: 0.4513\n",
      "Epoch 806/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4428 - accuracy: 0.5117 - val_loss: 1.6644 - val_accuracy: 0.4351\n",
      "Epoch 807/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4198 - accuracy: 0.5168 - val_loss: 1.6511 - val_accuracy: 0.4448\n",
      "Epoch 808/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4233 - accuracy: 0.5078 - val_loss: 1.6395 - val_accuracy: 0.4740\n",
      "Epoch 809/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4610 - accuracy: 0.4932 - val_loss: 1.6328 - val_accuracy: 0.4708\n",
      "Epoch 810/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3646 - accuracy: 0.5265 - val_loss: 1.6317 - val_accuracy: 0.4708\n",
      "Epoch 811/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5046 - accuracy: 0.4756 - val_loss: 1.6441 - val_accuracy: 0.4675\n",
      "Epoch 812/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.3783 - accuracy: 0.5205 - val_loss: 1.6438 - val_accuracy: 0.4740\n",
      "Epoch 813/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4211 - accuracy: 0.4986 - val_loss: 1.6663 - val_accuracy: 0.4708\n",
      "Epoch 814/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4284 - accuracy: 0.4707 - val_loss: 1.6786 - val_accuracy: 0.4773\n",
      "Epoch 815/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4586 - accuracy: 0.5014 - val_loss: 1.6835 - val_accuracy: 0.4675\n",
      "Epoch 816/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3773 - accuracy: 0.5000 - val_loss: 1.6734 - val_accuracy: 0.4675\n",
      "Epoch 817/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4373 - accuracy: 0.5010 - val_loss: 1.6915 - val_accuracy: 0.4578\n",
      "Epoch 818/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3527 - accuracy: 0.5244 - val_loss: 1.6629 - val_accuracy: 0.4610\n",
      "Epoch 819/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4532 - accuracy: 0.4736 - val_loss: 1.6324 - val_accuracy: 0.4675\n",
      "Epoch 820/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4458 - accuracy: 0.4775 - val_loss: 1.5807 - val_accuracy: 0.5000\n",
      "Epoch 821/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3821 - accuracy: 0.5084 - val_loss: 1.5442 - val_accuracy: 0.5065\n",
      "Epoch 822/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.4027 - accuracy: 0.5215 - val_loss: 1.5180 - val_accuracy: 0.5065\n",
      "Epoch 823/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4147 - accuracy: 0.5117 - val_loss: 1.5108 - val_accuracy: 0.5097\n",
      "Epoch 824/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.4128 - accuracy: 0.5215 - val_loss: 1.4865 - val_accuracy: 0.5097\n",
      "Epoch 825/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4097 - accuracy: 0.5168 - val_loss: 1.4634 - val_accuracy: 0.5065\n",
      "Epoch 826/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3858 - accuracy: 0.5059 - val_loss: 1.4573 - val_accuracy: 0.5097\n",
      "Epoch 827/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4263 - accuracy: 0.5039 - val_loss: 1.4654 - val_accuracy: 0.5000\n",
      "Epoch 828/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.4518 - accuracy: 0.5112 - val_loss: 1.4920 - val_accuracy: 0.4968\n",
      "Epoch 829/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.3736 - accuracy: 0.5205 - val_loss: 1.5204 - val_accuracy: 0.4805\n",
      "Epoch 830/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3801 - accuracy: 0.5154 - val_loss: 1.5425 - val_accuracy: 0.4740\n",
      "Epoch 831/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4409 - accuracy: 0.4888 - val_loss: 1.5773 - val_accuracy: 0.4578\n",
      "Epoch 832/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.4313 - accuracy: 0.5020 - val_loss: 1.5919 - val_accuracy: 0.4643\n",
      "Epoch 833/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3570 - accuracy: 0.5117 - val_loss: 1.6133 - val_accuracy: 0.4545\n",
      "Epoch 834/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4642 - accuracy: 0.4824 - val_loss: 1.6446 - val_accuracy: 0.4513\n",
      "Epoch 835/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4592 - accuracy: 0.4972 - val_loss: 1.6875 - val_accuracy: 0.4351\n",
      "Epoch 836/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4106 - accuracy: 0.5042 - val_loss: 1.7135 - val_accuracy: 0.4351\n",
      "Epoch 837/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4167 - accuracy: 0.5088 - val_loss: 1.7175 - val_accuracy: 0.4318\n",
      "Epoch 838/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4241 - accuracy: 0.5029 - val_loss: 1.7105 - val_accuracy: 0.4351\n",
      "Epoch 839/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3715 - accuracy: 0.5176 - val_loss: 1.6894 - val_accuracy: 0.4383\n",
      "Epoch 840/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3994 - accuracy: 0.5127 - val_loss: 1.6540 - val_accuracy: 0.4416\n",
      "Epoch 841/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3995 - accuracy: 0.5112 - val_loss: 1.6073 - val_accuracy: 0.4578\n",
      "Epoch 842/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4275 - accuracy: 0.4902 - val_loss: 1.5738 - val_accuracy: 0.4773\n",
      "Epoch 843/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4395 - accuracy: 0.5088 - val_loss: 1.5593 - val_accuracy: 0.4935\n",
      "Epoch 844/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4444 - accuracy: 0.4902 - val_loss: 1.5699 - val_accuracy: 0.4968\n",
      "Epoch 845/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3818 - accuracy: 0.5137 - val_loss: 1.5729 - val_accuracy: 0.5000\n",
      "Epoch 846/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3768 - accuracy: 0.5209 - val_loss: 1.5924 - val_accuracy: 0.4968\n",
      "Epoch 847/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3845 - accuracy: 0.5098 - val_loss: 1.6226 - val_accuracy: 0.4903\n",
      "Epoch 848/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3693 - accuracy: 0.5223 - val_loss: 1.6651 - val_accuracy: 0.4643\n",
      "Epoch 849/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4593 - accuracy: 0.4688 - val_loss: 1.6818 - val_accuracy: 0.4513\n",
      "Epoch 850/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3737 - accuracy: 0.5251 - val_loss: 1.6788 - val_accuracy: 0.4578\n",
      "Epoch 851/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3912 - accuracy: 0.5209 - val_loss: 1.6425 - val_accuracy: 0.4675\n",
      "Epoch 852/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3572 - accuracy: 0.5279 - val_loss: 1.6020 - val_accuracy: 0.4935\n",
      "Epoch 853/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3872 - accuracy: 0.5215 - val_loss: 1.5691 - val_accuracy: 0.5065\n",
      "Epoch 854/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3964 - accuracy: 0.5237 - val_loss: 1.5512 - val_accuracy: 0.5130\n",
      "Epoch 855/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3991 - accuracy: 0.4860 - val_loss: 1.5636 - val_accuracy: 0.5065\n",
      "Epoch 856/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3838 - accuracy: 0.5168 - val_loss: 1.6140 - val_accuracy: 0.4805\n",
      "Epoch 857/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3809 - accuracy: 0.5168 - val_loss: 1.6831 - val_accuracy: 0.4643\n",
      "Epoch 858/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4559 - accuracy: 0.4717 - val_loss: 1.7616 - val_accuracy: 0.4610\n",
      "Epoch 859/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4036 - accuracy: 0.5042 - val_loss: 1.8207 - val_accuracy: 0.4351\n",
      "Epoch 860/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.4042 - accuracy: 0.5146 - val_loss: 1.8310 - val_accuracy: 0.4221\n",
      "Epoch 861/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3770 - accuracy: 0.5312 - val_loss: 1.8074 - val_accuracy: 0.4253\n",
      "Epoch 862/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4181 - accuracy: 0.5166 - val_loss: 1.7462 - val_accuracy: 0.4416\n",
      "Epoch 863/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3957 - accuracy: 0.5042 - val_loss: 1.6836 - val_accuracy: 0.4416\n",
      "Epoch 864/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3345 - accuracy: 0.5303 - val_loss: 1.6011 - val_accuracy: 0.4578\n",
      "Epoch 865/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3598 - accuracy: 0.5056 - val_loss: 1.5240 - val_accuracy: 0.5000\n",
      "Epoch 866/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4047 - accuracy: 0.5098 - val_loss: 1.4670 - val_accuracy: 0.5000\n",
      "Epoch 867/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.3726 - accuracy: 0.5293 - val_loss: 1.4255 - val_accuracy: 0.5162\n",
      "Epoch 868/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3714 - accuracy: 0.5307 - val_loss: 1.4034 - val_accuracy: 0.5227\n",
      "Epoch 869/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3140 - accuracy: 0.5461 - val_loss: 1.3990 - val_accuracy: 0.5227\n",
      "Epoch 870/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4051 - accuracy: 0.5029 - val_loss: 1.4138 - val_accuracy: 0.5000\n",
      "Epoch 871/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4190 - accuracy: 0.5056 - val_loss: 1.4236 - val_accuracy: 0.5000\n",
      "Epoch 872/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.3586 - accuracy: 0.5137 - val_loss: 1.4368 - val_accuracy: 0.5065\n",
      "Epoch 873/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3692 - accuracy: 0.5283 - val_loss: 1.4527 - val_accuracy: 0.5032\n",
      "Epoch 874/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3924 - accuracy: 0.4971 - val_loss: 1.4545 - val_accuracy: 0.5032\n",
      "Epoch 875/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3846 - accuracy: 0.5244 - val_loss: 1.4524 - val_accuracy: 0.5000\n",
      "Epoch 876/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3925 - accuracy: 0.5237 - val_loss: 1.4216 - val_accuracy: 0.5032\n",
      "Epoch 877/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3159 - accuracy: 0.5349 - val_loss: 1.4207 - val_accuracy: 0.5227\n",
      "Epoch 878/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3254 - accuracy: 0.5140 - val_loss: 1.4073 - val_accuracy: 0.5325\n",
      "Epoch 879/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3727 - accuracy: 0.5049 - val_loss: 1.3997 - val_accuracy: 0.5195\n",
      "Epoch 880/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3822 - accuracy: 0.5405 - val_loss: 1.4094 - val_accuracy: 0.5162\n",
      "Epoch 881/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3383 - accuracy: 0.5447 - val_loss: 1.4127 - val_accuracy: 0.5260\n",
      "Epoch 882/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2981 - accuracy: 0.5381 - val_loss: 1.4097 - val_accuracy: 0.5292\n",
      "Epoch 883/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3659 - accuracy: 0.5293 - val_loss: 1.3846 - val_accuracy: 0.5390\n",
      "Epoch 884/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3282 - accuracy: 0.5503 - val_loss: 1.3725 - val_accuracy: 0.5584\n",
      "Epoch 885/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3638 - accuracy: 0.5293 - val_loss: 1.3757 - val_accuracy: 0.5390\n",
      "Epoch 886/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3353 - accuracy: 0.5205 - val_loss: 1.3694 - val_accuracy: 0.5422\n",
      "Epoch 887/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3359 - accuracy: 0.5489 - val_loss: 1.3837 - val_accuracy: 0.5422\n",
      "Epoch 888/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3828 - accuracy: 0.5166 - val_loss: 1.4001 - val_accuracy: 0.5422\n",
      "Epoch 889/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3681 - accuracy: 0.5293 - val_loss: 1.4160 - val_accuracy: 0.5325\n",
      "Epoch 890/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3639 - accuracy: 0.5234 - val_loss: 1.4392 - val_accuracy: 0.5195\n",
      "Epoch 891/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3419 - accuracy: 0.5215 - val_loss: 1.4680 - val_accuracy: 0.5097\n",
      "Epoch 892/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3912 - accuracy: 0.5196 - val_loss: 1.4957 - val_accuracy: 0.5032\n",
      "Epoch 893/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3234 - accuracy: 0.5391 - val_loss: 1.5021 - val_accuracy: 0.5032\n",
      "Epoch 894/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3155 - accuracy: 0.5461 - val_loss: 1.5160 - val_accuracy: 0.5130\n",
      "Epoch 895/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3471 - accuracy: 0.5293 - val_loss: 1.5128 - val_accuracy: 0.5162\n",
      "Epoch 896/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3495 - accuracy: 0.5312 - val_loss: 1.4913 - val_accuracy: 0.5357\n",
      "Epoch 897/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3539 - accuracy: 0.5410 - val_loss: 1.4703 - val_accuracy: 0.5390\n",
      "Epoch 898/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3278 - accuracy: 0.5461 - val_loss: 1.4680 - val_accuracy: 0.5325\n",
      "Epoch 899/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3410 - accuracy: 0.5363 - val_loss: 1.4914 - val_accuracy: 0.5260\n",
      "Epoch 900/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3818 - accuracy: 0.5342 - val_loss: 1.5187 - val_accuracy: 0.5032\n",
      "Epoch 901/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2892 - accuracy: 0.5587 - val_loss: 1.5582 - val_accuracy: 0.4935\n",
      "Epoch 902/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3647 - accuracy: 0.5156 - val_loss: 1.5847 - val_accuracy: 0.4675\n",
      "Epoch 903/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3581 - accuracy: 0.5196 - val_loss: 1.6094 - val_accuracy: 0.4708\n",
      "Epoch 904/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3901 - accuracy: 0.5166 - val_loss: 1.6019 - val_accuracy: 0.4643\n",
      "Epoch 905/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3265 - accuracy: 0.5447 - val_loss: 1.5383 - val_accuracy: 0.4935\n",
      "Epoch 906/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.4168 - accuracy: 0.4980 - val_loss: 1.4607 - val_accuracy: 0.5065\n",
      "Epoch 907/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3289 - accuracy: 0.5312 - val_loss: 1.4006 - val_accuracy: 0.5390\n",
      "Epoch 908/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2939 - accuracy: 0.5391 - val_loss: 1.3503 - val_accuracy: 0.5584\n",
      "Epoch 909/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2607 - accuracy: 0.5664 - val_loss: 1.3155 - val_accuracy: 0.5714\n",
      "Epoch 910/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3326 - accuracy: 0.5264 - val_loss: 1.3267 - val_accuracy: 0.5649\n",
      "Epoch 911/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3743 - accuracy: 0.5430 - val_loss: 1.3765 - val_accuracy: 0.5649\n",
      "Epoch 912/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3084 - accuracy: 0.5391 - val_loss: 1.4301 - val_accuracy: 0.5325\n",
      "Epoch 913/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3619 - accuracy: 0.5391 - val_loss: 1.4874 - val_accuracy: 0.5000\n",
      "Epoch 914/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3717 - accuracy: 0.5056 - val_loss: 1.5587 - val_accuracy: 0.4935\n",
      "Epoch 915/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3069 - accuracy: 0.5400 - val_loss: 1.5937 - val_accuracy: 0.4935\n",
      "Epoch 916/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3357 - accuracy: 0.5293 - val_loss: 1.5925 - val_accuracy: 0.4903\n",
      "Epoch 917/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3160 - accuracy: 0.5447 - val_loss: 1.5271 - val_accuracy: 0.5097\n",
      "Epoch 918/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.2883 - accuracy: 0.5352 - val_loss: 1.4380 - val_accuracy: 0.5455\n",
      "Epoch 919/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3326 - accuracy: 0.5405 - val_loss: 1.3378 - val_accuracy: 0.5519\n",
      "Epoch 920/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2804 - accuracy: 0.5447 - val_loss: 1.2743 - val_accuracy: 0.5877\n",
      "Epoch 921/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3016 - accuracy: 0.5419 - val_loss: 1.2591 - val_accuracy: 0.5779\n",
      "Epoch 922/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3541 - accuracy: 0.5215 - val_loss: 1.2579 - val_accuracy: 0.5714\n",
      "Epoch 923/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3443 - accuracy: 0.5469 - val_loss: 1.2582 - val_accuracy: 0.5714\n",
      "Epoch 924/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2793 - accuracy: 0.5791 - val_loss: 1.2566 - val_accuracy: 0.5682\n",
      "Epoch 925/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3545 - accuracy: 0.5127 - val_loss: 1.2667 - val_accuracy: 0.5682\n",
      "Epoch 926/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2874 - accuracy: 0.5684 - val_loss: 1.2817 - val_accuracy: 0.5617\n",
      "Epoch 927/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3433 - accuracy: 0.5400 - val_loss: 1.3108 - val_accuracy: 0.5455\n",
      "Epoch 928/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3176 - accuracy: 0.5449 - val_loss: 1.3352 - val_accuracy: 0.5552\n",
      "Epoch 929/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3236 - accuracy: 0.5531 - val_loss: 1.3524 - val_accuracy: 0.5617\n",
      "Epoch 930/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3221 - accuracy: 0.5391 - val_loss: 1.3609 - val_accuracy: 0.5584\n",
      "Epoch 931/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3309 - accuracy: 0.5405 - val_loss: 1.3658 - val_accuracy: 0.5584\n",
      "Epoch 932/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3239 - accuracy: 0.5307 - val_loss: 1.3637 - val_accuracy: 0.5552\n",
      "Epoch 933/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2940 - accuracy: 0.5654 - val_loss: 1.3485 - val_accuracy: 0.5519\n",
      "Epoch 934/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3136 - accuracy: 0.5283 - val_loss: 1.3362 - val_accuracy: 0.5487\n",
      "Epoch 935/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4034 - accuracy: 0.5059 - val_loss: 1.3224 - val_accuracy: 0.5357\n",
      "Epoch 936/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2514 - accuracy: 0.5670 - val_loss: 1.3044 - val_accuracy: 0.5455\n",
      "Epoch 937/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3037 - accuracy: 0.5537 - val_loss: 1.2966 - val_accuracy: 0.5487\n",
      "Epoch 938/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3572 - accuracy: 0.5377 - val_loss: 1.2978 - val_accuracy: 0.5487\n",
      "Epoch 939/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3051 - accuracy: 0.5405 - val_loss: 1.3024 - val_accuracy: 0.5487\n",
      "Epoch 940/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2912 - accuracy: 0.5517 - val_loss: 1.3066 - val_accuracy: 0.5682\n",
      "Epoch 941/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.3205 - accuracy: 0.5488 - val_loss: 1.3203 - val_accuracy: 0.5682\n",
      "Epoch 942/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.2849 - accuracy: 0.5488 - val_loss: 1.3381 - val_accuracy: 0.5682\n",
      "Epoch 943/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2901 - accuracy: 0.5479 - val_loss: 1.3529 - val_accuracy: 0.5682\n",
      "Epoch 944/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3216 - accuracy: 0.5475 - val_loss: 1.3626 - val_accuracy: 0.5682\n",
      "Epoch 945/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3260 - accuracy: 0.5459 - val_loss: 1.3667 - val_accuracy: 0.5584\n",
      "Epoch 946/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3646 - accuracy: 0.5332 - val_loss: 1.3805 - val_accuracy: 0.5649\n",
      "Epoch 947/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2985 - accuracy: 0.5573 - val_loss: 1.3764 - val_accuracy: 0.5584\n",
      "Epoch 948/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2870 - accuracy: 0.5625 - val_loss: 1.3640 - val_accuracy: 0.5584\n",
      "Epoch 949/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3424 - accuracy: 0.5363 - val_loss: 1.3870 - val_accuracy: 0.5390\n",
      "Epoch 950/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3188 - accuracy: 0.5527 - val_loss: 1.4148 - val_accuracy: 0.5357\n",
      "Epoch 951/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.2151 - accuracy: 0.5964 - val_loss: 1.4349 - val_accuracy: 0.5097\n",
      "Epoch 952/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3106 - accuracy: 0.5670 - val_loss: 1.4317 - val_accuracy: 0.5130\n",
      "Epoch 953/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.3203 - accuracy: 0.5332 - val_loss: 1.4000 - val_accuracy: 0.5260\n",
      "Epoch 954/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3411 - accuracy: 0.5615 - val_loss: 1.3700 - val_accuracy: 0.5487\n",
      "Epoch 955/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2619 - accuracy: 0.5625 - val_loss: 1.3487 - val_accuracy: 0.5519\n",
      "Epoch 956/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3376 - accuracy: 0.5307 - val_loss: 1.3520 - val_accuracy: 0.5519\n",
      "Epoch 957/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.2801 - accuracy: 0.5596 - val_loss: 1.3690 - val_accuracy: 0.5487\n",
      "Epoch 958/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2976 - accuracy: 0.5557 - val_loss: 1.3788 - val_accuracy: 0.5390\n",
      "Epoch 959/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3114 - accuracy: 0.5566 - val_loss: 1.3939 - val_accuracy: 0.5292\n",
      "Epoch 960/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2899 - accuracy: 0.5469 - val_loss: 1.4102 - val_accuracy: 0.5357\n",
      "Epoch 961/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2487 - accuracy: 0.5605 - val_loss: 1.4042 - val_accuracy: 0.5292\n",
      "Epoch 962/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3254 - accuracy: 0.5559 - val_loss: 1.4001 - val_accuracy: 0.5487\n",
      "Epoch 963/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2823 - accuracy: 0.5587 - val_loss: 1.3831 - val_accuracy: 0.5519\n",
      "Epoch 964/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.2883 - accuracy: 0.5645 - val_loss: 1.3745 - val_accuracy: 0.5519\n",
      "Epoch 965/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2315 - accuracy: 0.5922 - val_loss: 1.3696 - val_accuracy: 0.5519\n",
      "Epoch 966/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.2646 - accuracy: 0.5586 - val_loss: 1.3680 - val_accuracy: 0.5422\n",
      "Epoch 967/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2727 - accuracy: 0.5547 - val_loss: 1.3589 - val_accuracy: 0.5390\n",
      "Epoch 968/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2436 - accuracy: 0.5754 - val_loss: 1.3888 - val_accuracy: 0.5292\n",
      "Epoch 969/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2587 - accuracy: 0.5508 - val_loss: 1.4223 - val_accuracy: 0.5162\n",
      "Epoch 970/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3311 - accuracy: 0.5251 - val_loss: 1.4360 - val_accuracy: 0.5000\n",
      "Epoch 971/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2989 - accuracy: 0.5469 - val_loss: 1.4211 - val_accuracy: 0.5162\n",
      "Epoch 972/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3307 - accuracy: 0.5391 - val_loss: 1.3917 - val_accuracy: 0.5195\n",
      "Epoch 973/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3652 - accuracy: 0.5279 - val_loss: 1.3415 - val_accuracy: 0.5519\n",
      "Epoch 974/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2571 - accuracy: 0.5628 - val_loss: 1.2995 - val_accuracy: 0.5942\n",
      "Epoch 975/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2777 - accuracy: 0.5517 - val_loss: 1.2724 - val_accuracy: 0.5877\n",
      "Epoch 976/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2493 - accuracy: 0.5449 - val_loss: 1.2528 - val_accuracy: 0.5942\n",
      "Epoch 977/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2969 - accuracy: 0.5782 - val_loss: 1.2469 - val_accuracy: 0.5812\n",
      "Epoch 978/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2366 - accuracy: 0.5537 - val_loss: 1.2363 - val_accuracy: 0.5909\n",
      "Epoch 979/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2482 - accuracy: 0.5674 - val_loss: 1.2214 - val_accuracy: 0.5877\n",
      "Epoch 980/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2860 - accuracy: 0.5615 - val_loss: 1.2124 - val_accuracy: 0.5877\n",
      "Epoch 981/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.2731 - accuracy: 0.5566 - val_loss: 1.2105 - val_accuracy: 0.5844\n",
      "Epoch 982/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2722 - accuracy: 0.5656 - val_loss: 1.2077 - val_accuracy: 0.5844\n",
      "Epoch 983/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2639 - accuracy: 0.5587 - val_loss: 1.2073 - val_accuracy: 0.5779\n",
      "Epoch 984/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2682 - accuracy: 0.5531 - val_loss: 1.2074 - val_accuracy: 0.5779\n",
      "Epoch 985/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3016 - accuracy: 0.5410 - val_loss: 1.2065 - val_accuracy: 0.5747\n",
      "Epoch 986/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2664 - accuracy: 0.5850 - val_loss: 1.2045 - val_accuracy: 0.5844\n",
      "Epoch 987/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.2396 - accuracy: 0.5684 - val_loss: 1.2141 - val_accuracy: 0.5877\n",
      "Epoch 988/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2594 - accuracy: 0.5547 - val_loss: 1.2313 - val_accuracy: 0.5779\n",
      "Epoch 989/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2828 - accuracy: 0.5596 - val_loss: 1.2458 - val_accuracy: 0.5747\n",
      "Epoch 990/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2766 - accuracy: 0.5615 - val_loss: 1.2788 - val_accuracy: 0.5584\n",
      "Epoch 991/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.2349 - accuracy: 0.5635 - val_loss: 1.3154 - val_accuracy: 0.5682\n",
      "Epoch 992/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2864 - accuracy: 0.5537 - val_loss: 1.3475 - val_accuracy: 0.5552\n",
      "Epoch 993/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2056 - accuracy: 0.5684 - val_loss: 1.3577 - val_accuracy: 0.5519\n",
      "Epoch 994/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2787 - accuracy: 0.5656 - val_loss: 1.3880 - val_accuracy: 0.5390\n",
      "Epoch 995/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.2270 - accuracy: 0.5752 - val_loss: 1.4091 - val_accuracy: 0.5357\n",
      "Epoch 996/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2299 - accuracy: 0.5791 - val_loss: 1.4271 - val_accuracy: 0.5390\n",
      "Epoch 997/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3195 - accuracy: 0.5503 - val_loss: 1.4356 - val_accuracy: 0.5325\n",
      "Epoch 998/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3091 - accuracy: 0.5419 - val_loss: 1.4181 - val_accuracy: 0.5455\n",
      "Epoch 999/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1796 - accuracy: 0.5830 - val_loss: 1.3944 - val_accuracy: 0.5422\n",
      "Epoch 1000/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2275 - accuracy: 0.5964 - val_loss: 1.3657 - val_accuracy: 0.5422\n",
      "Epoch 1001/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.2496 - accuracy: 0.5605 - val_loss: 1.3458 - val_accuracy: 0.5519\n",
      "Epoch 1002/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3094 - accuracy: 0.5419 - val_loss: 1.3266 - val_accuracy: 0.5617\n",
      "Epoch 1003/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2852 - accuracy: 0.5693 - val_loss: 1.3198 - val_accuracy: 0.5714\n",
      "Epoch 1004/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2692 - accuracy: 0.5559 - val_loss: 1.3230 - val_accuracy: 0.5649\n",
      "Epoch 1005/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2253 - accuracy: 0.5768 - val_loss: 1.3626 - val_accuracy: 0.5617\n",
      "Epoch 1006/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3211 - accuracy: 0.5527 - val_loss: 1.4025 - val_accuracy: 0.5455\n",
      "Epoch 1007/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2109 - accuracy: 0.5908 - val_loss: 1.4461 - val_accuracy: 0.5390\n",
      "Epoch 1008/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2215 - accuracy: 0.5908 - val_loss: 1.4856 - val_accuracy: 0.5260\n",
      "Epoch 1009/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2146 - accuracy: 0.5850 - val_loss: 1.5216 - val_accuracy: 0.5032\n",
      "Epoch 1010/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2612 - accuracy: 0.5605 - val_loss: 1.5464 - val_accuracy: 0.4968\n",
      "Epoch 1011/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2605 - accuracy: 0.5475 - val_loss: 1.5206 - val_accuracy: 0.4968\n",
      "Epoch 1012/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2525 - accuracy: 0.5615 - val_loss: 1.4652 - val_accuracy: 0.5000\n",
      "Epoch 1013/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2098 - accuracy: 0.5967 - val_loss: 1.4040 - val_accuracy: 0.5162\n",
      "Epoch 1014/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2532 - accuracy: 0.5664 - val_loss: 1.3500 - val_accuracy: 0.5487\n",
      "Epoch 1015/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2125 - accuracy: 0.5752 - val_loss: 1.3102 - val_accuracy: 0.5682\n",
      "Epoch 1016/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.2197 - accuracy: 0.5820 - val_loss: 1.2945 - val_accuracy: 0.5649\n",
      "Epoch 1017/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1753 - accuracy: 0.5964 - val_loss: 1.2956 - val_accuracy: 0.5617\n",
      "Epoch 1018/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1910 - accuracy: 0.5964 - val_loss: 1.2753 - val_accuracy: 0.5682\n",
      "Epoch 1019/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2292 - accuracy: 0.5880 - val_loss: 1.2599 - val_accuracy: 0.5682\n",
      "Epoch 1020/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2336 - accuracy: 0.5573 - val_loss: 1.2335 - val_accuracy: 0.5747\n",
      "Epoch 1021/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2254 - accuracy: 0.5752 - val_loss: 1.2215 - val_accuracy: 0.5877\n",
      "Epoch 1022/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.2316 - accuracy: 0.5879 - val_loss: 1.2124 - val_accuracy: 0.5877\n",
      "Epoch 1023/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2390 - accuracy: 0.5573 - val_loss: 1.2050 - val_accuracy: 0.5974\n",
      "Epoch 1024/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2341 - accuracy: 0.5869 - val_loss: 1.2068 - val_accuracy: 0.5909\n",
      "Epoch 1025/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2464 - accuracy: 0.5670 - val_loss: 1.2098 - val_accuracy: 0.5844\n",
      "Epoch 1026/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2463 - accuracy: 0.5811 - val_loss: 1.2191 - val_accuracy: 0.5877\n",
      "Epoch 1027/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2501 - accuracy: 0.5938 - val_loss: 1.2250 - val_accuracy: 0.5909\n",
      "Epoch 1028/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.2204 - accuracy: 0.5967 - val_loss: 1.2247 - val_accuracy: 0.5909\n",
      "Epoch 1029/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2077 - accuracy: 0.5859 - val_loss: 1.2222 - val_accuracy: 0.5909\n",
      "Epoch 1030/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2088 - accuracy: 0.5801 - val_loss: 1.2268 - val_accuracy: 0.5909\n",
      "Epoch 1031/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2132 - accuracy: 0.5824 - val_loss: 1.2318 - val_accuracy: 0.5877\n",
      "Epoch 1032/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2084 - accuracy: 0.5859 - val_loss: 1.2400 - val_accuracy: 0.6006\n",
      "Epoch 1033/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2413 - accuracy: 0.5740 - val_loss: 1.2206 - val_accuracy: 0.5974\n",
      "Epoch 1034/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2341 - accuracy: 0.5781 - val_loss: 1.2012 - val_accuracy: 0.6136\n",
      "Epoch 1035/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2146 - accuracy: 0.5684 - val_loss: 1.1807 - val_accuracy: 0.6136\n",
      "Epoch 1036/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1905 - accuracy: 0.5810 - val_loss: 1.1795 - val_accuracy: 0.6104\n",
      "Epoch 1037/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.2256 - accuracy: 0.5754 - val_loss: 1.1923 - val_accuracy: 0.6234\n",
      "Epoch 1038/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2068 - accuracy: 0.5947 - val_loss: 1.2094 - val_accuracy: 0.6104\n",
      "Epoch 1039/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1566 - accuracy: 0.5967 - val_loss: 1.2216 - val_accuracy: 0.6104\n",
      "Epoch 1040/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2035 - accuracy: 0.5723 - val_loss: 1.2212 - val_accuracy: 0.6136\n",
      "Epoch 1041/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2102 - accuracy: 0.5938 - val_loss: 1.2267 - val_accuracy: 0.6039\n",
      "Epoch 1042/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.3049 - accuracy: 0.5381 - val_loss: 1.2409 - val_accuracy: 0.5877\n",
      "Epoch 1043/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2020 - accuracy: 0.5964 - val_loss: 1.2421 - val_accuracy: 0.5844\n",
      "Epoch 1044/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.1811 - accuracy: 0.6034 - val_loss: 1.2356 - val_accuracy: 0.5877\n",
      "Epoch 1045/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2283 - accuracy: 0.5824 - val_loss: 1.2307 - val_accuracy: 0.5942\n",
      "Epoch 1046/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1843 - accuracy: 0.5824 - val_loss: 1.2222 - val_accuracy: 0.5909\n",
      "Epoch 1047/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2413 - accuracy: 0.5820 - val_loss: 1.2179 - val_accuracy: 0.5877\n",
      "Epoch 1048/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1547 - accuracy: 0.6034 - val_loss: 1.2327 - val_accuracy: 0.5779\n",
      "Epoch 1049/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1930 - accuracy: 0.5918 - val_loss: 1.2416 - val_accuracy: 0.5812\n",
      "Epoch 1050/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2373 - accuracy: 0.5726 - val_loss: 1.2754 - val_accuracy: 0.5779\n",
      "Epoch 1051/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1375 - accuracy: 0.5936 - val_loss: 1.2791 - val_accuracy: 0.5747\n",
      "Epoch 1052/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1452 - accuracy: 0.6064 - val_loss: 1.2705 - val_accuracy: 0.5779\n",
      "Epoch 1053/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2193 - accuracy: 0.5824 - val_loss: 1.2199 - val_accuracy: 0.5909\n",
      "Epoch 1054/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2235 - accuracy: 0.5850 - val_loss: 1.1808 - val_accuracy: 0.5942\n",
      "Epoch 1055/4000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2586 - accuracy: 0.5830 - val_loss: 1.1383 - val_accuracy: 0.6169\n",
      "Epoch 1056/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1504 - accuracy: 0.6094 - val_loss: 1.1146 - val_accuracy: 0.6396\n",
      "Epoch 1057/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1464 - accuracy: 0.6047 - val_loss: 1.1010 - val_accuracy: 0.6429\n",
      "Epoch 1058/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1601 - accuracy: 0.5824 - val_loss: 1.1084 - val_accuracy: 0.6429\n",
      "Epoch 1059/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2111 - accuracy: 0.5889 - val_loss: 1.1201 - val_accuracy: 0.6494\n",
      "Epoch 1060/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2084 - accuracy: 0.5796 - val_loss: 1.1405 - val_accuracy: 0.6429\n",
      "Epoch 1061/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1662 - accuracy: 0.5928 - val_loss: 1.1681 - val_accuracy: 0.6201\n",
      "Epoch 1062/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1768 - accuracy: 0.6064 - val_loss: 1.1934 - val_accuracy: 0.6104\n",
      "Epoch 1063/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2121 - accuracy: 0.5791 - val_loss: 1.2133 - val_accuracy: 0.6039\n",
      "Epoch 1064/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2324 - accuracy: 0.5880 - val_loss: 1.2318 - val_accuracy: 0.5974\n",
      "Epoch 1065/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1184 - accuracy: 0.6133 - val_loss: 1.2224 - val_accuracy: 0.5974\n",
      "Epoch 1066/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1656 - accuracy: 0.6075 - val_loss: 1.1857 - val_accuracy: 0.6071\n",
      "Epoch 1067/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.2457 - accuracy: 0.5596 - val_loss: 1.1480 - val_accuracy: 0.6201\n",
      "Epoch 1068/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2215 - accuracy: 0.5782 - val_loss: 1.1353 - val_accuracy: 0.6136\n",
      "Epoch 1069/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1842 - accuracy: 0.5894 - val_loss: 1.1675 - val_accuracy: 0.6006\n",
      "Epoch 1070/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1944 - accuracy: 0.5768 - val_loss: 1.1901 - val_accuracy: 0.5974\n",
      "Epoch 1071/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1533 - accuracy: 0.6133 - val_loss: 1.2279 - val_accuracy: 0.5974\n",
      "Epoch 1072/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1871 - accuracy: 0.5781 - val_loss: 1.2759 - val_accuracy: 0.5714\n",
      "Epoch 1073/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2172 - accuracy: 0.5918 - val_loss: 1.3023 - val_accuracy: 0.5487\n",
      "Epoch 1074/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1602 - accuracy: 0.6075 - val_loss: 1.3132 - val_accuracy: 0.5455\n",
      "Epoch 1075/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1793 - accuracy: 0.5879 - val_loss: 1.2748 - val_accuracy: 0.5487\n",
      "Epoch 1076/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1855 - accuracy: 0.5922 - val_loss: 1.2248 - val_accuracy: 0.5844\n",
      "Epoch 1077/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1803 - accuracy: 0.6145 - val_loss: 1.1837 - val_accuracy: 0.6006\n",
      "Epoch 1078/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1468 - accuracy: 0.5986 - val_loss: 1.1491 - val_accuracy: 0.6039\n",
      "Epoch 1079/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2290 - accuracy: 0.5852 - val_loss: 1.1247 - val_accuracy: 0.6169\n",
      "Epoch 1080/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1844 - accuracy: 0.5938 - val_loss: 1.1126 - val_accuracy: 0.6331\n",
      "Epoch 1081/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1199 - accuracy: 0.6215 - val_loss: 1.1024 - val_accuracy: 0.6299\n",
      "Epoch 1082/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1666 - accuracy: 0.5879 - val_loss: 1.1049 - val_accuracy: 0.6266\n",
      "Epoch 1083/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2073 - accuracy: 0.5782 - val_loss: 1.1068 - val_accuracy: 0.6201\n",
      "Epoch 1084/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1817 - accuracy: 0.6084 - val_loss: 1.1121 - val_accuracy: 0.6104\n",
      "Epoch 1085/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1556 - accuracy: 0.6089 - val_loss: 1.1186 - val_accuracy: 0.6136\n",
      "Epoch 1086/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1980 - accuracy: 0.5964 - val_loss: 1.1272 - val_accuracy: 0.6104\n",
      "Epoch 1087/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1287 - accuracy: 0.6064 - val_loss: 1.1387 - val_accuracy: 0.6104\n",
      "Epoch 1088/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.2165 - accuracy: 0.5645 - val_loss: 1.1521 - val_accuracy: 0.5974\n",
      "Epoch 1089/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2081 - accuracy: 0.5936 - val_loss: 1.1560 - val_accuracy: 0.5942\n",
      "Epoch 1090/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1520 - accuracy: 0.6074 - val_loss: 1.1616 - val_accuracy: 0.5909\n",
      "Epoch 1091/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.1097 - accuracy: 0.6411 - val_loss: 1.1606 - val_accuracy: 0.5942\n",
      "Epoch 1092/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1542 - accuracy: 0.6035 - val_loss: 1.1517 - val_accuracy: 0.5974\n",
      "Epoch 1093/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1316 - accuracy: 0.6211 - val_loss: 1.1421 - val_accuracy: 0.6006\n",
      "Epoch 1094/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1748 - accuracy: 0.6133 - val_loss: 1.1180 - val_accuracy: 0.6071\n",
      "Epoch 1095/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1326 - accuracy: 0.6025 - val_loss: 1.0961 - val_accuracy: 0.6234\n",
      "Epoch 1096/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2117 - accuracy: 0.5740 - val_loss: 1.0878 - val_accuracy: 0.6201\n",
      "Epoch 1097/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1867 - accuracy: 0.5894 - val_loss: 1.0914 - val_accuracy: 0.6136\n",
      "Epoch 1098/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1341 - accuracy: 0.6103 - val_loss: 1.0949 - val_accuracy: 0.6201\n",
      "Epoch 1099/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1416 - accuracy: 0.6162 - val_loss: 1.0887 - val_accuracy: 0.6299\n",
      "Epoch 1100/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1185 - accuracy: 0.6103 - val_loss: 1.0843 - val_accuracy: 0.6201\n",
      "Epoch 1101/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1187 - accuracy: 0.6103 - val_loss: 1.0835 - val_accuracy: 0.6169\n",
      "Epoch 1102/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1604 - accuracy: 0.6074 - val_loss: 1.0937 - val_accuracy: 0.6136\n",
      "Epoch 1103/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1000 - accuracy: 0.6289 - val_loss: 1.1091 - val_accuracy: 0.6071\n",
      "Epoch 1104/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2400 - accuracy: 0.5503 - val_loss: 1.1244 - val_accuracy: 0.5974\n",
      "Epoch 1105/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1718 - accuracy: 0.6016 - val_loss: 1.1428 - val_accuracy: 0.6006\n",
      "Epoch 1106/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1989 - accuracy: 0.5889 - val_loss: 1.1635 - val_accuracy: 0.6039\n",
      "Epoch 1107/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0755 - accuracy: 0.6215 - val_loss: 1.1907 - val_accuracy: 0.5942\n",
      "Epoch 1108/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0999 - accuracy: 0.6162 - val_loss: 1.2165 - val_accuracy: 0.5844\n",
      "Epoch 1109/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1175 - accuracy: 0.6271 - val_loss: 1.2531 - val_accuracy: 0.5747\n",
      "Epoch 1110/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1693 - accuracy: 0.6145 - val_loss: 1.2761 - val_accuracy: 0.5552\n",
      "Epoch 1111/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0948 - accuracy: 0.6145 - val_loss: 1.2962 - val_accuracy: 0.5617\n",
      "Epoch 1112/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1637 - accuracy: 0.6123 - val_loss: 1.3033 - val_accuracy: 0.5519\n",
      "Epoch 1113/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1495 - accuracy: 0.6240 - val_loss: 1.3104 - val_accuracy: 0.5487\n",
      "Epoch 1114/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1391 - accuracy: 0.6172 - val_loss: 1.2822 - val_accuracy: 0.5584\n",
      "Epoch 1115/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1197 - accuracy: 0.6172 - val_loss: 1.2699 - val_accuracy: 0.5519\n",
      "Epoch 1116/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2109 - accuracy: 0.6020 - val_loss: 1.2795 - val_accuracy: 0.5682\n",
      "Epoch 1117/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1681 - accuracy: 0.5894 - val_loss: 1.3234 - val_accuracy: 0.5682\n",
      "Epoch 1118/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0816 - accuracy: 0.6104 - val_loss: 1.3892 - val_accuracy: 0.5519\n",
      "Epoch 1119/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1038 - accuracy: 0.6133 - val_loss: 1.4714 - val_accuracy: 0.5390\n",
      "Epoch 1120/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0930 - accuracy: 0.6313 - val_loss: 1.5615 - val_accuracy: 0.5162\n",
      "Epoch 1121/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0933 - accuracy: 0.6377 - val_loss: 1.6096 - val_accuracy: 0.5227\n",
      "Epoch 1122/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1404 - accuracy: 0.6201 - val_loss: 1.6209 - val_accuracy: 0.5097\n",
      "Epoch 1123/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.1227 - accuracy: 0.6020 - val_loss: 1.6042 - val_accuracy: 0.5097\n",
      "Epoch 1124/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1682 - accuracy: 0.6104 - val_loss: 1.5164 - val_accuracy: 0.5195\n",
      "Epoch 1125/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.1325 - accuracy: 0.6173 - val_loss: 1.4168 - val_accuracy: 0.5519\n",
      "Epoch 1126/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1113 - accuracy: 0.6201 - val_loss: 1.3176 - val_accuracy: 0.5682\n",
      "Epoch 1127/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1382 - accuracy: 0.6299 - val_loss: 1.2583 - val_accuracy: 0.5844\n",
      "Epoch 1128/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0894 - accuracy: 0.6145 - val_loss: 1.2020 - val_accuracy: 0.5909\n",
      "Epoch 1129/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1406 - accuracy: 0.6143 - val_loss: 1.1726 - val_accuracy: 0.5974\n",
      "Epoch 1130/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2017 - accuracy: 0.5908 - val_loss: 1.1679 - val_accuracy: 0.5877\n",
      "Epoch 1131/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1071 - accuracy: 0.6191 - val_loss: 1.1714 - val_accuracy: 0.5942\n",
      "Epoch 1132/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0936 - accuracy: 0.6162 - val_loss: 1.1613 - val_accuracy: 0.5974\n",
      "Epoch 1133/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.1135 - accuracy: 0.6243 - val_loss: 1.1484 - val_accuracy: 0.5974\n",
      "Epoch 1134/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0885 - accuracy: 0.6445 - val_loss: 1.1313 - val_accuracy: 0.6006\n",
      "Epoch 1135/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1138 - accuracy: 0.6313 - val_loss: 1.1324 - val_accuracy: 0.5942\n",
      "Epoch 1136/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1588 - accuracy: 0.5838 - val_loss: 1.1542 - val_accuracy: 0.5942\n",
      "Epoch 1137/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1655 - accuracy: 0.6094 - val_loss: 1.1956 - val_accuracy: 0.5844\n",
      "Epoch 1138/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1088 - accuracy: 0.6172 - val_loss: 1.2373 - val_accuracy: 0.5649\n",
      "Epoch 1139/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1132 - accuracy: 0.6211 - val_loss: 1.2824 - val_accuracy: 0.5487\n",
      "Epoch 1140/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0894 - accuracy: 0.6215 - val_loss: 1.3260 - val_accuracy: 0.5390\n",
      "Epoch 1141/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1443 - accuracy: 0.5922 - val_loss: 1.3226 - val_accuracy: 0.5390\n",
      "Epoch 1142/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0452 - accuracy: 0.6367 - val_loss: 1.2961 - val_accuracy: 0.5357\n",
      "Epoch 1143/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1183 - accuracy: 0.6123 - val_loss: 1.2720 - val_accuracy: 0.5455\n",
      "Epoch 1144/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1568 - accuracy: 0.5936 - val_loss: 1.2268 - val_accuracy: 0.5682\n",
      "Epoch 1145/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1254 - accuracy: 0.6257 - val_loss: 1.1817 - val_accuracy: 0.5747\n",
      "Epoch 1146/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1072 - accuracy: 0.6243 - val_loss: 1.1469 - val_accuracy: 0.5942\n",
      "Epoch 1147/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1222 - accuracy: 0.6229 - val_loss: 1.1315 - val_accuracy: 0.5974\n",
      "Epoch 1148/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1087 - accuracy: 0.6377 - val_loss: 1.1400 - val_accuracy: 0.6006\n",
      "Epoch 1149/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1174 - accuracy: 0.6075 - val_loss: 1.1578 - val_accuracy: 0.6006\n",
      "Epoch 1150/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0727 - accuracy: 0.6299 - val_loss: 1.1622 - val_accuracy: 0.6039\n",
      "Epoch 1151/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1116 - accuracy: 0.6341 - val_loss: 1.1592 - val_accuracy: 0.6136\n",
      "Epoch 1152/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1283 - accuracy: 0.6094 - val_loss: 1.1501 - val_accuracy: 0.6039\n",
      "Epoch 1153/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0821 - accuracy: 0.6257 - val_loss: 1.1577 - val_accuracy: 0.6136\n",
      "Epoch 1154/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1092 - accuracy: 0.6260 - val_loss: 1.1685 - val_accuracy: 0.5942\n",
      "Epoch 1155/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1123 - accuracy: 0.6211 - val_loss: 1.1631 - val_accuracy: 0.5909\n",
      "Epoch 1156/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0523 - accuracy: 0.6466 - val_loss: 1.1604 - val_accuracy: 0.5942\n",
      "Epoch 1157/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0600 - accuracy: 0.6270 - val_loss: 1.1506 - val_accuracy: 0.6104\n",
      "Epoch 1158/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0771 - accuracy: 0.6285 - val_loss: 1.1488 - val_accuracy: 0.6136\n",
      "Epoch 1159/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0744 - accuracy: 0.6230 - val_loss: 1.1392 - val_accuracy: 0.6234\n",
      "Epoch 1160/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1228 - accuracy: 0.6369 - val_loss: 1.1312 - val_accuracy: 0.6169\n",
      "Epoch 1161/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0662 - accuracy: 0.6309 - val_loss: 1.1282 - val_accuracy: 0.5942\n",
      "Epoch 1162/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.1267 - accuracy: 0.6215 - val_loss: 1.1308 - val_accuracy: 0.5747\n",
      "Epoch 1163/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0885 - accuracy: 0.6230 - val_loss: 1.1315 - val_accuracy: 0.5747\n",
      "Epoch 1164/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0370 - accuracy: 0.6338 - val_loss: 1.1388 - val_accuracy: 0.5812\n",
      "Epoch 1165/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0981 - accuracy: 0.6285 - val_loss: 1.1546 - val_accuracy: 0.5779\n",
      "Epoch 1166/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0811 - accuracy: 0.6211 - val_loss: 1.1694 - val_accuracy: 0.5714\n",
      "Epoch 1167/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1701 - accuracy: 0.6006 - val_loss: 1.1787 - val_accuracy: 0.5747\n",
      "Epoch 1168/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1059 - accuracy: 0.6494 - val_loss: 1.1804 - val_accuracy: 0.5779\n",
      "Epoch 1169/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0844 - accuracy: 0.6369 - val_loss: 1.1846 - val_accuracy: 0.5779\n",
      "Epoch 1170/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.0303 - accuracy: 0.6504 - val_loss: 1.1970 - val_accuracy: 0.5844\n",
      "Epoch 1171/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0819 - accuracy: 0.6426 - val_loss: 1.2008 - val_accuracy: 0.5909\n",
      "Epoch 1172/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.1226 - accuracy: 0.6162 - val_loss: 1.1930 - val_accuracy: 0.5779\n",
      "Epoch 1173/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0357 - accuracy: 0.6550 - val_loss: 1.1889 - val_accuracy: 0.5844\n",
      "Epoch 1174/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0722 - accuracy: 0.6397 - val_loss: 1.1821 - val_accuracy: 0.5747\n",
      "Epoch 1175/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0585 - accuracy: 0.6426 - val_loss: 1.1738 - val_accuracy: 0.5779\n",
      "Epoch 1176/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0388 - accuracy: 0.6536 - val_loss: 1.1643 - val_accuracy: 0.5877\n",
      "Epoch 1177/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1054 - accuracy: 0.6240 - val_loss: 1.1613 - val_accuracy: 0.5974\n",
      "Epoch 1178/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0460 - accuracy: 0.6387 - val_loss: 1.1633 - val_accuracy: 0.5942\n",
      "Epoch 1179/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1101 - accuracy: 0.6270 - val_loss: 1.1798 - val_accuracy: 0.5812\n",
      "Epoch 1180/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.1432 - accuracy: 0.6327 - val_loss: 1.1917 - val_accuracy: 0.5747\n",
      "Epoch 1181/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0415 - accuracy: 0.6439 - val_loss: 1.1889 - val_accuracy: 0.6104\n",
      "Epoch 1182/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0912 - accuracy: 0.6387 - val_loss: 1.1673 - val_accuracy: 0.6104\n",
      "Epoch 1183/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1005 - accuracy: 0.6131 - val_loss: 1.1425 - val_accuracy: 0.6169\n",
      "Epoch 1184/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0978 - accuracy: 0.6328 - val_loss: 1.1180 - val_accuracy: 0.6234\n",
      "Epoch 1185/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.0899 - accuracy: 0.6289 - val_loss: 1.0981 - val_accuracy: 0.6039\n",
      "Epoch 1186/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0244 - accuracy: 0.6508 - val_loss: 1.0873 - val_accuracy: 0.6169\n",
      "Epoch 1187/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1087 - accuracy: 0.6240 - val_loss: 1.1045 - val_accuracy: 0.6169\n",
      "Epoch 1188/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.0618 - accuracy: 0.6348 - val_loss: 1.1269 - val_accuracy: 0.6104\n",
      "Epoch 1189/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.0734 - accuracy: 0.6260 - val_loss: 1.1488 - val_accuracy: 0.5942\n",
      "Epoch 1190/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.0806 - accuracy: 0.6289 - val_loss: 1.1920 - val_accuracy: 0.5974\n",
      "Epoch 1191/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.0684 - accuracy: 0.6377 - val_loss: 1.2183 - val_accuracy: 0.5909\n",
      "Epoch 1192/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0424 - accuracy: 0.6369 - val_loss: 1.2524 - val_accuracy: 0.5877\n",
      "Epoch 1193/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0467 - accuracy: 0.6426 - val_loss: 1.2665 - val_accuracy: 0.5844\n",
      "Epoch 1194/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0370 - accuracy: 0.6536 - val_loss: 1.2554 - val_accuracy: 0.5844\n",
      "Epoch 1195/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0374 - accuracy: 0.6480 - val_loss: 1.2496 - val_accuracy: 0.5877\n",
      "Epoch 1196/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0797 - accuracy: 0.6113 - val_loss: 1.2572 - val_accuracy: 0.5844\n",
      "Epoch 1197/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0541 - accuracy: 0.6543 - val_loss: 1.2729 - val_accuracy: 0.5812\n",
      "Epoch 1198/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0505 - accuracy: 0.6355 - val_loss: 1.3056 - val_accuracy: 0.5649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1199/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1041 - accuracy: 0.6191 - val_loss: 1.3476 - val_accuracy: 0.5714\n",
      "Epoch 1200/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0735 - accuracy: 0.6348 - val_loss: 1.3675 - val_accuracy: 0.5617\n",
      "Epoch 1201/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0849 - accuracy: 0.6338 - val_loss: 1.3686 - val_accuracy: 0.5584\n",
      "Epoch 1202/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0975 - accuracy: 0.6445 - val_loss: 1.3793 - val_accuracy: 0.5552\n",
      "Epoch 1203/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0328 - accuracy: 0.6523 - val_loss: 1.3727 - val_accuracy: 0.5617\n",
      "Epoch 1204/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0505 - accuracy: 0.6257 - val_loss: 1.3932 - val_accuracy: 0.5455\n",
      "Epoch 1205/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0169 - accuracy: 0.6522 - val_loss: 1.4052 - val_accuracy: 0.5455\n",
      "Epoch 1206/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0235 - accuracy: 0.6465 - val_loss: 1.4137 - val_accuracy: 0.5390\n",
      "Epoch 1207/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0551 - accuracy: 0.6367 - val_loss: 1.4393 - val_accuracy: 0.5357\n",
      "Epoch 1208/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0090 - accuracy: 0.6522 - val_loss: 1.4223 - val_accuracy: 0.5325\n",
      "Epoch 1209/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1371 - accuracy: 0.6182 - val_loss: 1.3974 - val_accuracy: 0.5390\n",
      "Epoch 1210/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0428 - accuracy: 0.6411 - val_loss: 1.3710 - val_accuracy: 0.5422\n",
      "Epoch 1211/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1033 - accuracy: 0.6250 - val_loss: 1.3407 - val_accuracy: 0.5552\n",
      "Epoch 1212/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1013 - accuracy: 0.6172 - val_loss: 1.3072 - val_accuracy: 0.5682\n",
      "Epoch 1213/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0115 - accuracy: 0.6704 - val_loss: 1.3113 - val_accuracy: 0.5714\n",
      "Epoch 1214/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0264 - accuracy: 0.6357 - val_loss: 1.3278 - val_accuracy: 0.5682\n",
      "Epoch 1215/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0848 - accuracy: 0.6367 - val_loss: 1.3459 - val_accuracy: 0.5487\n",
      "Epoch 1216/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0575 - accuracy: 0.6465 - val_loss: 1.3490 - val_accuracy: 0.5487\n",
      "Epoch 1217/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0033 - accuracy: 0.6662 - val_loss: 1.3412 - val_accuracy: 0.5455\n",
      "Epoch 1218/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0377 - accuracy: 0.6411 - val_loss: 1.3149 - val_accuracy: 0.5552\n",
      "Epoch 1219/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0666 - accuracy: 0.6355 - val_loss: 1.2897 - val_accuracy: 0.5519\n",
      "Epoch 1220/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0136 - accuracy: 0.6550 - val_loss: 1.2604 - val_accuracy: 0.5682\n",
      "Epoch 1221/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9955 - accuracy: 0.6650 - val_loss: 1.2333 - val_accuracy: 0.5682\n",
      "Epoch 1222/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0765 - accuracy: 0.6318 - val_loss: 1.2057 - val_accuracy: 0.5779\n",
      "Epoch 1223/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0546 - accuracy: 0.6387 - val_loss: 1.1909 - val_accuracy: 0.5844\n",
      "Epoch 1224/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0430 - accuracy: 0.6455 - val_loss: 1.2011 - val_accuracy: 0.5812\n",
      "Epoch 1225/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0412 - accuracy: 0.6522 - val_loss: 1.2430 - val_accuracy: 0.5747\n",
      "Epoch 1226/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0388 - accuracy: 0.6533 - val_loss: 1.2949 - val_accuracy: 0.5649\n",
      "Epoch 1227/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0026 - accuracy: 0.6611 - val_loss: 1.3389 - val_accuracy: 0.5487\n",
      "Epoch 1228/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0843 - accuracy: 0.6355 - val_loss: 1.3568 - val_accuracy: 0.5519\n",
      "Epoch 1229/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0324 - accuracy: 0.6411 - val_loss: 1.3612 - val_accuracy: 0.5487\n",
      "Epoch 1230/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0718 - accuracy: 0.6367 - val_loss: 1.3389 - val_accuracy: 0.5552\n",
      "Epoch 1231/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0991 - accuracy: 0.6313 - val_loss: 1.3441 - val_accuracy: 0.5617\n",
      "Epoch 1232/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0451 - accuracy: 0.6348 - val_loss: 1.3693 - val_accuracy: 0.5552\n",
      "Epoch 1233/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0908 - accuracy: 0.6466 - val_loss: 1.3741 - val_accuracy: 0.5552\n",
      "Epoch 1234/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0111 - accuracy: 0.6709 - val_loss: 1.4032 - val_accuracy: 0.5519\n",
      "Epoch 1235/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0466 - accuracy: 0.6377 - val_loss: 1.4311 - val_accuracy: 0.5357\n",
      "Epoch 1236/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0357 - accuracy: 0.6453 - val_loss: 1.4237 - val_accuracy: 0.5357\n",
      "Epoch 1237/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9842 - accuracy: 0.6729 - val_loss: 1.4091 - val_accuracy: 0.5455\n",
      "Epoch 1238/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0321 - accuracy: 0.6453 - val_loss: 1.3565 - val_accuracy: 0.5617\n",
      "Epoch 1239/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0915 - accuracy: 0.6338 - val_loss: 1.3121 - val_accuracy: 0.5682\n",
      "Epoch 1240/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0205 - accuracy: 0.6536 - val_loss: 1.2630 - val_accuracy: 0.5747\n",
      "Epoch 1241/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0268 - accuracy: 0.6508 - val_loss: 1.2469 - val_accuracy: 0.5779\n",
      "Epoch 1242/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0682 - accuracy: 0.6285 - val_loss: 1.2306 - val_accuracy: 0.5747\n",
      "Epoch 1243/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0793 - accuracy: 0.6313 - val_loss: 1.2186 - val_accuracy: 0.5877\n",
      "Epoch 1244/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9883 - accuracy: 0.6650 - val_loss: 1.2180 - val_accuracy: 0.5877\n",
      "Epoch 1245/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0763 - accuracy: 0.6327 - val_loss: 1.2305 - val_accuracy: 0.5844\n",
      "Epoch 1246/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0585 - accuracy: 0.6411 - val_loss: 1.2647 - val_accuracy: 0.5779\n",
      "Epoch 1247/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0076 - accuracy: 0.6523 - val_loss: 1.2985 - val_accuracy: 0.5779\n",
      "Epoch 1248/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0817 - accuracy: 0.6250 - val_loss: 1.3202 - val_accuracy: 0.5812\n",
      "Epoch 1249/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0303 - accuracy: 0.6411 - val_loss: 1.3177 - val_accuracy: 0.5747\n",
      "Epoch 1250/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0017 - accuracy: 0.6582 - val_loss: 1.3021 - val_accuracy: 0.5682\n",
      "Epoch 1251/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0408 - accuracy: 0.6872 - val_loss: 1.2974 - val_accuracy: 0.5617\n",
      "Epoch 1252/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0425 - accuracy: 0.6550 - val_loss: 1.2969 - val_accuracy: 0.5422\n",
      "Epoch 1253/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0416 - accuracy: 0.6522 - val_loss: 1.3080 - val_accuracy: 0.5292\n",
      "Epoch 1254/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0324 - accuracy: 0.6564 - val_loss: 1.2976 - val_accuracy: 0.5390\n",
      "Epoch 1255/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1154 - accuracy: 0.6348 - val_loss: 1.2649 - val_accuracy: 0.5519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1256/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9710 - accuracy: 0.6660 - val_loss: 1.2305 - val_accuracy: 0.5682\n",
      "Epoch 1257/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0238 - accuracy: 0.6606 - val_loss: 1.2512 - val_accuracy: 0.5812\n",
      "Epoch 1258/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.0424 - accuracy: 0.6494 - val_loss: 1.2700 - val_accuracy: 0.5779\n",
      "Epoch 1259/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1036 - accuracy: 0.6055 - val_loss: 1.2601 - val_accuracy: 0.5779\n",
      "Epoch 1260/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0136 - accuracy: 0.6634 - val_loss: 1.2257 - val_accuracy: 0.5942\n",
      "Epoch 1261/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9823 - accuracy: 0.6816 - val_loss: 1.1874 - val_accuracy: 0.5974\n",
      "Epoch 1262/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0253 - accuracy: 0.6453 - val_loss: 1.1527 - val_accuracy: 0.5942\n",
      "Epoch 1263/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0434 - accuracy: 0.6533 - val_loss: 1.1384 - val_accuracy: 0.5974\n",
      "Epoch 1264/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.0067 - accuracy: 0.6504 - val_loss: 1.1308 - val_accuracy: 0.5909\n",
      "Epoch 1265/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0381 - accuracy: 0.6299 - val_loss: 1.1456 - val_accuracy: 0.6104\n",
      "Epoch 1266/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0040 - accuracy: 0.6572 - val_loss: 1.1615 - val_accuracy: 0.6071\n",
      "Epoch 1267/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9896 - accuracy: 0.6660 - val_loss: 1.1733 - val_accuracy: 0.5909\n",
      "Epoch 1268/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.9906 - accuracy: 0.6729 - val_loss: 1.1880 - val_accuracy: 0.5942\n",
      "Epoch 1269/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9883 - accuracy: 0.6816 - val_loss: 1.1986 - val_accuracy: 0.5844\n",
      "Epoch 1270/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0412 - accuracy: 0.6406 - val_loss: 1.1911 - val_accuracy: 0.5812\n",
      "Epoch 1271/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0655 - accuracy: 0.6397 - val_loss: 1.1705 - val_accuracy: 0.5877\n",
      "Epoch 1272/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0183 - accuracy: 0.6508 - val_loss: 1.1399 - val_accuracy: 0.5974\n",
      "Epoch 1273/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9494 - accuracy: 0.6680 - val_loss: 1.1046 - val_accuracy: 0.6071\n",
      "Epoch 1274/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0437 - accuracy: 0.6369 - val_loss: 1.0852 - val_accuracy: 0.6104\n",
      "Epoch 1275/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0316 - accuracy: 0.6536 - val_loss: 1.0759 - val_accuracy: 0.6104\n",
      "Epoch 1276/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0409 - accuracy: 0.6313 - val_loss: 1.0758 - val_accuracy: 0.6039\n",
      "Epoch 1277/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0658 - accuracy: 0.6494 - val_loss: 1.0758 - val_accuracy: 0.6136\n",
      "Epoch 1278/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9940 - accuracy: 0.6662 - val_loss: 1.0959 - val_accuracy: 0.6234\n",
      "Epoch 1279/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9522 - accuracy: 0.6865 - val_loss: 1.1112 - val_accuracy: 0.6136\n",
      "Epoch 1280/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9935 - accuracy: 0.6631 - val_loss: 1.1101 - val_accuracy: 0.6234\n",
      "Epoch 1281/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9629 - accuracy: 0.6788 - val_loss: 1.0902 - val_accuracy: 0.6266\n",
      "Epoch 1282/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.0311 - accuracy: 0.6660 - val_loss: 1.0708 - val_accuracy: 0.6331\n",
      "Epoch 1283/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9700 - accuracy: 0.6453 - val_loss: 1.0571 - val_accuracy: 0.6364\n",
      "Epoch 1284/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9929 - accuracy: 0.6680 - val_loss: 1.0640 - val_accuracy: 0.6299\n",
      "Epoch 1285/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0177 - accuracy: 0.6648 - val_loss: 1.0841 - val_accuracy: 0.6169\n",
      "Epoch 1286/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0257 - accuracy: 0.6578 - val_loss: 1.1142 - val_accuracy: 0.6201\n",
      "Epoch 1287/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0137 - accuracy: 0.6436 - val_loss: 1.1546 - val_accuracy: 0.6006\n",
      "Epoch 1288/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9735 - accuracy: 0.6662 - val_loss: 1.2072 - val_accuracy: 0.6006\n",
      "Epoch 1289/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9555 - accuracy: 0.6924 - val_loss: 1.2431 - val_accuracy: 0.6071\n",
      "Epoch 1290/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9863 - accuracy: 0.6690 - val_loss: 1.2512 - val_accuracy: 0.5974\n",
      "Epoch 1291/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9833 - accuracy: 0.6484 - val_loss: 1.2292 - val_accuracy: 0.6071\n",
      "Epoch 1292/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.9950 - accuracy: 0.6572 - val_loss: 1.2171 - val_accuracy: 0.5942\n",
      "Epoch 1293/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0168 - accuracy: 0.6550 - val_loss: 1.2084 - val_accuracy: 0.6006\n",
      "Epoch 1294/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9739 - accuracy: 0.6955 - val_loss: 1.1851 - val_accuracy: 0.6201\n",
      "Epoch 1295/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9530 - accuracy: 0.6872 - val_loss: 1.1399 - val_accuracy: 0.6201\n",
      "Epoch 1296/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0168 - accuracy: 0.6621 - val_loss: 1.1148 - val_accuracy: 0.6136\n",
      "Epoch 1297/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9978 - accuracy: 0.6611 - val_loss: 1.1065 - val_accuracy: 0.6136\n",
      "Epoch 1298/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9997 - accuracy: 0.6578 - val_loss: 1.1015 - val_accuracy: 0.6201\n",
      "Epoch 1299/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9799 - accuracy: 0.6802 - val_loss: 1.1128 - val_accuracy: 0.6201\n",
      "Epoch 1300/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9750 - accuracy: 0.6738 - val_loss: 1.1304 - val_accuracy: 0.6136\n",
      "Epoch 1301/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9795 - accuracy: 0.6760 - val_loss: 1.1459 - val_accuracy: 0.6071\n",
      "Epoch 1302/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9485 - accuracy: 0.6758 - val_loss: 1.1549 - val_accuracy: 0.6039\n",
      "Epoch 1303/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9432 - accuracy: 0.6760 - val_loss: 1.1561 - val_accuracy: 0.6006\n",
      "Epoch 1304/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0031 - accuracy: 0.6611 - val_loss: 1.1693 - val_accuracy: 0.5974\n",
      "Epoch 1305/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0025 - accuracy: 0.6641 - val_loss: 1.1589 - val_accuracy: 0.6039\n",
      "Epoch 1306/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9675 - accuracy: 0.6631 - val_loss: 1.1347 - val_accuracy: 0.6136\n",
      "Epoch 1307/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9737 - accuracy: 0.6777 - val_loss: 1.1015 - val_accuracy: 0.6299\n",
      "Epoch 1308/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9655 - accuracy: 0.6797 - val_loss: 1.0749 - val_accuracy: 0.6364\n",
      "Epoch 1309/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9393 - accuracy: 0.6875 - val_loss: 1.0584 - val_accuracy: 0.6429\n",
      "Epoch 1310/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9726 - accuracy: 0.6592 - val_loss: 1.0465 - val_accuracy: 0.6396\n",
      "Epoch 1311/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9466 - accuracy: 0.6844 - val_loss: 1.0427 - val_accuracy: 0.6494\n",
      "Epoch 1312/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9486 - accuracy: 0.6865 - val_loss: 1.0410 - val_accuracy: 0.6461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1313/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9726 - accuracy: 0.6816 - val_loss: 1.0493 - val_accuracy: 0.6266\n",
      "Epoch 1314/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0321 - accuracy: 0.6602 - val_loss: 1.0572 - val_accuracy: 0.6266\n",
      "Epoch 1315/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9759 - accuracy: 0.6699 - val_loss: 1.0698 - val_accuracy: 0.6364\n",
      "Epoch 1316/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9165 - accuracy: 0.6836 - val_loss: 1.0696 - val_accuracy: 0.6299\n",
      "Epoch 1317/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9605 - accuracy: 0.6924 - val_loss: 1.0667 - val_accuracy: 0.6364\n",
      "Epoch 1318/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9307 - accuracy: 0.6758 - val_loss: 1.0716 - val_accuracy: 0.6364\n",
      "Epoch 1319/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9961 - accuracy: 0.6718 - val_loss: 1.0687 - val_accuracy: 0.6429\n",
      "Epoch 1320/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9725 - accuracy: 0.6718 - val_loss: 1.0691 - val_accuracy: 0.6331\n",
      "Epoch 1321/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9605 - accuracy: 0.6802 - val_loss: 1.0539 - val_accuracy: 0.6364\n",
      "Epoch 1322/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9323 - accuracy: 0.6768 - val_loss: 1.0502 - val_accuracy: 0.6266\n",
      "Epoch 1323/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9543 - accuracy: 0.6858 - val_loss: 1.0570 - val_accuracy: 0.6266\n",
      "Epoch 1324/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9393 - accuracy: 0.6844 - val_loss: 1.0673 - val_accuracy: 0.6299\n",
      "Epoch 1325/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9339 - accuracy: 0.6746 - val_loss: 1.0852 - val_accuracy: 0.6234\n",
      "Epoch 1326/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.0095 - accuracy: 0.6504 - val_loss: 1.0984 - val_accuracy: 0.6169\n",
      "Epoch 1327/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9856 - accuracy: 0.6729 - val_loss: 1.0902 - val_accuracy: 0.6039\n",
      "Epoch 1328/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9673 - accuracy: 0.6621 - val_loss: 1.0793 - val_accuracy: 0.6006\n",
      "Epoch 1329/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9301 - accuracy: 0.6941 - val_loss: 1.0763 - val_accuracy: 0.6201\n",
      "Epoch 1330/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9668 - accuracy: 0.6830 - val_loss: 1.0749 - val_accuracy: 0.6234\n",
      "Epoch 1331/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9747 - accuracy: 0.6729 - val_loss: 1.0815 - val_accuracy: 0.6234\n",
      "Epoch 1332/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9257 - accuracy: 0.6969 - val_loss: 1.0964 - val_accuracy: 0.6071\n",
      "Epoch 1333/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0539 - accuracy: 0.6328 - val_loss: 1.1074 - val_accuracy: 0.6104\n",
      "Epoch 1334/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9592 - accuracy: 0.6774 - val_loss: 1.1167 - val_accuracy: 0.6104\n",
      "Epoch 1335/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9716 - accuracy: 0.6592 - val_loss: 1.1390 - val_accuracy: 0.6104\n",
      "Epoch 1336/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9502 - accuracy: 0.6816 - val_loss: 1.1435 - val_accuracy: 0.6071\n",
      "Epoch 1337/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9798 - accuracy: 0.6872 - val_loss: 1.1409 - val_accuracy: 0.6136\n",
      "Epoch 1338/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9908 - accuracy: 0.6606 - val_loss: 1.1430 - val_accuracy: 0.6169\n",
      "Epoch 1339/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.9257 - accuracy: 0.6844 - val_loss: 1.1365 - val_accuracy: 0.6104\n",
      "Epoch 1340/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9422 - accuracy: 0.6895 - val_loss: 1.1272 - val_accuracy: 0.6169\n",
      "Epoch 1341/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9653 - accuracy: 0.6621 - val_loss: 1.1096 - val_accuracy: 0.6201\n",
      "Epoch 1342/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9158 - accuracy: 0.6963 - val_loss: 1.0835 - val_accuracy: 0.6331\n",
      "Epoch 1343/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.9845 - accuracy: 0.6689 - val_loss: 1.0830 - val_accuracy: 0.6364\n",
      "Epoch 1344/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8784 - accuracy: 0.7123 - val_loss: 1.0837 - val_accuracy: 0.6299\n",
      "Epoch 1345/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9445 - accuracy: 0.6872 - val_loss: 1.0930 - val_accuracy: 0.6396\n",
      "Epoch 1346/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9643 - accuracy: 0.6802 - val_loss: 1.0989 - val_accuracy: 0.6266\n",
      "Epoch 1347/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0106 - accuracy: 0.6582 - val_loss: 1.1112 - val_accuracy: 0.6234\n",
      "Epoch 1348/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9384 - accuracy: 0.6858 - val_loss: 1.1537 - val_accuracy: 0.6136\n",
      "Epoch 1349/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9545 - accuracy: 0.6885 - val_loss: 1.2026 - val_accuracy: 0.6039\n",
      "Epoch 1350/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9433 - accuracy: 0.6620 - val_loss: 1.2300 - val_accuracy: 0.5974\n",
      "Epoch 1351/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9353 - accuracy: 0.6885 - val_loss: 1.2798 - val_accuracy: 0.5877\n",
      "Epoch 1352/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9376 - accuracy: 0.6914 - val_loss: 1.3333 - val_accuracy: 0.5779\n",
      "Epoch 1353/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9750 - accuracy: 0.6564 - val_loss: 1.3509 - val_accuracy: 0.5617\n",
      "Epoch 1354/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9598 - accuracy: 0.6621 - val_loss: 1.3297 - val_accuracy: 0.5487\n",
      "Epoch 1355/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9071 - accuracy: 0.6973 - val_loss: 1.2970 - val_accuracy: 0.5487\n",
      "Epoch 1356/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9436 - accuracy: 0.6927 - val_loss: 1.2562 - val_accuracy: 0.5584\n",
      "Epoch 1357/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9916 - accuracy: 0.6494 - val_loss: 1.2166 - val_accuracy: 0.5812\n",
      "Epoch 1358/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.9619 - accuracy: 0.6836 - val_loss: 1.1843 - val_accuracy: 0.5779\n",
      "Epoch 1359/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9463 - accuracy: 0.6830 - val_loss: 1.1707 - val_accuracy: 0.5844\n",
      "Epoch 1360/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9711 - accuracy: 0.6816 - val_loss: 1.1553 - val_accuracy: 0.6039\n",
      "Epoch 1361/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9865 - accuracy: 0.6592 - val_loss: 1.1503 - val_accuracy: 0.6234\n",
      "Epoch 1362/4000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.9266 - accuracy: 0.6807 - val_loss: 1.1422 - val_accuracy: 0.6169\n",
      "Epoch 1363/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9628 - accuracy: 0.6983 - val_loss: 1.1501 - val_accuracy: 0.6169\n",
      "Epoch 1364/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9037 - accuracy: 0.6914 - val_loss: 1.1645 - val_accuracy: 0.6071\n",
      "Epoch 1365/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9316 - accuracy: 0.6904 - val_loss: 1.1633 - val_accuracy: 0.6136\n",
      "Epoch 1366/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0076 - accuracy: 0.6602 - val_loss: 1.1713 - val_accuracy: 0.6136\n",
      "Epoch 1367/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9260 - accuracy: 0.6826 - val_loss: 1.1902 - val_accuracy: 0.5909\n",
      "Epoch 1368/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8984 - accuracy: 0.7053 - val_loss: 1.1988 - val_accuracy: 0.5909\n",
      "Epoch 1369/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9653 - accuracy: 0.6855 - val_loss: 1.1971 - val_accuracy: 0.5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1370/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9536 - accuracy: 0.6836 - val_loss: 1.1855 - val_accuracy: 0.5909\n",
      "Epoch 1371/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8744 - accuracy: 0.6941 - val_loss: 1.1664 - val_accuracy: 0.5877\n",
      "Epoch 1372/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9765 - accuracy: 0.6680 - val_loss: 1.1423 - val_accuracy: 0.5909\n",
      "Epoch 1373/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.9388 - accuracy: 0.6760 - val_loss: 1.1295 - val_accuracy: 0.6006\n",
      "Epoch 1374/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9582 - accuracy: 0.6774 - val_loss: 1.1451 - val_accuracy: 0.5974\n",
      "Epoch 1375/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9122 - accuracy: 0.6826 - val_loss: 1.1526 - val_accuracy: 0.5974\n",
      "Epoch 1376/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9427 - accuracy: 0.6816 - val_loss: 1.1432 - val_accuracy: 0.6136\n",
      "Epoch 1377/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9284 - accuracy: 0.6895 - val_loss: 1.1468 - val_accuracy: 0.6169\n",
      "Epoch 1378/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9498 - accuracy: 0.6846 - val_loss: 1.1700 - val_accuracy: 0.6071\n",
      "Epoch 1379/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9552 - accuracy: 0.6797 - val_loss: 1.2065 - val_accuracy: 0.6006\n",
      "Epoch 1380/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9377 - accuracy: 0.6826 - val_loss: 1.2155 - val_accuracy: 0.6039\n",
      "Epoch 1381/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8888 - accuracy: 0.6924 - val_loss: 1.2180 - val_accuracy: 0.6104\n",
      "Epoch 1382/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9844 - accuracy: 0.6826 - val_loss: 1.2309 - val_accuracy: 0.6039\n",
      "Epoch 1383/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0134 - accuracy: 0.6746 - val_loss: 1.2137 - val_accuracy: 0.6039\n",
      "Epoch 1384/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8902 - accuracy: 0.7039 - val_loss: 1.1814 - val_accuracy: 0.6201\n",
      "Epoch 1385/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8643 - accuracy: 0.7151 - val_loss: 1.1582 - val_accuracy: 0.6201\n",
      "Epoch 1386/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8888 - accuracy: 0.6885 - val_loss: 1.1484 - val_accuracy: 0.6299\n",
      "Epoch 1387/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8558 - accuracy: 0.7053 - val_loss: 1.1436 - val_accuracy: 0.6299\n",
      "Epoch 1388/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9920 - accuracy: 0.6611 - val_loss: 1.1386 - val_accuracy: 0.6331\n",
      "Epoch 1389/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9511 - accuracy: 0.6924 - val_loss: 1.1259 - val_accuracy: 0.6299\n",
      "Epoch 1390/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9442 - accuracy: 0.6816 - val_loss: 1.1137 - val_accuracy: 0.6234\n",
      "Epoch 1391/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9256 - accuracy: 0.6875 - val_loss: 1.1062 - val_accuracy: 0.6201\n",
      "Epoch 1392/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9413 - accuracy: 0.7025 - val_loss: 1.1034 - val_accuracy: 0.6201\n",
      "Epoch 1393/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8948 - accuracy: 0.7151 - val_loss: 1.1014 - val_accuracy: 0.6299\n",
      "Epoch 1394/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9131 - accuracy: 0.6836 - val_loss: 1.1007 - val_accuracy: 0.6266\n",
      "Epoch 1395/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8910 - accuracy: 0.7041 - val_loss: 1.1061 - val_accuracy: 0.6266\n",
      "Epoch 1396/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8997 - accuracy: 0.7151 - val_loss: 1.1167 - val_accuracy: 0.6299\n",
      "Epoch 1397/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9383 - accuracy: 0.6858 - val_loss: 1.1273 - val_accuracy: 0.6234\n",
      "Epoch 1398/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.9202 - accuracy: 0.7002 - val_loss: 1.1291 - val_accuracy: 0.6299\n",
      "Epoch 1399/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9362 - accuracy: 0.6913 - val_loss: 1.1293 - val_accuracy: 0.6364\n",
      "Epoch 1400/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8703 - accuracy: 0.7031 - val_loss: 1.1147 - val_accuracy: 0.6396\n",
      "Epoch 1401/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9006 - accuracy: 0.6941 - val_loss: 1.1004 - val_accuracy: 0.6429\n",
      "Epoch 1402/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.9688 - accuracy: 0.6816 - val_loss: 1.0894 - val_accuracy: 0.6494\n",
      "Epoch 1403/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9083 - accuracy: 0.6872 - val_loss: 1.0921 - val_accuracy: 0.6429\n",
      "Epoch 1404/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8995 - accuracy: 0.6973 - val_loss: 1.1014 - val_accuracy: 0.6396\n",
      "Epoch 1405/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8987 - accuracy: 0.6885 - val_loss: 1.1067 - val_accuracy: 0.6364\n",
      "Epoch 1406/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9403 - accuracy: 0.6885 - val_loss: 1.1118 - val_accuracy: 0.6299\n",
      "Epoch 1407/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9787 - accuracy: 0.6826 - val_loss: 1.1092 - val_accuracy: 0.6266\n",
      "Epoch 1408/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9291 - accuracy: 0.6846 - val_loss: 1.1050 - val_accuracy: 0.6201\n",
      "Epoch 1409/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8551 - accuracy: 0.7197 - val_loss: 1.1097 - val_accuracy: 0.6201\n",
      "Epoch 1410/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.9232 - accuracy: 0.7109 - val_loss: 1.1210 - val_accuracy: 0.6136\n",
      "Epoch 1411/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9108 - accuracy: 0.6787 - val_loss: 1.1281 - val_accuracy: 0.6266\n",
      "Epoch 1412/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8607 - accuracy: 0.7039 - val_loss: 1.1325 - val_accuracy: 0.6104\n",
      "Epoch 1413/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8824 - accuracy: 0.7002 - val_loss: 1.1291 - val_accuracy: 0.6136\n",
      "Epoch 1414/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9167 - accuracy: 0.6943 - val_loss: 1.1197 - val_accuracy: 0.6071\n",
      "Epoch 1415/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9447 - accuracy: 0.6899 - val_loss: 1.1122 - val_accuracy: 0.6396\n",
      "Epoch 1416/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8932 - accuracy: 0.7051 - val_loss: 1.1062 - val_accuracy: 0.6364\n",
      "Epoch 1417/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9209 - accuracy: 0.6934 - val_loss: 1.0899 - val_accuracy: 0.6331\n",
      "Epoch 1418/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9057 - accuracy: 0.6913 - val_loss: 1.0770 - val_accuracy: 0.6429\n",
      "Epoch 1419/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8591 - accuracy: 0.7137 - val_loss: 1.0628 - val_accuracy: 0.6396\n",
      "Epoch 1420/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.9304 - accuracy: 0.6865 - val_loss: 1.0532 - val_accuracy: 0.6494\n",
      "Epoch 1421/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8719 - accuracy: 0.6913 - val_loss: 1.0447 - val_accuracy: 0.6494\n",
      "Epoch 1422/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8761 - accuracy: 0.7109 - val_loss: 1.0419 - val_accuracy: 0.6494\n",
      "Epoch 1423/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8821 - accuracy: 0.7025 - val_loss: 1.0535 - val_accuracy: 0.6429\n",
      "Epoch 1424/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8685 - accuracy: 0.7123 - val_loss: 1.0701 - val_accuracy: 0.6396\n",
      "Epoch 1425/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8948 - accuracy: 0.6992 - val_loss: 1.0952 - val_accuracy: 0.6494\n",
      "Epoch 1426/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.9342 - accuracy: 0.7081 - val_loss: 1.1479 - val_accuracy: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1427/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9357 - accuracy: 0.6858 - val_loss: 1.1937 - val_accuracy: 0.6104\n",
      "Epoch 1428/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8900 - accuracy: 0.7158 - val_loss: 1.2270 - val_accuracy: 0.5974\n",
      "Epoch 1429/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8966 - accuracy: 0.6973 - val_loss: 1.1997 - val_accuracy: 0.6006\n",
      "Epoch 1430/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8488 - accuracy: 0.7123 - val_loss: 1.1691 - val_accuracy: 0.6104\n",
      "Epoch 1431/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9171 - accuracy: 0.6914 - val_loss: 1.1402 - val_accuracy: 0.6104\n",
      "Epoch 1432/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8458 - accuracy: 0.7179 - val_loss: 1.1447 - val_accuracy: 0.6071\n",
      "Epoch 1433/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9064 - accuracy: 0.6997 - val_loss: 1.1808 - val_accuracy: 0.5942\n",
      "Epoch 1434/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9279 - accuracy: 0.6913 - val_loss: 1.2626 - val_accuracy: 0.5714\n",
      "Epoch 1435/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8880 - accuracy: 0.6963 - val_loss: 1.3415 - val_accuracy: 0.5422\n",
      "Epoch 1436/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9035 - accuracy: 0.6982 - val_loss: 1.4038 - val_accuracy: 0.5325\n",
      "Epoch 1437/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8458 - accuracy: 0.7227 - val_loss: 1.4064 - val_accuracy: 0.5390\n",
      "Epoch 1438/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8818 - accuracy: 0.6802 - val_loss: 1.3843 - val_accuracy: 0.5519\n",
      "Epoch 1439/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8629 - accuracy: 0.7227 - val_loss: 1.3598 - val_accuracy: 0.5714\n",
      "Epoch 1440/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9154 - accuracy: 0.6914 - val_loss: 1.3075 - val_accuracy: 0.5714\n",
      "Epoch 1441/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9279 - accuracy: 0.6802 - val_loss: 1.2435 - val_accuracy: 0.5844\n",
      "Epoch 1442/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8411 - accuracy: 0.7246 - val_loss: 1.1859 - val_accuracy: 0.6169\n",
      "Epoch 1443/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9494 - accuracy: 0.6997 - val_loss: 1.1455 - val_accuracy: 0.6299\n",
      "Epoch 1444/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8914 - accuracy: 0.6872 - val_loss: 1.1241 - val_accuracy: 0.6299\n",
      "Epoch 1445/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9094 - accuracy: 0.6927 - val_loss: 1.1072 - val_accuracy: 0.6429\n",
      "Epoch 1446/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9300 - accuracy: 0.6816 - val_loss: 1.0969 - val_accuracy: 0.6591\n",
      "Epoch 1447/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8533 - accuracy: 0.7081 - val_loss: 1.0959 - val_accuracy: 0.6494\n",
      "Epoch 1448/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9442 - accuracy: 0.7039 - val_loss: 1.1079 - val_accuracy: 0.6429\n",
      "Epoch 1449/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8737 - accuracy: 0.6934 - val_loss: 1.1289 - val_accuracy: 0.6299\n",
      "Epoch 1450/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9435 - accuracy: 0.6943 - val_loss: 1.1628 - val_accuracy: 0.6104\n",
      "Epoch 1451/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8876 - accuracy: 0.6955 - val_loss: 1.2012 - val_accuracy: 0.5942\n",
      "Epoch 1452/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8571 - accuracy: 0.7158 - val_loss: 1.2195 - val_accuracy: 0.6039\n",
      "Epoch 1453/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8745 - accuracy: 0.7095 - val_loss: 1.2142 - val_accuracy: 0.6006\n",
      "Epoch 1454/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8962 - accuracy: 0.6816 - val_loss: 1.2048 - val_accuracy: 0.6104\n",
      "Epoch 1455/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8998 - accuracy: 0.6997 - val_loss: 1.1906 - val_accuracy: 0.6104\n",
      "Epoch 1456/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8783 - accuracy: 0.7095 - val_loss: 1.1845 - val_accuracy: 0.6104\n",
      "Epoch 1457/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8951 - accuracy: 0.7090 - val_loss: 1.1830 - val_accuracy: 0.6104\n",
      "Epoch 1458/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8616 - accuracy: 0.7168 - val_loss: 1.1866 - val_accuracy: 0.6169\n",
      "Epoch 1459/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9256 - accuracy: 0.6872 - val_loss: 1.1842 - val_accuracy: 0.6169\n",
      "Epoch 1460/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8997 - accuracy: 0.7304 - val_loss: 1.1930 - val_accuracy: 0.6071\n",
      "Epoch 1461/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8633 - accuracy: 0.7061 - val_loss: 1.1987 - val_accuracy: 0.6006\n",
      "Epoch 1462/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8313 - accuracy: 0.7246 - val_loss: 1.2185 - val_accuracy: 0.6006\n",
      "Epoch 1463/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8851 - accuracy: 0.6865 - val_loss: 1.2198 - val_accuracy: 0.6006\n",
      "Epoch 1464/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9274 - accuracy: 0.7025 - val_loss: 1.2231 - val_accuracy: 0.5942\n",
      "Epoch 1465/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8321 - accuracy: 0.7344 - val_loss: 1.2294 - val_accuracy: 0.5812\n",
      "Epoch 1466/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8925 - accuracy: 0.7012 - val_loss: 1.2300 - val_accuracy: 0.5779\n",
      "Epoch 1467/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7465 - accuracy: 0.7430 - val_loss: 1.2202 - val_accuracy: 0.5844\n",
      "Epoch 1468/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9545 - accuracy: 0.6826 - val_loss: 1.2051 - val_accuracy: 0.5909\n",
      "Epoch 1469/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8887 - accuracy: 0.7041 - val_loss: 1.1922 - val_accuracy: 0.6006\n",
      "Epoch 1470/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9131 - accuracy: 0.6807 - val_loss: 1.1658 - val_accuracy: 0.6071\n",
      "Epoch 1471/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9058 - accuracy: 0.7012 - val_loss: 1.1489 - val_accuracy: 0.6169\n",
      "Epoch 1472/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8485 - accuracy: 0.7025 - val_loss: 1.1311 - val_accuracy: 0.6136\n",
      "Epoch 1473/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8679 - accuracy: 0.7178 - val_loss: 1.1268 - val_accuracy: 0.6136\n",
      "Epoch 1474/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9061 - accuracy: 0.7109 - val_loss: 1.1533 - val_accuracy: 0.6104\n",
      "Epoch 1475/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9143 - accuracy: 0.6983 - val_loss: 1.1899 - val_accuracy: 0.6006\n",
      "Epoch 1476/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8945 - accuracy: 0.7193 - val_loss: 1.2317 - val_accuracy: 0.5812\n",
      "Epoch 1477/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8539 - accuracy: 0.7061 - val_loss: 1.2665 - val_accuracy: 0.5812\n",
      "Epoch 1478/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8097 - accuracy: 0.7304 - val_loss: 1.3005 - val_accuracy: 0.5779\n",
      "Epoch 1479/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8969 - accuracy: 0.6941 - val_loss: 1.3162 - val_accuracy: 0.5844\n",
      "Epoch 1480/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8976 - accuracy: 0.7109 - val_loss: 1.3098 - val_accuracy: 0.5909\n",
      "Epoch 1481/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8702 - accuracy: 0.7151 - val_loss: 1.2651 - val_accuracy: 0.5909\n",
      "Epoch 1482/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8343 - accuracy: 0.7285 - val_loss: 1.2263 - val_accuracy: 0.5942\n",
      "Epoch 1483/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9393 - accuracy: 0.6904 - val_loss: 1.1803 - val_accuracy: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1484/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8196 - accuracy: 0.7360 - val_loss: 1.1331 - val_accuracy: 0.6266\n",
      "Epoch 1485/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9236 - accuracy: 0.6955 - val_loss: 1.1074 - val_accuracy: 0.6331\n",
      "Epoch 1486/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8865 - accuracy: 0.7179 - val_loss: 1.0869 - val_accuracy: 0.6266\n",
      "Epoch 1487/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8274 - accuracy: 0.7053 - val_loss: 1.0880 - val_accuracy: 0.6266\n",
      "Epoch 1488/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8508 - accuracy: 0.7165 - val_loss: 1.1132 - val_accuracy: 0.6299\n",
      "Epoch 1489/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8052 - accuracy: 0.7246 - val_loss: 1.1538 - val_accuracy: 0.6071\n",
      "Epoch 1490/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8263 - accuracy: 0.7139 - val_loss: 1.1953 - val_accuracy: 0.6006\n",
      "Epoch 1491/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9064 - accuracy: 0.6997 - val_loss: 1.2296 - val_accuracy: 0.5974\n",
      "Epoch 1492/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8798 - accuracy: 0.6997 - val_loss: 1.2380 - val_accuracy: 0.6039\n",
      "Epoch 1493/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8744 - accuracy: 0.7151 - val_loss: 1.2478 - val_accuracy: 0.6201\n",
      "Epoch 1494/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9081 - accuracy: 0.6955 - val_loss: 1.2203 - val_accuracy: 0.6201\n",
      "Epoch 1495/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8304 - accuracy: 0.7137 - val_loss: 1.2004 - val_accuracy: 0.6201\n",
      "Epoch 1496/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7986 - accuracy: 0.7354 - val_loss: 1.2063 - val_accuracy: 0.6201\n",
      "Epoch 1497/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8639 - accuracy: 0.7090 - val_loss: 1.2230 - val_accuracy: 0.6104\n",
      "Epoch 1498/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8055 - accuracy: 0.7246 - val_loss: 1.2260 - val_accuracy: 0.6136\n",
      "Epoch 1499/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8519 - accuracy: 0.7277 - val_loss: 1.2344 - val_accuracy: 0.6104\n",
      "Epoch 1500/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8971 - accuracy: 0.6924 - val_loss: 1.2285 - val_accuracy: 0.6006\n",
      "Epoch 1501/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8826 - accuracy: 0.7100 - val_loss: 1.2412 - val_accuracy: 0.6006\n",
      "Epoch 1502/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8465 - accuracy: 0.7080 - val_loss: 1.2681 - val_accuracy: 0.5942\n",
      "Epoch 1503/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8492 - accuracy: 0.7263 - val_loss: 1.3118 - val_accuracy: 0.5844\n",
      "Epoch 1504/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9566 - accuracy: 0.6885 - val_loss: 1.3514 - val_accuracy: 0.5747\n",
      "Epoch 1505/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7967 - accuracy: 0.7360 - val_loss: 1.3421 - val_accuracy: 0.5714\n",
      "Epoch 1506/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8148 - accuracy: 0.7363 - val_loss: 1.3281 - val_accuracy: 0.5649\n",
      "Epoch 1507/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8455 - accuracy: 0.7318 - val_loss: 1.2703 - val_accuracy: 0.5844\n",
      "Epoch 1508/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8908 - accuracy: 0.6973 - val_loss: 1.2183 - val_accuracy: 0.5844\n",
      "Epoch 1509/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7963 - accuracy: 0.7451 - val_loss: 1.1905 - val_accuracy: 0.5909\n",
      "Epoch 1510/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8749 - accuracy: 0.7148 - val_loss: 1.1696 - val_accuracy: 0.5909\n",
      "Epoch 1511/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8397 - accuracy: 0.7109 - val_loss: 1.1635 - val_accuracy: 0.5974\n",
      "Epoch 1512/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8616 - accuracy: 0.7139 - val_loss: 1.1787 - val_accuracy: 0.6071\n",
      "Epoch 1513/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7890 - accuracy: 0.7363 - val_loss: 1.1837 - val_accuracy: 0.6104\n",
      "Epoch 1514/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8353 - accuracy: 0.7067 - val_loss: 1.1778 - val_accuracy: 0.6104\n",
      "Epoch 1515/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8678 - accuracy: 0.7067 - val_loss: 1.1698 - val_accuracy: 0.6266\n",
      "Epoch 1516/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8348 - accuracy: 0.7197 - val_loss: 1.1566 - val_accuracy: 0.6299\n",
      "Epoch 1517/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8293 - accuracy: 0.7165 - val_loss: 1.1105 - val_accuracy: 0.6364\n",
      "Epoch 1518/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7826 - accuracy: 0.7291 - val_loss: 1.0636 - val_accuracy: 0.6396\n",
      "Epoch 1519/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8577 - accuracy: 0.7061 - val_loss: 1.0426 - val_accuracy: 0.6364\n",
      "Epoch 1520/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.9035 - accuracy: 0.7100 - val_loss: 1.0341 - val_accuracy: 0.6461\n",
      "Epoch 1521/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8564 - accuracy: 0.7217 - val_loss: 1.0432 - val_accuracy: 0.6461\n",
      "Epoch 1522/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8627 - accuracy: 0.7025 - val_loss: 1.0760 - val_accuracy: 0.6201\n",
      "Epoch 1523/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8553 - accuracy: 0.7100 - val_loss: 1.1286 - val_accuracy: 0.6234\n",
      "Epoch 1524/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8270 - accuracy: 0.7304 - val_loss: 1.1987 - val_accuracy: 0.6136\n",
      "Epoch 1525/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8940 - accuracy: 0.7025 - val_loss: 1.2705 - val_accuracy: 0.6104\n",
      "Epoch 1526/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8344 - accuracy: 0.7100 - val_loss: 1.3302 - val_accuracy: 0.6039\n",
      "Epoch 1527/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8503 - accuracy: 0.7100 - val_loss: 1.3597 - val_accuracy: 0.5974\n",
      "Epoch 1528/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8655 - accuracy: 0.7221 - val_loss: 1.3441 - val_accuracy: 0.5844\n",
      "Epoch 1529/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9177 - accuracy: 0.7039 - val_loss: 1.3064 - val_accuracy: 0.5877\n",
      "Epoch 1530/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8213 - accuracy: 0.7168 - val_loss: 1.2498 - val_accuracy: 0.5942\n",
      "Epoch 1531/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8432 - accuracy: 0.7090 - val_loss: 1.1923 - val_accuracy: 0.6039\n",
      "Epoch 1532/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8272 - accuracy: 0.7236 - val_loss: 1.1560 - val_accuracy: 0.5974\n",
      "Epoch 1533/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8449 - accuracy: 0.7139 - val_loss: 1.1414 - val_accuracy: 0.6039\n",
      "Epoch 1534/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8062 - accuracy: 0.7304 - val_loss: 1.1363 - val_accuracy: 0.6071\n",
      "Epoch 1535/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8321 - accuracy: 0.7266 - val_loss: 1.1553 - val_accuracy: 0.6136\n",
      "Epoch 1536/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9159 - accuracy: 0.6826 - val_loss: 1.1853 - val_accuracy: 0.6039\n",
      "Epoch 1537/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8387 - accuracy: 0.7249 - val_loss: 1.2096 - val_accuracy: 0.6006\n",
      "Epoch 1538/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8045 - accuracy: 0.7393 - val_loss: 1.2339 - val_accuracy: 0.6071\n",
      "Epoch 1539/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7916 - accuracy: 0.7354 - val_loss: 1.2542 - val_accuracy: 0.6071\n",
      "Epoch 1540/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8447 - accuracy: 0.7285 - val_loss: 1.2469 - val_accuracy: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1541/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8375 - accuracy: 0.7363 - val_loss: 1.2163 - val_accuracy: 0.6071\n",
      "Epoch 1542/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8436 - accuracy: 0.7236 - val_loss: 1.1702 - val_accuracy: 0.6104\n",
      "Epoch 1543/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8198 - accuracy: 0.7363 - val_loss: 1.1168 - val_accuracy: 0.6169\n",
      "Epoch 1544/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9080 - accuracy: 0.6955 - val_loss: 1.0803 - val_accuracy: 0.6266\n",
      "Epoch 1545/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8448 - accuracy: 0.7193 - val_loss: 1.0735 - val_accuracy: 0.6331\n",
      "Epoch 1546/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8604 - accuracy: 0.7100 - val_loss: 1.0749 - val_accuracy: 0.6331\n",
      "Epoch 1547/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8325 - accuracy: 0.7291 - val_loss: 1.0665 - val_accuracy: 0.6364\n",
      "Epoch 1548/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8372 - accuracy: 0.7109 - val_loss: 1.0541 - val_accuracy: 0.6364\n",
      "Epoch 1549/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8037 - accuracy: 0.7324 - val_loss: 1.0441 - val_accuracy: 0.6364\n",
      "Epoch 1550/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7993 - accuracy: 0.7277 - val_loss: 1.0492 - val_accuracy: 0.6331\n",
      "Epoch 1551/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7718 - accuracy: 0.7430 - val_loss: 1.0502 - val_accuracy: 0.6201\n",
      "Epoch 1552/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8700 - accuracy: 0.7179 - val_loss: 1.0524 - val_accuracy: 0.6266\n",
      "Epoch 1553/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8574 - accuracy: 0.7263 - val_loss: 1.0832 - val_accuracy: 0.6364\n",
      "Epoch 1554/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7827 - accuracy: 0.7363 - val_loss: 1.1186 - val_accuracy: 0.6396\n",
      "Epoch 1555/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8433 - accuracy: 0.7139 - val_loss: 1.1361 - val_accuracy: 0.6429\n",
      "Epoch 1556/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8247 - accuracy: 0.7314 - val_loss: 1.1361 - val_accuracy: 0.6364\n",
      "Epoch 1557/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8376 - accuracy: 0.7188 - val_loss: 1.1350 - val_accuracy: 0.6266\n",
      "Epoch 1558/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8701 - accuracy: 0.7041 - val_loss: 1.1411 - val_accuracy: 0.6266\n",
      "Epoch 1559/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8038 - accuracy: 0.7486 - val_loss: 1.1683 - val_accuracy: 0.6299\n",
      "Epoch 1560/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8361 - accuracy: 0.7291 - val_loss: 1.1916 - val_accuracy: 0.6364\n",
      "Epoch 1561/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8526 - accuracy: 0.7002 - val_loss: 1.2352 - val_accuracy: 0.6266\n",
      "Epoch 1562/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8306 - accuracy: 0.7263 - val_loss: 1.2818 - val_accuracy: 0.6071\n",
      "Epoch 1563/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8312 - accuracy: 0.7256 - val_loss: 1.3050 - val_accuracy: 0.6039\n",
      "Epoch 1564/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8213 - accuracy: 0.7256 - val_loss: 1.3137 - val_accuracy: 0.6071\n",
      "Epoch 1565/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8195 - accuracy: 0.7188 - val_loss: 1.3012 - val_accuracy: 0.6039\n",
      "Epoch 1566/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8380 - accuracy: 0.7179 - val_loss: 1.2595 - val_accuracy: 0.6169\n",
      "Epoch 1567/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8347 - accuracy: 0.7263 - val_loss: 1.2150 - val_accuracy: 0.6234\n",
      "Epoch 1568/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7975 - accuracy: 0.7416 - val_loss: 1.1807 - val_accuracy: 0.6331\n",
      "Epoch 1569/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8593 - accuracy: 0.7285 - val_loss: 1.1622 - val_accuracy: 0.6299\n",
      "Epoch 1570/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8357 - accuracy: 0.7246 - val_loss: 1.1596 - val_accuracy: 0.6396\n",
      "Epoch 1571/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8287 - accuracy: 0.7304 - val_loss: 1.1718 - val_accuracy: 0.6234\n",
      "Epoch 1572/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7707 - accuracy: 0.7500 - val_loss: 1.1835 - val_accuracy: 0.6201\n",
      "Epoch 1573/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7625 - accuracy: 0.7432 - val_loss: 1.1978 - val_accuracy: 0.6071\n",
      "Epoch 1574/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7911 - accuracy: 0.7416 - val_loss: 1.2144 - val_accuracy: 0.6039\n",
      "Epoch 1575/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8721 - accuracy: 0.7109 - val_loss: 1.2217 - val_accuracy: 0.6039\n",
      "Epoch 1576/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8163 - accuracy: 0.7277 - val_loss: 1.2274 - val_accuracy: 0.6006\n",
      "Epoch 1577/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8256 - accuracy: 0.7332 - val_loss: 1.2392 - val_accuracy: 0.6039\n",
      "Epoch 1578/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7952 - accuracy: 0.7500 - val_loss: 1.2454 - val_accuracy: 0.5942\n",
      "Epoch 1579/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8586 - accuracy: 0.7207 - val_loss: 1.2589 - val_accuracy: 0.5877\n",
      "Epoch 1580/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7625 - accuracy: 0.7500 - val_loss: 1.2615 - val_accuracy: 0.5877\n",
      "Epoch 1581/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8174 - accuracy: 0.7129 - val_loss: 1.2707 - val_accuracy: 0.6071\n",
      "Epoch 1582/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7967 - accuracy: 0.7373 - val_loss: 1.2860 - val_accuracy: 0.6136\n",
      "Epoch 1583/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8648 - accuracy: 0.7249 - val_loss: 1.3011 - val_accuracy: 0.6201\n",
      "Epoch 1584/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7813 - accuracy: 0.7374 - val_loss: 1.3273 - val_accuracy: 0.6104\n",
      "Epoch 1585/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7681 - accuracy: 0.7514 - val_loss: 1.3552 - val_accuracy: 0.6136\n",
      "Epoch 1586/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.9046 - accuracy: 0.7002 - val_loss: 1.3897 - val_accuracy: 0.6006\n",
      "Epoch 1587/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7855 - accuracy: 0.7402 - val_loss: 1.4029 - val_accuracy: 0.5844\n",
      "Epoch 1588/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8344 - accuracy: 0.7165 - val_loss: 1.3864 - val_accuracy: 0.5682\n",
      "Epoch 1589/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8183 - accuracy: 0.7295 - val_loss: 1.2935 - val_accuracy: 0.5812\n",
      "Epoch 1590/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7677 - accuracy: 0.7388 - val_loss: 1.2240 - val_accuracy: 0.6039\n",
      "Epoch 1591/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7345 - accuracy: 0.7607 - val_loss: 1.1532 - val_accuracy: 0.6136\n",
      "Epoch 1592/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7933 - accuracy: 0.7416 - val_loss: 1.1098 - val_accuracy: 0.6331\n",
      "Epoch 1593/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7945 - accuracy: 0.7305 - val_loss: 1.0901 - val_accuracy: 0.6364\n",
      "Epoch 1594/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8408 - accuracy: 0.7193 - val_loss: 1.0720 - val_accuracy: 0.6331\n",
      "Epoch 1595/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7771 - accuracy: 0.7451 - val_loss: 1.0843 - val_accuracy: 0.6331\n",
      "Epoch 1596/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7772 - accuracy: 0.7432 - val_loss: 1.1131 - val_accuracy: 0.6234\n",
      "Epoch 1597/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7762 - accuracy: 0.7528 - val_loss: 1.1691 - val_accuracy: 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1598/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8128 - accuracy: 0.7318 - val_loss: 1.2305 - val_accuracy: 0.6136\n",
      "Epoch 1599/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8944 - accuracy: 0.7053 - val_loss: 1.2694 - val_accuracy: 0.6136\n",
      "Epoch 1600/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7884 - accuracy: 0.7363 - val_loss: 1.2825 - val_accuracy: 0.6104\n",
      "Epoch 1601/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8597 - accuracy: 0.7188 - val_loss: 1.2786 - val_accuracy: 0.6104\n",
      "Epoch 1602/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8242 - accuracy: 0.7383 - val_loss: 1.2005 - val_accuracy: 0.6299\n",
      "Epoch 1603/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8255 - accuracy: 0.7277 - val_loss: 1.1291 - val_accuracy: 0.6331\n",
      "Epoch 1604/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8160 - accuracy: 0.7305 - val_loss: 1.0798 - val_accuracy: 0.6396\n",
      "Epoch 1605/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8064 - accuracy: 0.7249 - val_loss: 1.0643 - val_accuracy: 0.6526\n",
      "Epoch 1606/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7960 - accuracy: 0.7422 - val_loss: 1.0683 - val_accuracy: 0.6526\n",
      "Epoch 1607/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8098 - accuracy: 0.7402 - val_loss: 1.0736 - val_accuracy: 0.6526\n",
      "Epoch 1608/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7627 - accuracy: 0.7510 - val_loss: 1.0749 - val_accuracy: 0.6558\n",
      "Epoch 1609/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8084 - accuracy: 0.7207 - val_loss: 1.0712 - val_accuracy: 0.6526\n",
      "Epoch 1610/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7149 - accuracy: 0.7849 - val_loss: 1.0790 - val_accuracy: 0.6331\n",
      "Epoch 1611/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7993 - accuracy: 0.7393 - val_loss: 1.0998 - val_accuracy: 0.6234\n",
      "Epoch 1612/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7534 - accuracy: 0.7441 - val_loss: 1.1537 - val_accuracy: 0.6104\n",
      "Epoch 1613/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7787 - accuracy: 0.7275 - val_loss: 1.2164 - val_accuracy: 0.5974\n",
      "Epoch 1614/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8202 - accuracy: 0.7067 - val_loss: 1.2711 - val_accuracy: 0.5942\n",
      "Epoch 1615/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8309 - accuracy: 0.7236 - val_loss: 1.2796 - val_accuracy: 0.5942\n",
      "Epoch 1616/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7400 - accuracy: 0.7584 - val_loss: 1.2607 - val_accuracy: 0.6039\n",
      "Epoch 1617/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7908 - accuracy: 0.7432 - val_loss: 1.2523 - val_accuracy: 0.6006\n",
      "Epoch 1618/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8026 - accuracy: 0.7430 - val_loss: 1.2600 - val_accuracy: 0.6006\n",
      "Epoch 1619/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8056 - accuracy: 0.7363 - val_loss: 1.2751 - val_accuracy: 0.6071\n",
      "Epoch 1620/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7884 - accuracy: 0.7430 - val_loss: 1.2433 - val_accuracy: 0.6169\n",
      "Epoch 1621/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7820 - accuracy: 0.7472 - val_loss: 1.2292 - val_accuracy: 0.6234\n",
      "Epoch 1622/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7503 - accuracy: 0.7500 - val_loss: 1.2295 - val_accuracy: 0.6169\n",
      "Epoch 1623/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7602 - accuracy: 0.7393 - val_loss: 1.2384 - val_accuracy: 0.6071\n",
      "Epoch 1624/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7487 - accuracy: 0.7559 - val_loss: 1.2511 - val_accuracy: 0.6104\n",
      "Epoch 1625/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7634 - accuracy: 0.7402 - val_loss: 1.2412 - val_accuracy: 0.6136\n",
      "Epoch 1626/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7598 - accuracy: 0.7584 - val_loss: 1.2770 - val_accuracy: 0.6071\n",
      "Epoch 1627/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7622 - accuracy: 0.7549 - val_loss: 1.3044 - val_accuracy: 0.6006\n",
      "Epoch 1628/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8247 - accuracy: 0.7137 - val_loss: 1.3642 - val_accuracy: 0.5974\n",
      "Epoch 1629/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7988 - accuracy: 0.7263 - val_loss: 1.3713 - val_accuracy: 0.6039\n",
      "Epoch 1630/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7436 - accuracy: 0.7402 - val_loss: 1.3485 - val_accuracy: 0.6039\n",
      "Epoch 1631/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7906 - accuracy: 0.7451 - val_loss: 1.3488 - val_accuracy: 0.5974\n",
      "Epoch 1632/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8096 - accuracy: 0.7354 - val_loss: 1.3445 - val_accuracy: 0.6006\n",
      "Epoch 1633/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7802 - accuracy: 0.7542 - val_loss: 1.3584 - val_accuracy: 0.5942\n",
      "Epoch 1634/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7959 - accuracy: 0.7263 - val_loss: 1.3926 - val_accuracy: 0.5844\n",
      "Epoch 1635/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8288 - accuracy: 0.7314 - val_loss: 1.4236 - val_accuracy: 0.5714\n",
      "Epoch 1636/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8050 - accuracy: 0.7256 - val_loss: 1.4465 - val_accuracy: 0.5779\n",
      "Epoch 1637/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8442 - accuracy: 0.7346 - val_loss: 1.4512 - val_accuracy: 0.5747\n",
      "Epoch 1638/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8026 - accuracy: 0.7249 - val_loss: 1.4816 - val_accuracy: 0.5682\n",
      "Epoch 1639/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8226 - accuracy: 0.7061 - val_loss: 1.4900 - val_accuracy: 0.5714\n",
      "Epoch 1640/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7714 - accuracy: 0.7549 - val_loss: 1.4141 - val_accuracy: 0.5714\n",
      "Epoch 1641/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7758 - accuracy: 0.7458 - val_loss: 1.3243 - val_accuracy: 0.5942\n",
      "Epoch 1642/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7783 - accuracy: 0.7472 - val_loss: 1.2360 - val_accuracy: 0.6234\n",
      "Epoch 1643/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7661 - accuracy: 0.7383 - val_loss: 1.1436 - val_accuracy: 0.6558\n",
      "Epoch 1644/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7545 - accuracy: 0.7607 - val_loss: 1.0722 - val_accuracy: 0.6591\n",
      "Epoch 1645/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7626 - accuracy: 0.7441 - val_loss: 1.0327 - val_accuracy: 0.6688\n",
      "Epoch 1646/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8158 - accuracy: 0.7221 - val_loss: 1.0154 - val_accuracy: 0.6688\n",
      "Epoch 1647/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7983 - accuracy: 0.7332 - val_loss: 1.0093 - val_accuracy: 0.6688\n",
      "Epoch 1648/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7876 - accuracy: 0.7568 - val_loss: 1.0089 - val_accuracy: 0.6558\n",
      "Epoch 1649/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7568 - accuracy: 0.7480 - val_loss: 1.0226 - val_accuracy: 0.6494\n",
      "Epoch 1650/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7577 - accuracy: 0.7514 - val_loss: 1.0334 - val_accuracy: 0.6396\n",
      "Epoch 1651/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7662 - accuracy: 0.7458 - val_loss: 1.0285 - val_accuracy: 0.6396\n",
      "Epoch 1652/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7625 - accuracy: 0.7471 - val_loss: 1.0182 - val_accuracy: 0.6558\n",
      "Epoch 1653/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7673 - accuracy: 0.7441 - val_loss: 1.0165 - val_accuracy: 0.6558\n",
      "Epoch 1654/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7711 - accuracy: 0.7412 - val_loss: 1.0209 - val_accuracy: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1655/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7771 - accuracy: 0.7360 - val_loss: 1.0336 - val_accuracy: 0.6688\n",
      "Epoch 1656/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7849 - accuracy: 0.7263 - val_loss: 1.0576 - val_accuracy: 0.6623\n",
      "Epoch 1657/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8007 - accuracy: 0.7393 - val_loss: 1.0834 - val_accuracy: 0.6623\n",
      "Epoch 1658/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7628 - accuracy: 0.7556 - val_loss: 1.1362 - val_accuracy: 0.6396\n",
      "Epoch 1659/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7369 - accuracy: 0.7668 - val_loss: 1.1976 - val_accuracy: 0.6169\n",
      "Epoch 1660/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7490 - accuracy: 0.7514 - val_loss: 1.2231 - val_accuracy: 0.6266\n",
      "Epoch 1661/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7472 - accuracy: 0.7637 - val_loss: 1.2286 - val_accuracy: 0.6331\n",
      "Epoch 1662/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7751 - accuracy: 0.7332 - val_loss: 1.2220 - val_accuracy: 0.6331\n",
      "Epoch 1663/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7861 - accuracy: 0.7458 - val_loss: 1.2316 - val_accuracy: 0.6331\n",
      "Epoch 1664/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7820 - accuracy: 0.7246 - val_loss: 1.2313 - val_accuracy: 0.6364\n",
      "Epoch 1665/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7715 - accuracy: 0.7500 - val_loss: 1.2186 - val_accuracy: 0.6364\n",
      "Epoch 1666/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8326 - accuracy: 0.7295 - val_loss: 1.1937 - val_accuracy: 0.6364\n",
      "Epoch 1667/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7941 - accuracy: 0.7432 - val_loss: 1.1764 - val_accuracy: 0.6299\n",
      "Epoch 1668/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7500 - accuracy: 0.7444 - val_loss: 1.1438 - val_accuracy: 0.6364\n",
      "Epoch 1669/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7521 - accuracy: 0.7578 - val_loss: 1.1515 - val_accuracy: 0.6331\n",
      "Epoch 1670/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8471 - accuracy: 0.7346 - val_loss: 1.1752 - val_accuracy: 0.6136\n",
      "Epoch 1671/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7740 - accuracy: 0.7627 - val_loss: 1.2055 - val_accuracy: 0.6201\n",
      "Epoch 1672/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7524 - accuracy: 0.7461 - val_loss: 1.2594 - val_accuracy: 0.6169\n",
      "Epoch 1673/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7308 - accuracy: 0.7696 - val_loss: 1.3043 - val_accuracy: 0.6169\n",
      "Epoch 1674/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8147 - accuracy: 0.7332 - val_loss: 1.3798 - val_accuracy: 0.6136\n",
      "Epoch 1675/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8081 - accuracy: 0.7227 - val_loss: 1.4923 - val_accuracy: 0.5779\n",
      "Epoch 1676/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7488 - accuracy: 0.7539 - val_loss: 1.5971 - val_accuracy: 0.5552\n",
      "Epoch 1677/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.8058 - accuracy: 0.7373 - val_loss: 1.6989 - val_accuracy: 0.5227\n",
      "Epoch 1678/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7618 - accuracy: 0.7472 - val_loss: 1.7155 - val_accuracy: 0.5162\n",
      "Epoch 1679/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7401 - accuracy: 0.7598 - val_loss: 1.6651 - val_accuracy: 0.5292\n",
      "Epoch 1680/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7816 - accuracy: 0.7383 - val_loss: 1.5747 - val_accuracy: 0.5487\n",
      "Epoch 1681/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7982 - accuracy: 0.7256 - val_loss: 1.4899 - val_accuracy: 0.5812\n",
      "Epoch 1682/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7709 - accuracy: 0.7528 - val_loss: 1.4254 - val_accuracy: 0.6006\n",
      "Epoch 1683/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7327 - accuracy: 0.7514 - val_loss: 1.3945 - val_accuracy: 0.6104\n",
      "Epoch 1684/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7436 - accuracy: 0.7480 - val_loss: 1.3792 - val_accuracy: 0.6201\n",
      "Epoch 1685/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6880 - accuracy: 0.7654 - val_loss: 1.3603 - val_accuracy: 0.6169\n",
      "Epoch 1686/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7825 - accuracy: 0.7291 - val_loss: 1.2998 - val_accuracy: 0.6299\n",
      "Epoch 1687/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7514 - accuracy: 0.7584 - val_loss: 1.2260 - val_accuracy: 0.6429\n",
      "Epoch 1688/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7467 - accuracy: 0.7472 - val_loss: 1.1824 - val_accuracy: 0.6591\n",
      "Epoch 1689/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7368 - accuracy: 0.7584 - val_loss: 1.1530 - val_accuracy: 0.6591\n",
      "Epoch 1690/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7708 - accuracy: 0.7490 - val_loss: 1.1288 - val_accuracy: 0.6591\n",
      "Epoch 1691/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7316 - accuracy: 0.7520 - val_loss: 1.1229 - val_accuracy: 0.6558\n",
      "Epoch 1692/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7759 - accuracy: 0.7559 - val_loss: 1.1221 - val_accuracy: 0.6656\n",
      "Epoch 1693/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8163 - accuracy: 0.7332 - val_loss: 1.1305 - val_accuracy: 0.6558\n",
      "Epoch 1694/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7632 - accuracy: 0.7539 - val_loss: 1.1232 - val_accuracy: 0.6591\n",
      "Epoch 1695/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7915 - accuracy: 0.7402 - val_loss: 1.1244 - val_accuracy: 0.6429\n",
      "Epoch 1696/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8019 - accuracy: 0.7354 - val_loss: 1.1337 - val_accuracy: 0.6364\n",
      "Epoch 1697/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7427 - accuracy: 0.7471 - val_loss: 1.1474 - val_accuracy: 0.6364\n",
      "Epoch 1698/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7859 - accuracy: 0.7277 - val_loss: 1.1344 - val_accuracy: 0.6396\n",
      "Epoch 1699/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6913 - accuracy: 0.7754 - val_loss: 1.1150 - val_accuracy: 0.6526\n",
      "Epoch 1700/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7224 - accuracy: 0.7705 - val_loss: 1.1320 - val_accuracy: 0.6558\n",
      "Epoch 1701/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7858 - accuracy: 0.7441 - val_loss: 1.1778 - val_accuracy: 0.6494\n",
      "Epoch 1702/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6778 - accuracy: 0.7821 - val_loss: 1.2494 - val_accuracy: 0.6266\n",
      "Epoch 1703/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7340 - accuracy: 0.7402 - val_loss: 1.3158 - val_accuracy: 0.6201\n",
      "Epoch 1704/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7733 - accuracy: 0.7578 - val_loss: 1.3234 - val_accuracy: 0.6201\n",
      "Epoch 1705/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7566 - accuracy: 0.7472 - val_loss: 1.3061 - val_accuracy: 0.6266\n",
      "Epoch 1706/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7531 - accuracy: 0.7640 - val_loss: 1.2717 - val_accuracy: 0.6234\n",
      "Epoch 1707/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7348 - accuracy: 0.7617 - val_loss: 1.2617 - val_accuracy: 0.6234\n",
      "Epoch 1708/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7496 - accuracy: 0.7542 - val_loss: 1.2764 - val_accuracy: 0.6266\n",
      "Epoch 1709/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7544 - accuracy: 0.7480 - val_loss: 1.2724 - val_accuracy: 0.6364\n",
      "Epoch 1710/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7183 - accuracy: 0.7607 - val_loss: 1.2576 - val_accuracy: 0.6396\n",
      "Epoch 1711/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7797 - accuracy: 0.7461 - val_loss: 1.2340 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1712/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7811 - accuracy: 0.7388 - val_loss: 1.2149 - val_accuracy: 0.6331\n",
      "Epoch 1713/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7447 - accuracy: 0.7570 - val_loss: 1.1996 - val_accuracy: 0.6266\n",
      "Epoch 1714/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7268 - accuracy: 0.7514 - val_loss: 1.1916 - val_accuracy: 0.6331\n",
      "Epoch 1715/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7172 - accuracy: 0.7656 - val_loss: 1.1834 - val_accuracy: 0.6396\n",
      "Epoch 1716/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6961 - accuracy: 0.7744 - val_loss: 1.1802 - val_accuracy: 0.6331\n",
      "Epoch 1717/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7435 - accuracy: 0.7360 - val_loss: 1.1799 - val_accuracy: 0.6396\n",
      "Epoch 1718/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7589 - accuracy: 0.7607 - val_loss: 1.1801 - val_accuracy: 0.6299\n",
      "Epoch 1719/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7081 - accuracy: 0.7656 - val_loss: 1.1673 - val_accuracy: 0.6331\n",
      "Epoch 1720/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7662 - accuracy: 0.7490 - val_loss: 1.1599 - val_accuracy: 0.6331\n",
      "Epoch 1721/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7755 - accuracy: 0.7422 - val_loss: 1.1481 - val_accuracy: 0.6364\n",
      "Epoch 1722/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8346 - accuracy: 0.7430 - val_loss: 1.1427 - val_accuracy: 0.6364\n",
      "Epoch 1723/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7711 - accuracy: 0.7514 - val_loss: 1.1473 - val_accuracy: 0.6364\n",
      "Epoch 1724/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6880 - accuracy: 0.7646 - val_loss: 1.1551 - val_accuracy: 0.6364\n",
      "Epoch 1725/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7485 - accuracy: 0.7374 - val_loss: 1.1775 - val_accuracy: 0.6169\n",
      "Epoch 1726/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7171 - accuracy: 0.7686 - val_loss: 1.1975 - val_accuracy: 0.6169\n",
      "Epoch 1727/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7352 - accuracy: 0.7490 - val_loss: 1.2084 - val_accuracy: 0.6136\n",
      "Epoch 1728/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7603 - accuracy: 0.7556 - val_loss: 1.2048 - val_accuracy: 0.6006\n",
      "Epoch 1729/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7173 - accuracy: 0.7598 - val_loss: 1.2155 - val_accuracy: 0.5974\n",
      "Epoch 1730/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7949 - accuracy: 0.7344 - val_loss: 1.2041 - val_accuracy: 0.5974\n",
      "Epoch 1731/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7199 - accuracy: 0.7637 - val_loss: 1.1900 - val_accuracy: 0.6039\n",
      "Epoch 1732/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7345 - accuracy: 0.7559 - val_loss: 1.1764 - val_accuracy: 0.6104\n",
      "Epoch 1733/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7489 - accuracy: 0.7383 - val_loss: 1.1644 - val_accuracy: 0.6234\n",
      "Epoch 1734/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7704 - accuracy: 0.7451 - val_loss: 1.1547 - val_accuracy: 0.6234\n",
      "Epoch 1735/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8425 - accuracy: 0.7137 - val_loss: 1.1369 - val_accuracy: 0.6331\n",
      "Epoch 1736/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7153 - accuracy: 0.7637 - val_loss: 1.1184 - val_accuracy: 0.6364\n",
      "Epoch 1737/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7717 - accuracy: 0.7360 - val_loss: 1.0977 - val_accuracy: 0.6494\n",
      "Epoch 1738/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7632 - accuracy: 0.7451 - val_loss: 1.0854 - val_accuracy: 0.6526\n",
      "Epoch 1739/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7458 - accuracy: 0.7500 - val_loss: 1.0737 - val_accuracy: 0.6623\n",
      "Epoch 1740/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7026 - accuracy: 0.7654 - val_loss: 1.0692 - val_accuracy: 0.6526\n",
      "Epoch 1741/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7061 - accuracy: 0.7695 - val_loss: 1.0664 - val_accuracy: 0.6558\n",
      "Epoch 1742/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7665 - accuracy: 0.7514 - val_loss: 1.0682 - val_accuracy: 0.6526\n",
      "Epoch 1743/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7839 - accuracy: 0.7441 - val_loss: 1.0812 - val_accuracy: 0.6364\n",
      "Epoch 1744/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7347 - accuracy: 0.7520 - val_loss: 1.0928 - val_accuracy: 0.6461\n",
      "Epoch 1745/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7483 - accuracy: 0.7520 - val_loss: 1.1104 - val_accuracy: 0.6299\n",
      "Epoch 1746/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7232 - accuracy: 0.7612 - val_loss: 1.1247 - val_accuracy: 0.6234\n",
      "Epoch 1747/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6910 - accuracy: 0.7723 - val_loss: 1.1207 - val_accuracy: 0.6234\n",
      "Epoch 1748/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7036 - accuracy: 0.7646 - val_loss: 1.1171 - val_accuracy: 0.6331\n",
      "Epoch 1749/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.7237 - accuracy: 0.7559 - val_loss: 1.1052 - val_accuracy: 0.6558\n",
      "Epoch 1750/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7884 - accuracy: 0.7444 - val_loss: 1.1045 - val_accuracy: 0.6558\n",
      "Epoch 1751/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7583 - accuracy: 0.7607 - val_loss: 1.0985 - val_accuracy: 0.6494\n",
      "Epoch 1752/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7095 - accuracy: 0.7793 - val_loss: 1.0964 - val_accuracy: 0.6591\n",
      "Epoch 1753/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7105 - accuracy: 0.7539 - val_loss: 1.0989 - val_accuracy: 0.6558\n",
      "Epoch 1754/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7422 - accuracy: 0.7402 - val_loss: 1.1029 - val_accuracy: 0.6526\n",
      "Epoch 1755/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7082 - accuracy: 0.7542 - val_loss: 1.1077 - val_accuracy: 0.6396\n",
      "Epoch 1756/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6964 - accuracy: 0.7849 - val_loss: 1.1237 - val_accuracy: 0.6461\n",
      "Epoch 1757/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7247 - accuracy: 0.7607 - val_loss: 1.1365 - val_accuracy: 0.6396\n",
      "Epoch 1758/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7881 - accuracy: 0.7318 - val_loss: 1.1528 - val_accuracy: 0.6396\n",
      "Epoch 1759/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8142 - accuracy: 0.7291 - val_loss: 1.1605 - val_accuracy: 0.6494\n",
      "Epoch 1760/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7644 - accuracy: 0.7458 - val_loss: 1.1454 - val_accuracy: 0.6331\n",
      "Epoch 1761/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6851 - accuracy: 0.7765 - val_loss: 1.1377 - val_accuracy: 0.6429\n",
      "Epoch 1762/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.7814 - accuracy: 0.7510 - val_loss: 1.1333 - val_accuracy: 0.6688\n",
      "Epoch 1763/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7646 - accuracy: 0.7500 - val_loss: 1.1311 - val_accuracy: 0.6526\n",
      "Epoch 1764/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7482 - accuracy: 0.7500 - val_loss: 1.1411 - val_accuracy: 0.6591\n",
      "Epoch 1765/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7048 - accuracy: 0.7682 - val_loss: 1.1579 - val_accuracy: 0.6623\n",
      "Epoch 1766/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7690 - accuracy: 0.7402 - val_loss: 1.1769 - val_accuracy: 0.6623\n",
      "Epoch 1767/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8039 - accuracy: 0.7402 - val_loss: 1.1936 - val_accuracy: 0.6494\n",
      "Epoch 1768/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7381 - accuracy: 0.7490 - val_loss: 1.2040 - val_accuracy: 0.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1769/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7233 - accuracy: 0.7646 - val_loss: 1.2095 - val_accuracy: 0.6234\n",
      "Epoch 1770/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7225 - accuracy: 0.7715 - val_loss: 1.2088 - val_accuracy: 0.6039\n",
      "Epoch 1771/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7315 - accuracy: 0.7773 - val_loss: 1.2005 - val_accuracy: 0.6136\n",
      "Epoch 1772/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7859 - accuracy: 0.7432 - val_loss: 1.1952 - val_accuracy: 0.6299\n",
      "Epoch 1773/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6498 - accuracy: 0.7723 - val_loss: 1.1933 - val_accuracy: 0.6331\n",
      "Epoch 1774/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7555 - accuracy: 0.7556 - val_loss: 1.1831 - val_accuracy: 0.6299\n",
      "Epoch 1775/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6843 - accuracy: 0.7822 - val_loss: 1.1751 - val_accuracy: 0.6396\n",
      "Epoch 1776/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7301 - accuracy: 0.7539 - val_loss: 1.1519 - val_accuracy: 0.6396\n",
      "Epoch 1777/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7056 - accuracy: 0.7723 - val_loss: 1.1295 - val_accuracy: 0.6461\n",
      "Epoch 1778/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7269 - accuracy: 0.7705 - val_loss: 1.1013 - val_accuracy: 0.6494\n",
      "Epoch 1779/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7350 - accuracy: 0.7584 - val_loss: 1.0840 - val_accuracy: 0.6558\n",
      "Epoch 1780/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6847 - accuracy: 0.7842 - val_loss: 1.0855 - val_accuracy: 0.6429\n",
      "Epoch 1781/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6977 - accuracy: 0.7607 - val_loss: 1.1115 - val_accuracy: 0.6396\n",
      "Epoch 1782/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6904 - accuracy: 0.7779 - val_loss: 1.1430 - val_accuracy: 0.6299\n",
      "Epoch 1783/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6679 - accuracy: 0.7861 - val_loss: 1.1332 - val_accuracy: 0.6396\n",
      "Epoch 1784/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7378 - accuracy: 0.7529 - val_loss: 1.1172 - val_accuracy: 0.6429\n",
      "Epoch 1785/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7476 - accuracy: 0.7668 - val_loss: 1.0980 - val_accuracy: 0.6331\n",
      "Epoch 1786/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6572 - accuracy: 0.7877 - val_loss: 1.0854 - val_accuracy: 0.6558\n",
      "Epoch 1787/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7422 - accuracy: 0.7578 - val_loss: 1.0828 - val_accuracy: 0.6623\n",
      "Epoch 1788/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7263 - accuracy: 0.7744 - val_loss: 1.0859 - val_accuracy: 0.6558\n",
      "Epoch 1789/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7381 - accuracy: 0.7520 - val_loss: 1.0887 - val_accuracy: 0.6494\n",
      "Epoch 1790/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7064 - accuracy: 0.7656 - val_loss: 1.0927 - val_accuracy: 0.6429\n",
      "Epoch 1791/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7467 - accuracy: 0.7514 - val_loss: 1.1052 - val_accuracy: 0.6429\n",
      "Epoch 1792/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7388 - accuracy: 0.7402 - val_loss: 1.1219 - val_accuracy: 0.6364\n",
      "Epoch 1793/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7162 - accuracy: 0.7458 - val_loss: 1.1504 - val_accuracy: 0.6234\n",
      "Epoch 1794/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7827 - accuracy: 0.7682 - val_loss: 1.1757 - val_accuracy: 0.6234\n",
      "Epoch 1795/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7020 - accuracy: 0.7793 - val_loss: 1.2091 - val_accuracy: 0.6136\n",
      "Epoch 1796/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7141 - accuracy: 0.7461 - val_loss: 1.2191 - val_accuracy: 0.6006\n",
      "Epoch 1797/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6919 - accuracy: 0.7754 - val_loss: 1.1929 - val_accuracy: 0.6039\n",
      "Epoch 1798/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7114 - accuracy: 0.7744 - val_loss: 1.1651 - val_accuracy: 0.6169\n",
      "Epoch 1799/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7195 - accuracy: 0.7607 - val_loss: 1.1449 - val_accuracy: 0.6201\n",
      "Epoch 1800/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7381 - accuracy: 0.7444 - val_loss: 1.1384 - val_accuracy: 0.6299\n",
      "Epoch 1801/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7254 - accuracy: 0.7556 - val_loss: 1.1448 - val_accuracy: 0.6201\n",
      "Epoch 1802/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7356 - accuracy: 0.7682 - val_loss: 1.1565 - val_accuracy: 0.6299\n",
      "Epoch 1803/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6917 - accuracy: 0.7668 - val_loss: 1.1678 - val_accuracy: 0.6169\n",
      "Epoch 1804/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7620 - accuracy: 0.7314 - val_loss: 1.1680 - val_accuracy: 0.6266\n",
      "Epoch 1805/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7207 - accuracy: 0.7528 - val_loss: 1.1702 - val_accuracy: 0.6299\n",
      "Epoch 1806/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7138 - accuracy: 0.7656 - val_loss: 1.1737 - val_accuracy: 0.6429\n",
      "Epoch 1807/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7329 - accuracy: 0.7637 - val_loss: 1.1793 - val_accuracy: 0.6461\n",
      "Epoch 1808/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6746 - accuracy: 0.7682 - val_loss: 1.1903 - val_accuracy: 0.6429\n",
      "Epoch 1809/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7047 - accuracy: 0.7754 - val_loss: 1.2026 - val_accuracy: 0.6461\n",
      "Epoch 1810/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7652 - accuracy: 0.7617 - val_loss: 1.2021 - val_accuracy: 0.6331\n",
      "Epoch 1811/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7112 - accuracy: 0.7578 - val_loss: 1.1837 - val_accuracy: 0.6429\n",
      "Epoch 1812/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7351 - accuracy: 0.7598 - val_loss: 1.1732 - val_accuracy: 0.6526\n",
      "Epoch 1813/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6997 - accuracy: 0.7598 - val_loss: 1.1409 - val_accuracy: 0.6396\n",
      "Epoch 1814/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7389 - accuracy: 0.7549 - val_loss: 1.1252 - val_accuracy: 0.6429\n",
      "Epoch 1815/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7226 - accuracy: 0.7542 - val_loss: 1.1131 - val_accuracy: 0.6526\n",
      "Epoch 1816/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6665 - accuracy: 0.7637 - val_loss: 1.0983 - val_accuracy: 0.6558\n",
      "Epoch 1817/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7561 - accuracy: 0.7510 - val_loss: 1.0963 - val_accuracy: 0.6656\n",
      "Epoch 1818/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6433 - accuracy: 0.8017 - val_loss: 1.0987 - val_accuracy: 0.6526\n",
      "Epoch 1819/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6613 - accuracy: 0.7863 - val_loss: 1.1023 - val_accuracy: 0.6591\n",
      "Epoch 1820/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6982 - accuracy: 0.7751 - val_loss: 1.1032 - val_accuracy: 0.6591\n",
      "Epoch 1821/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7813 - accuracy: 0.7393 - val_loss: 1.1022 - val_accuracy: 0.6461\n",
      "Epoch 1822/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7379 - accuracy: 0.7612 - val_loss: 1.1018 - val_accuracy: 0.6526\n",
      "Epoch 1823/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6864 - accuracy: 0.7821 - val_loss: 1.1091 - val_accuracy: 0.6494\n",
      "Epoch 1824/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7070 - accuracy: 0.7686 - val_loss: 1.1104 - val_accuracy: 0.6396\n",
      "Epoch 1825/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6947 - accuracy: 0.7626 - val_loss: 1.1069 - val_accuracy: 0.6461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1826/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6967 - accuracy: 0.7764 - val_loss: 1.1023 - val_accuracy: 0.6494\n",
      "Epoch 1827/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6811 - accuracy: 0.7654 - val_loss: 1.1036 - val_accuracy: 0.6558\n",
      "Epoch 1828/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7056 - accuracy: 0.7654 - val_loss: 1.1111 - val_accuracy: 0.6364\n",
      "Epoch 1829/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7070 - accuracy: 0.7709 - val_loss: 1.1190 - val_accuracy: 0.6331\n",
      "Epoch 1830/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6501 - accuracy: 0.7737 - val_loss: 1.1354 - val_accuracy: 0.6396\n",
      "Epoch 1831/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6641 - accuracy: 0.7900 - val_loss: 1.1386 - val_accuracy: 0.6461\n",
      "Epoch 1832/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7132 - accuracy: 0.7751 - val_loss: 1.1505 - val_accuracy: 0.6234\n",
      "Epoch 1833/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7082 - accuracy: 0.7723 - val_loss: 1.1741 - val_accuracy: 0.6136\n",
      "Epoch 1834/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7416 - accuracy: 0.7682 - val_loss: 1.2154 - val_accuracy: 0.6136\n",
      "Epoch 1835/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6985 - accuracy: 0.7617 - val_loss: 1.2499 - val_accuracy: 0.6169\n",
      "Epoch 1836/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6808 - accuracy: 0.7835 - val_loss: 1.2694 - val_accuracy: 0.6201\n",
      "Epoch 1837/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6462 - accuracy: 0.7900 - val_loss: 1.2688 - val_accuracy: 0.6169\n",
      "Epoch 1838/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7249 - accuracy: 0.7668 - val_loss: 1.2578 - val_accuracy: 0.6201\n",
      "Epoch 1839/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6781 - accuracy: 0.7709 - val_loss: 1.2184 - val_accuracy: 0.6299\n",
      "Epoch 1840/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6904 - accuracy: 0.7793 - val_loss: 1.1944 - val_accuracy: 0.6266\n",
      "Epoch 1841/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7194 - accuracy: 0.7598 - val_loss: 1.1900 - val_accuracy: 0.6234\n",
      "Epoch 1842/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7159 - accuracy: 0.7520 - val_loss: 1.1689 - val_accuracy: 0.6331\n",
      "Epoch 1843/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6951 - accuracy: 0.7612 - val_loss: 1.1573 - val_accuracy: 0.6429\n",
      "Epoch 1844/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7588 - accuracy: 0.7627 - val_loss: 1.1399 - val_accuracy: 0.6429\n",
      "Epoch 1845/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7349 - accuracy: 0.7539 - val_loss: 1.1162 - val_accuracy: 0.6461\n",
      "Epoch 1846/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6833 - accuracy: 0.7754 - val_loss: 1.0924 - val_accuracy: 0.6623\n",
      "Epoch 1847/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7325 - accuracy: 0.7514 - val_loss: 1.0800 - val_accuracy: 0.6721\n",
      "Epoch 1848/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7253 - accuracy: 0.7637 - val_loss: 1.0731 - val_accuracy: 0.6623\n",
      "Epoch 1849/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7018 - accuracy: 0.7682 - val_loss: 1.0724 - val_accuracy: 0.6591\n",
      "Epoch 1850/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6947 - accuracy: 0.7765 - val_loss: 1.0655 - val_accuracy: 0.6656\n",
      "Epoch 1851/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7238 - accuracy: 0.7472 - val_loss: 1.0706 - val_accuracy: 0.6656\n",
      "Epoch 1852/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6507 - accuracy: 0.7835 - val_loss: 1.0746 - val_accuracy: 0.6591\n",
      "Epoch 1853/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6418 - accuracy: 0.7861 - val_loss: 1.0806 - val_accuracy: 0.6494\n",
      "Epoch 1854/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6412 - accuracy: 0.7835 - val_loss: 1.0950 - val_accuracy: 0.6461\n",
      "Epoch 1855/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6996 - accuracy: 0.7725 - val_loss: 1.0917 - val_accuracy: 0.6299\n",
      "Epoch 1856/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7660 - accuracy: 0.7416 - val_loss: 1.0762 - val_accuracy: 0.6331\n",
      "Epoch 1857/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7012 - accuracy: 0.7783 - val_loss: 1.0722 - val_accuracy: 0.6364\n",
      "Epoch 1858/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7098 - accuracy: 0.7578 - val_loss: 1.0651 - val_accuracy: 0.6429\n",
      "Epoch 1859/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6704 - accuracy: 0.7822 - val_loss: 1.0602 - val_accuracy: 0.6461\n",
      "Epoch 1860/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7046 - accuracy: 0.7737 - val_loss: 1.0700 - val_accuracy: 0.6494\n",
      "Epoch 1861/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6729 - accuracy: 0.7835 - val_loss: 1.0755 - val_accuracy: 0.6623\n",
      "Epoch 1862/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7022 - accuracy: 0.7696 - val_loss: 1.0872 - val_accuracy: 0.6526\n",
      "Epoch 1863/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6878 - accuracy: 0.7919 - val_loss: 1.1153 - val_accuracy: 0.6461\n",
      "Epoch 1864/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6640 - accuracy: 0.7705 - val_loss: 1.1417 - val_accuracy: 0.6234\n",
      "Epoch 1865/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6926 - accuracy: 0.7744 - val_loss: 1.1613 - val_accuracy: 0.6364\n",
      "Epoch 1866/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7443 - accuracy: 0.7556 - val_loss: 1.1788 - val_accuracy: 0.6266\n",
      "Epoch 1867/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6611 - accuracy: 0.7930 - val_loss: 1.1893 - val_accuracy: 0.6266\n",
      "Epoch 1868/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6616 - accuracy: 0.7723 - val_loss: 1.2057 - val_accuracy: 0.6396\n",
      "Epoch 1869/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6304 - accuracy: 0.7849 - val_loss: 1.1774 - val_accuracy: 0.6429\n",
      "Epoch 1870/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7254 - accuracy: 0.7444 - val_loss: 1.1554 - val_accuracy: 0.6494\n",
      "Epoch 1871/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7392 - accuracy: 0.7559 - val_loss: 1.1121 - val_accuracy: 0.6429\n",
      "Epoch 1872/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7488 - accuracy: 0.7486 - val_loss: 1.0876 - val_accuracy: 0.6558\n",
      "Epoch 1873/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7077 - accuracy: 0.7584 - val_loss: 1.0847 - val_accuracy: 0.6558\n",
      "Epoch 1874/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7166 - accuracy: 0.7612 - val_loss: 1.0897 - val_accuracy: 0.6526\n",
      "Epoch 1875/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6564 - accuracy: 0.7979 - val_loss: 1.0964 - val_accuracy: 0.6688\n",
      "Epoch 1876/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6658 - accuracy: 0.7807 - val_loss: 1.1135 - val_accuracy: 0.6688\n",
      "Epoch 1877/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7145 - accuracy: 0.7725 - val_loss: 1.1418 - val_accuracy: 0.6494\n",
      "Epoch 1878/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6886 - accuracy: 0.7696 - val_loss: 1.1676 - val_accuracy: 0.6429\n",
      "Epoch 1879/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6792 - accuracy: 0.7754 - val_loss: 1.1874 - val_accuracy: 0.6396\n",
      "Epoch 1880/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6374 - accuracy: 0.7919 - val_loss: 1.1951 - val_accuracy: 0.6429\n",
      "Epoch 1881/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6986 - accuracy: 0.7588 - val_loss: 1.1880 - val_accuracy: 0.6494\n",
      "Epoch 1882/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7159 - accuracy: 0.7640 - val_loss: 1.1841 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1883/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7244 - accuracy: 0.7556 - val_loss: 1.1661 - val_accuracy: 0.6526\n",
      "Epoch 1884/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6688 - accuracy: 0.7779 - val_loss: 1.1541 - val_accuracy: 0.6623\n",
      "Epoch 1885/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6673 - accuracy: 0.7832 - val_loss: 1.1581 - val_accuracy: 0.6494\n",
      "Epoch 1886/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6670 - accuracy: 0.7910 - val_loss: 1.1488 - val_accuracy: 0.6494\n",
      "Epoch 1887/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6779 - accuracy: 0.7989 - val_loss: 1.1458 - val_accuracy: 0.6494\n",
      "Epoch 1888/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6950 - accuracy: 0.7737 - val_loss: 1.1254 - val_accuracy: 0.6526\n",
      "Epoch 1889/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6446 - accuracy: 0.7891 - val_loss: 1.1044 - val_accuracy: 0.6558\n",
      "Epoch 1890/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6868 - accuracy: 0.7754 - val_loss: 1.0922 - val_accuracy: 0.6721\n",
      "Epoch 1891/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6816 - accuracy: 0.7861 - val_loss: 1.0876 - val_accuracy: 0.6688\n",
      "Epoch 1892/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7090 - accuracy: 0.7709 - val_loss: 1.0841 - val_accuracy: 0.6623\n",
      "Epoch 1893/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6335 - accuracy: 0.7939 - val_loss: 1.0838 - val_accuracy: 0.6591\n",
      "Epoch 1894/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7100 - accuracy: 0.7584 - val_loss: 1.0854 - val_accuracy: 0.6656\n",
      "Epoch 1895/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6782 - accuracy: 0.7773 - val_loss: 1.0974 - val_accuracy: 0.6591\n",
      "Epoch 1896/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6680 - accuracy: 0.7754 - val_loss: 1.1161 - val_accuracy: 0.6591\n",
      "Epoch 1897/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6634 - accuracy: 0.7773 - val_loss: 1.1470 - val_accuracy: 0.6461\n",
      "Epoch 1898/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6308 - accuracy: 0.7881 - val_loss: 1.1778 - val_accuracy: 0.6331\n",
      "Epoch 1899/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7000 - accuracy: 0.7737 - val_loss: 1.1730 - val_accuracy: 0.6299\n",
      "Epoch 1900/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6858 - accuracy: 0.7751 - val_loss: 1.1376 - val_accuracy: 0.6461\n",
      "Epoch 1901/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7001 - accuracy: 0.7737 - val_loss: 1.1135 - val_accuracy: 0.6526\n",
      "Epoch 1902/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6523 - accuracy: 0.7979 - val_loss: 1.0816 - val_accuracy: 0.6688\n",
      "Epoch 1903/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6957 - accuracy: 0.7803 - val_loss: 1.0616 - val_accuracy: 0.6558\n",
      "Epoch 1904/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6662 - accuracy: 0.7725 - val_loss: 1.0527 - val_accuracy: 0.6494\n",
      "Epoch 1905/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6976 - accuracy: 0.7793 - val_loss: 1.0602 - val_accuracy: 0.6526\n",
      "Epoch 1906/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7118 - accuracy: 0.7682 - val_loss: 1.0671 - val_accuracy: 0.6623\n",
      "Epoch 1907/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6673 - accuracy: 0.7891 - val_loss: 1.0802 - val_accuracy: 0.6591\n",
      "Epoch 1908/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6439 - accuracy: 0.7910 - val_loss: 1.0907 - val_accuracy: 0.6494\n",
      "Epoch 1909/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6337 - accuracy: 0.7905 - val_loss: 1.0852 - val_accuracy: 0.6526\n",
      "Epoch 1910/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6140 - accuracy: 0.7947 - val_loss: 1.0835 - val_accuracy: 0.6558\n",
      "Epoch 1911/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6348 - accuracy: 0.7988 - val_loss: 1.0830 - val_accuracy: 0.6591\n",
      "Epoch 1912/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6259 - accuracy: 0.7920 - val_loss: 1.0756 - val_accuracy: 0.6688\n",
      "Epoch 1913/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7136 - accuracy: 0.7500 - val_loss: 1.0766 - val_accuracy: 0.6688\n",
      "Epoch 1914/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6701 - accuracy: 0.7779 - val_loss: 1.0837 - val_accuracy: 0.6558\n",
      "Epoch 1915/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6902 - accuracy: 0.7754 - val_loss: 1.0956 - val_accuracy: 0.6558\n",
      "Epoch 1916/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6947 - accuracy: 0.7598 - val_loss: 1.1134 - val_accuracy: 0.6526\n",
      "Epoch 1917/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6229 - accuracy: 0.7891 - val_loss: 1.1368 - val_accuracy: 0.6429\n",
      "Epoch 1918/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6191 - accuracy: 0.7947 - val_loss: 1.1460 - val_accuracy: 0.6396\n",
      "Epoch 1919/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6380 - accuracy: 0.7849 - val_loss: 1.1432 - val_accuracy: 0.6396\n",
      "Epoch 1920/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6908 - accuracy: 0.7905 - val_loss: 1.1317 - val_accuracy: 0.6429\n",
      "Epoch 1921/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7101 - accuracy: 0.7695 - val_loss: 1.1085 - val_accuracy: 0.6591\n",
      "Epoch 1922/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6516 - accuracy: 0.7822 - val_loss: 1.0868 - val_accuracy: 0.6558\n",
      "Epoch 1923/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6344 - accuracy: 0.7871 - val_loss: 1.0669 - val_accuracy: 0.6623\n",
      "Epoch 1924/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6888 - accuracy: 0.7734 - val_loss: 1.0469 - val_accuracy: 0.6591\n",
      "Epoch 1925/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6453 - accuracy: 0.7949 - val_loss: 1.0353 - val_accuracy: 0.6656\n",
      "Epoch 1926/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6616 - accuracy: 0.7737 - val_loss: 1.0257 - val_accuracy: 0.6883\n",
      "Epoch 1927/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6574 - accuracy: 0.7734 - val_loss: 1.0259 - val_accuracy: 0.6981\n",
      "Epoch 1928/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6293 - accuracy: 0.8047 - val_loss: 1.0353 - val_accuracy: 0.6981\n",
      "Epoch 1929/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6559 - accuracy: 0.7852 - val_loss: 1.0461 - val_accuracy: 0.6948\n",
      "Epoch 1930/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6543 - accuracy: 0.7793 - val_loss: 1.0740 - val_accuracy: 0.6721\n",
      "Epoch 1931/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6917 - accuracy: 0.7682 - val_loss: 1.1113 - val_accuracy: 0.6494\n",
      "Epoch 1932/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6986 - accuracy: 0.7607 - val_loss: 1.1488 - val_accuracy: 0.6396\n",
      "Epoch 1933/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6173 - accuracy: 0.8128 - val_loss: 1.1615 - val_accuracy: 0.6429\n",
      "Epoch 1934/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6602 - accuracy: 0.7852 - val_loss: 1.1821 - val_accuracy: 0.6331\n",
      "Epoch 1935/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6489 - accuracy: 0.7900 - val_loss: 1.1949 - val_accuracy: 0.6234\n",
      "Epoch 1936/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6598 - accuracy: 0.7682 - val_loss: 1.1871 - val_accuracy: 0.6331\n",
      "Epoch 1937/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7069 - accuracy: 0.7676 - val_loss: 1.1832 - val_accuracy: 0.6331\n",
      "Epoch 1938/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6987 - accuracy: 0.7812 - val_loss: 1.1828 - val_accuracy: 0.6461\n",
      "Epoch 1939/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6231 - accuracy: 0.7905 - val_loss: 1.1630 - val_accuracy: 0.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1940/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7395 - accuracy: 0.7510 - val_loss: 1.1602 - val_accuracy: 0.6429\n",
      "Epoch 1941/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6232 - accuracy: 0.7998 - val_loss: 1.1572 - val_accuracy: 0.6494\n",
      "Epoch 1942/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6328 - accuracy: 0.7939 - val_loss: 1.1527 - val_accuracy: 0.6494\n",
      "Epoch 1943/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6757 - accuracy: 0.7646 - val_loss: 1.1453 - val_accuracy: 0.6461\n",
      "Epoch 1944/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5724 - accuracy: 0.8086 - val_loss: 1.1403 - val_accuracy: 0.6526\n",
      "Epoch 1945/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6594 - accuracy: 0.7803 - val_loss: 1.1455 - val_accuracy: 0.6494\n",
      "Epoch 1946/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6217 - accuracy: 0.7891 - val_loss: 1.1517 - val_accuracy: 0.6526\n",
      "Epoch 1947/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6464 - accuracy: 0.7754 - val_loss: 1.1640 - val_accuracy: 0.6396\n",
      "Epoch 1948/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6871 - accuracy: 0.7764 - val_loss: 1.1942 - val_accuracy: 0.6299\n",
      "Epoch 1949/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6779 - accuracy: 0.7584 - val_loss: 1.2240 - val_accuracy: 0.6266\n",
      "Epoch 1950/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6820 - accuracy: 0.7881 - val_loss: 1.2273 - val_accuracy: 0.6266\n",
      "Epoch 1951/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6035 - accuracy: 0.7947 - val_loss: 1.2329 - val_accuracy: 0.6201\n",
      "Epoch 1952/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5917 - accuracy: 0.8066 - val_loss: 1.2336 - val_accuracy: 0.6169\n",
      "Epoch 1953/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6179 - accuracy: 0.7988 - val_loss: 1.2210 - val_accuracy: 0.6136\n",
      "Epoch 1954/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6101 - accuracy: 0.8073 - val_loss: 1.2081 - val_accuracy: 0.6169\n",
      "Epoch 1955/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6481 - accuracy: 0.7891 - val_loss: 1.2089 - val_accuracy: 0.6136\n",
      "Epoch 1956/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6627 - accuracy: 0.7881 - val_loss: 1.1913 - val_accuracy: 0.6201\n",
      "Epoch 1957/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6652 - accuracy: 0.7842 - val_loss: 1.1710 - val_accuracy: 0.6169\n",
      "Epoch 1958/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6175 - accuracy: 0.7863 - val_loss: 1.1545 - val_accuracy: 0.6299\n",
      "Epoch 1959/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6217 - accuracy: 0.7933 - val_loss: 1.1524 - val_accuracy: 0.6429\n",
      "Epoch 1960/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6257 - accuracy: 0.7891 - val_loss: 1.1474 - val_accuracy: 0.6396\n",
      "Epoch 1961/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6554 - accuracy: 0.7979 - val_loss: 1.1423 - val_accuracy: 0.6364\n",
      "Epoch 1962/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6416 - accuracy: 0.7668 - val_loss: 1.1472 - val_accuracy: 0.6266\n",
      "Epoch 1963/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6287 - accuracy: 0.7939 - val_loss: 1.1517 - val_accuracy: 0.6234\n",
      "Epoch 1964/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5994 - accuracy: 0.8027 - val_loss: 1.1666 - val_accuracy: 0.6364\n",
      "Epoch 1965/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6893 - accuracy: 0.7910 - val_loss: 1.1816 - val_accuracy: 0.6461\n",
      "Epoch 1966/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6534 - accuracy: 0.7905 - val_loss: 1.1787 - val_accuracy: 0.6364\n",
      "Epoch 1967/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6382 - accuracy: 0.7705 - val_loss: 1.1790 - val_accuracy: 0.6331\n",
      "Epoch 1968/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6545 - accuracy: 0.7871 - val_loss: 1.1607 - val_accuracy: 0.6169\n",
      "Epoch 1969/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6088 - accuracy: 0.7947 - val_loss: 1.1437 - val_accuracy: 0.6266\n",
      "Epoch 1970/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6370 - accuracy: 0.8008 - val_loss: 1.1270 - val_accuracy: 0.6266\n",
      "Epoch 1971/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6705 - accuracy: 0.7773 - val_loss: 1.1221 - val_accuracy: 0.6234\n",
      "Epoch 1972/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6279 - accuracy: 0.8045 - val_loss: 1.1158 - val_accuracy: 0.6234\n",
      "Epoch 1973/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6028 - accuracy: 0.8017 - val_loss: 1.1155 - val_accuracy: 0.6299\n",
      "Epoch 1974/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6125 - accuracy: 0.7900 - val_loss: 1.1159 - val_accuracy: 0.6331\n",
      "Epoch 1975/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6798 - accuracy: 0.7627 - val_loss: 1.1146 - val_accuracy: 0.6331\n",
      "Epoch 1976/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6322 - accuracy: 0.7905 - val_loss: 1.1175 - val_accuracy: 0.6331\n",
      "Epoch 1977/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6560 - accuracy: 0.7933 - val_loss: 1.1400 - val_accuracy: 0.6364\n",
      "Epoch 1978/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6434 - accuracy: 0.7961 - val_loss: 1.1725 - val_accuracy: 0.6331\n",
      "Epoch 1979/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5769 - accuracy: 0.8115 - val_loss: 1.1892 - val_accuracy: 0.6201\n",
      "Epoch 1980/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6305 - accuracy: 0.7754 - val_loss: 1.1873 - val_accuracy: 0.6234\n",
      "Epoch 1981/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7076 - accuracy: 0.7682 - val_loss: 1.1473 - val_accuracy: 0.6299\n",
      "Epoch 1982/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6490 - accuracy: 0.7696 - val_loss: 1.0964 - val_accuracy: 0.6461\n",
      "Epoch 1983/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6693 - accuracy: 0.7682 - val_loss: 1.0586 - val_accuracy: 0.6558\n",
      "Epoch 1984/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6478 - accuracy: 0.7803 - val_loss: 1.0479 - val_accuracy: 0.6558\n",
      "Epoch 1985/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6192 - accuracy: 0.7852 - val_loss: 1.0535 - val_accuracy: 0.6656\n",
      "Epoch 1986/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6029 - accuracy: 0.7939 - val_loss: 1.0640 - val_accuracy: 0.6753\n",
      "Epoch 1987/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6287 - accuracy: 0.7852 - val_loss: 1.0618 - val_accuracy: 0.6721\n",
      "Epoch 1988/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6110 - accuracy: 0.8128 - val_loss: 1.0600 - val_accuracy: 0.6721\n",
      "Epoch 1989/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6048 - accuracy: 0.7979 - val_loss: 1.0546 - val_accuracy: 0.6558\n",
      "Epoch 1990/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6308 - accuracy: 0.7910 - val_loss: 1.0655 - val_accuracy: 0.6526\n",
      "Epoch 1991/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6615 - accuracy: 0.7933 - val_loss: 1.0914 - val_accuracy: 0.6558\n",
      "Epoch 1992/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6639 - accuracy: 0.7852 - val_loss: 1.1368 - val_accuracy: 0.6526\n",
      "Epoch 1993/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6288 - accuracy: 0.7900 - val_loss: 1.1773 - val_accuracy: 0.6526\n",
      "Epoch 1994/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6048 - accuracy: 0.8008 - val_loss: 1.1807 - val_accuracy: 0.6461\n",
      "Epoch 1995/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6683 - accuracy: 0.7919 - val_loss: 1.1850 - val_accuracy: 0.6396\n",
      "Epoch 1996/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5729 - accuracy: 0.8037 - val_loss: 1.1765 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1997/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6222 - accuracy: 0.7877 - val_loss: 1.1461 - val_accuracy: 0.6429\n",
      "Epoch 1998/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6324 - accuracy: 0.7933 - val_loss: 1.1219 - val_accuracy: 0.6331\n",
      "Epoch 1999/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6257 - accuracy: 0.7961 - val_loss: 1.1188 - val_accuracy: 0.6331\n",
      "Epoch 2000/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5651 - accuracy: 0.8115 - val_loss: 1.1225 - val_accuracy: 0.6299\n",
      "Epoch 2001/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6723 - accuracy: 0.7807 - val_loss: 1.1259 - val_accuracy: 0.6429\n",
      "Epoch 2002/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6680 - accuracy: 0.7779 - val_loss: 1.1271 - val_accuracy: 0.6461\n",
      "Epoch 2003/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6443 - accuracy: 0.7793 - val_loss: 1.1401 - val_accuracy: 0.6299\n",
      "Epoch 2004/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5864 - accuracy: 0.8105 - val_loss: 1.1631 - val_accuracy: 0.6396\n",
      "Epoch 2005/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6030 - accuracy: 0.8105 - val_loss: 1.1809 - val_accuracy: 0.6364\n",
      "Epoch 2006/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6235 - accuracy: 0.7959 - val_loss: 1.1909 - val_accuracy: 0.6234\n",
      "Epoch 2007/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6023 - accuracy: 0.8076 - val_loss: 1.1860 - val_accuracy: 0.6266\n",
      "Epoch 2008/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5994 - accuracy: 0.8047 - val_loss: 1.1843 - val_accuracy: 0.6331\n",
      "Epoch 2009/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6277 - accuracy: 0.7910 - val_loss: 1.1783 - val_accuracy: 0.6364\n",
      "Epoch 2010/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6402 - accuracy: 0.7920 - val_loss: 1.1749 - val_accuracy: 0.6331\n",
      "Epoch 2011/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6113 - accuracy: 0.8087 - val_loss: 1.1727 - val_accuracy: 0.6299\n",
      "Epoch 2012/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6504 - accuracy: 0.7793 - val_loss: 1.1832 - val_accuracy: 0.6234\n",
      "Epoch 2013/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5629 - accuracy: 0.8045 - val_loss: 1.2039 - val_accuracy: 0.6429\n",
      "Epoch 2014/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6032 - accuracy: 0.7975 - val_loss: 1.2338 - val_accuracy: 0.6364\n",
      "Epoch 2015/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5652 - accuracy: 0.8226 - val_loss: 1.2647 - val_accuracy: 0.6201\n",
      "Epoch 2016/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6201 - accuracy: 0.8008 - val_loss: 1.2956 - val_accuracy: 0.6201\n",
      "Epoch 2017/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6327 - accuracy: 0.7877 - val_loss: 1.3209 - val_accuracy: 0.6266\n",
      "Epoch 2018/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6602 - accuracy: 0.7949 - val_loss: 1.2958 - val_accuracy: 0.6299\n",
      "Epoch 2019/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6537 - accuracy: 0.7881 - val_loss: 1.2479 - val_accuracy: 0.6429\n",
      "Epoch 2020/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5784 - accuracy: 0.8101 - val_loss: 1.1893 - val_accuracy: 0.6396\n",
      "Epoch 2021/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5936 - accuracy: 0.7988 - val_loss: 1.1567 - val_accuracy: 0.6364\n",
      "Epoch 2022/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6643 - accuracy: 0.7871 - val_loss: 1.1434 - val_accuracy: 0.6331\n",
      "Epoch 2023/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6057 - accuracy: 0.7939 - val_loss: 1.1362 - val_accuracy: 0.6461\n",
      "Epoch 2024/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6128 - accuracy: 0.8003 - val_loss: 1.1360 - val_accuracy: 0.6623\n",
      "Epoch 2025/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5995 - accuracy: 0.7961 - val_loss: 1.1280 - val_accuracy: 0.6656\n",
      "Epoch 2026/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6071 - accuracy: 0.8105 - val_loss: 1.1122 - val_accuracy: 0.6656\n",
      "Epoch 2027/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6441 - accuracy: 0.7835 - val_loss: 1.1058 - val_accuracy: 0.6656\n",
      "Epoch 2028/4000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.6594 - accuracy: 0.7832 - val_loss: 1.1079 - val_accuracy: 0.6558\n",
      "Epoch 2029/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6269 - accuracy: 0.7959 - val_loss: 1.1164 - val_accuracy: 0.6526\n",
      "Epoch 2030/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6674 - accuracy: 0.7871 - val_loss: 1.1301 - val_accuracy: 0.6526\n",
      "Epoch 2031/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6243 - accuracy: 0.7863 - val_loss: 1.1160 - val_accuracy: 0.6591\n",
      "Epoch 2032/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6147 - accuracy: 0.7877 - val_loss: 1.0940 - val_accuracy: 0.6688\n",
      "Epoch 2033/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5932 - accuracy: 0.8066 - val_loss: 1.0883 - val_accuracy: 0.6623\n",
      "Epoch 2034/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6045 - accuracy: 0.7947 - val_loss: 1.0909 - val_accuracy: 0.6688\n",
      "Epoch 2035/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5878 - accuracy: 0.7988 - val_loss: 1.1063 - val_accuracy: 0.6721\n",
      "Epoch 2036/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6197 - accuracy: 0.8017 - val_loss: 1.1133 - val_accuracy: 0.6656\n",
      "Epoch 2037/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6653 - accuracy: 0.7877 - val_loss: 1.1108 - val_accuracy: 0.6688\n",
      "Epoch 2038/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6204 - accuracy: 0.8076 - val_loss: 1.0920 - val_accuracy: 0.6623\n",
      "Epoch 2039/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6522 - accuracy: 0.7900 - val_loss: 1.0725 - val_accuracy: 0.6656\n",
      "Epoch 2040/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5816 - accuracy: 0.7989 - val_loss: 1.0673 - val_accuracy: 0.6721\n",
      "Epoch 2041/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6534 - accuracy: 0.7891 - val_loss: 1.0671 - val_accuracy: 0.6721\n",
      "Epoch 2042/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6693 - accuracy: 0.7821 - val_loss: 1.0652 - val_accuracy: 0.6623\n",
      "Epoch 2043/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6878 - accuracy: 0.7807 - val_loss: 1.0847 - val_accuracy: 0.6461\n",
      "Epoch 2044/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6037 - accuracy: 0.8017 - val_loss: 1.1061 - val_accuracy: 0.6396\n",
      "Epoch 2045/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6200 - accuracy: 0.7947 - val_loss: 1.1308 - val_accuracy: 0.6429\n",
      "Epoch 2046/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6682 - accuracy: 0.7754 - val_loss: 1.1381 - val_accuracy: 0.6396\n",
      "Epoch 2047/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6006 - accuracy: 0.8027 - val_loss: 1.1354 - val_accuracy: 0.6234\n",
      "Epoch 2048/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6351 - accuracy: 0.8003 - val_loss: 1.1404 - val_accuracy: 0.6364\n",
      "Epoch 2049/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6214 - accuracy: 0.7988 - val_loss: 1.1302 - val_accuracy: 0.6429\n",
      "Epoch 2050/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6717 - accuracy: 0.7779 - val_loss: 1.1159 - val_accuracy: 0.6623\n",
      "Epoch 2051/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6414 - accuracy: 0.7835 - val_loss: 1.0964 - val_accuracy: 0.6656\n",
      "Epoch 2052/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6102 - accuracy: 0.7998 - val_loss: 1.0755 - val_accuracy: 0.6721\n",
      "Epoch 2053/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5958 - accuracy: 0.8087 - val_loss: 1.0691 - val_accuracy: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2054/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6987 - accuracy: 0.7514 - val_loss: 1.0716 - val_accuracy: 0.6461\n",
      "Epoch 2055/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6891 - accuracy: 0.7696 - val_loss: 1.0890 - val_accuracy: 0.6494\n",
      "Epoch 2056/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6145 - accuracy: 0.7905 - val_loss: 1.1069 - val_accuracy: 0.6558\n",
      "Epoch 2057/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5975 - accuracy: 0.7849 - val_loss: 1.1056 - val_accuracy: 0.6526\n",
      "Epoch 2058/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6340 - accuracy: 0.7861 - val_loss: 1.0996 - val_accuracy: 0.6656\n",
      "Epoch 2059/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6576 - accuracy: 0.7751 - val_loss: 1.0952 - val_accuracy: 0.6623\n",
      "Epoch 2060/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5606 - accuracy: 0.8170 - val_loss: 1.0982 - val_accuracy: 0.6753\n",
      "Epoch 2061/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6483 - accuracy: 0.7852 - val_loss: 1.0909 - val_accuracy: 0.6721\n",
      "Epoch 2062/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6162 - accuracy: 0.7998 - val_loss: 1.0763 - val_accuracy: 0.6786\n",
      "Epoch 2063/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6719 - accuracy: 0.7905 - val_loss: 1.0824 - val_accuracy: 0.6753\n",
      "Epoch 2064/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5795 - accuracy: 0.8057 - val_loss: 1.1044 - val_accuracy: 0.6558\n",
      "Epoch 2065/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6227 - accuracy: 0.7734 - val_loss: 1.1454 - val_accuracy: 0.6429\n",
      "Epoch 2066/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5943 - accuracy: 0.8008 - val_loss: 1.1715 - val_accuracy: 0.6299\n",
      "Epoch 2067/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5975 - accuracy: 0.8003 - val_loss: 1.1944 - val_accuracy: 0.6266\n",
      "Epoch 2068/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5994 - accuracy: 0.7905 - val_loss: 1.2138 - val_accuracy: 0.6266\n",
      "Epoch 2069/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5620 - accuracy: 0.8198 - val_loss: 1.2244 - val_accuracy: 0.6234\n",
      "Epoch 2070/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6805 - accuracy: 0.7723 - val_loss: 1.2396 - val_accuracy: 0.6299\n",
      "Epoch 2071/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5598 - accuracy: 0.8125 - val_loss: 1.2700 - val_accuracy: 0.6169\n",
      "Epoch 2072/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5945 - accuracy: 0.8047 - val_loss: 1.3145 - val_accuracy: 0.6169\n",
      "Epoch 2073/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6329 - accuracy: 0.8226 - val_loss: 1.3378 - val_accuracy: 0.6039\n",
      "Epoch 2074/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6615 - accuracy: 0.7871 - val_loss: 1.3808 - val_accuracy: 0.5844\n",
      "Epoch 2075/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6256 - accuracy: 0.7849 - val_loss: 1.4080 - val_accuracy: 0.5747\n",
      "Epoch 2076/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6520 - accuracy: 0.7988 - val_loss: 1.3801 - val_accuracy: 0.5779\n",
      "Epoch 2077/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5749 - accuracy: 0.7979 - val_loss: 1.3269 - val_accuracy: 0.5844\n",
      "Epoch 2078/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5774 - accuracy: 0.8076 - val_loss: 1.2853 - val_accuracy: 0.5877\n",
      "Epoch 2079/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5575 - accuracy: 0.8226 - val_loss: 1.2648 - val_accuracy: 0.5942\n",
      "Epoch 2080/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6252 - accuracy: 0.7979 - val_loss: 1.2565 - val_accuracy: 0.5909\n",
      "Epoch 2081/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5754 - accuracy: 0.8125 - val_loss: 1.2303 - val_accuracy: 0.6006\n",
      "Epoch 2082/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5804 - accuracy: 0.8115 - val_loss: 1.2470 - val_accuracy: 0.6234\n",
      "Epoch 2083/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5383 - accuracy: 0.8359 - val_loss: 1.2612 - val_accuracy: 0.6299\n",
      "Epoch 2084/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5598 - accuracy: 0.8170 - val_loss: 1.2540 - val_accuracy: 0.6364\n",
      "Epoch 2085/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5953 - accuracy: 0.8045 - val_loss: 1.2520 - val_accuracy: 0.6364\n",
      "Epoch 2086/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6140 - accuracy: 0.7900 - val_loss: 1.2613 - val_accuracy: 0.6331\n",
      "Epoch 2087/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5880 - accuracy: 0.8115 - val_loss: 1.2436 - val_accuracy: 0.6396\n",
      "Epoch 2088/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5940 - accuracy: 0.8086 - val_loss: 1.2341 - val_accuracy: 0.6299\n",
      "Epoch 2089/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6021 - accuracy: 0.7933 - val_loss: 1.2348 - val_accuracy: 0.6266\n",
      "Epoch 2090/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6346 - accuracy: 0.7961 - val_loss: 1.2052 - val_accuracy: 0.6364\n",
      "Epoch 2091/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6025 - accuracy: 0.8018 - val_loss: 1.1907 - val_accuracy: 0.6234\n",
      "Epoch 2092/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6161 - accuracy: 0.7989 - val_loss: 1.1742 - val_accuracy: 0.6266\n",
      "Epoch 2093/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5749 - accuracy: 0.8059 - val_loss: 1.1584 - val_accuracy: 0.6364\n",
      "Epoch 2094/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5213 - accuracy: 0.8240 - val_loss: 1.1651 - val_accuracy: 0.6266\n",
      "Epoch 2095/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5700 - accuracy: 0.8338 - val_loss: 1.1608 - val_accuracy: 0.6266\n",
      "Epoch 2096/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6231 - accuracy: 0.7933 - val_loss: 1.1585 - val_accuracy: 0.6364\n",
      "Epoch 2097/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5703 - accuracy: 0.8066 - val_loss: 1.1602 - val_accuracy: 0.6364\n",
      "Epoch 2098/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5658 - accuracy: 0.8076 - val_loss: 1.1663 - val_accuracy: 0.6201\n",
      "Epoch 2099/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6547 - accuracy: 0.7930 - val_loss: 1.1778 - val_accuracy: 0.6136\n",
      "Epoch 2100/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6742 - accuracy: 0.7905 - val_loss: 1.1710 - val_accuracy: 0.6136\n",
      "Epoch 2101/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5608 - accuracy: 0.8045 - val_loss: 1.1304 - val_accuracy: 0.6201\n",
      "Epoch 2102/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5932 - accuracy: 0.8027 - val_loss: 1.0940 - val_accuracy: 0.6266\n",
      "Epoch 2103/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6060 - accuracy: 0.8184 - val_loss: 1.0714 - val_accuracy: 0.6461\n",
      "Epoch 2104/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5887 - accuracy: 0.7961 - val_loss: 1.0649 - val_accuracy: 0.6396\n",
      "Epoch 2105/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6256 - accuracy: 0.8073 - val_loss: 1.0724 - val_accuracy: 0.6526\n",
      "Epoch 2106/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5951 - accuracy: 0.8027 - val_loss: 1.0923 - val_accuracy: 0.6429\n",
      "Epoch 2107/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6011 - accuracy: 0.7988 - val_loss: 1.1178 - val_accuracy: 0.6364\n",
      "Epoch 2108/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5785 - accuracy: 0.8047 - val_loss: 1.1611 - val_accuracy: 0.6234\n",
      "Epoch 2109/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6631 - accuracy: 0.7744 - val_loss: 1.2271 - val_accuracy: 0.6169\n",
      "Epoch 2110/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6158 - accuracy: 0.7881 - val_loss: 1.2742 - val_accuracy: 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2111/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5939 - accuracy: 0.8156 - val_loss: 1.2939 - val_accuracy: 0.6169\n",
      "Epoch 2112/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6493 - accuracy: 0.7920 - val_loss: 1.2474 - val_accuracy: 0.6234\n",
      "Epoch 2113/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5826 - accuracy: 0.8174 - val_loss: 1.2011 - val_accuracy: 0.6201\n",
      "Epoch 2114/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6505 - accuracy: 0.7919 - val_loss: 1.1880 - val_accuracy: 0.6364\n",
      "Epoch 2115/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6219 - accuracy: 0.7863 - val_loss: 1.1773 - val_accuracy: 0.6299\n",
      "Epoch 2116/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6493 - accuracy: 0.7988 - val_loss: 1.1483 - val_accuracy: 0.6364\n",
      "Epoch 2117/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5950 - accuracy: 0.8027 - val_loss: 1.1140 - val_accuracy: 0.6266\n",
      "Epoch 2118/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6012 - accuracy: 0.7905 - val_loss: 1.1073 - val_accuracy: 0.6331\n",
      "Epoch 2119/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6107 - accuracy: 0.7891 - val_loss: 1.0986 - val_accuracy: 0.6266\n",
      "Epoch 2120/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5847 - accuracy: 0.8128 - val_loss: 1.0985 - val_accuracy: 0.6331\n",
      "Epoch 2121/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6217 - accuracy: 0.7998 - val_loss: 1.1044 - val_accuracy: 0.6364\n",
      "Epoch 2122/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6023 - accuracy: 0.7910 - val_loss: 1.1268 - val_accuracy: 0.6331\n",
      "Epoch 2123/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6426 - accuracy: 0.8017 - val_loss: 1.1634 - val_accuracy: 0.6364\n",
      "Epoch 2124/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6005 - accuracy: 0.8008 - val_loss: 1.2293 - val_accuracy: 0.6201\n",
      "Epoch 2125/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6047 - accuracy: 0.8087 - val_loss: 1.3066 - val_accuracy: 0.6071\n",
      "Epoch 2126/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6312 - accuracy: 0.7961 - val_loss: 1.3422 - val_accuracy: 0.6071\n",
      "Epoch 2127/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5834 - accuracy: 0.8059 - val_loss: 1.3545 - val_accuracy: 0.5974\n",
      "Epoch 2128/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6113 - accuracy: 0.7939 - val_loss: 1.3393 - val_accuracy: 0.6104\n",
      "Epoch 2129/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5982 - accuracy: 0.8145 - val_loss: 1.3033 - val_accuracy: 0.6266\n",
      "Epoch 2130/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6045 - accuracy: 0.7989 - val_loss: 1.2729 - val_accuracy: 0.6104\n",
      "Epoch 2131/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5885 - accuracy: 0.8008 - val_loss: 1.2244 - val_accuracy: 0.6364\n",
      "Epoch 2132/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6045 - accuracy: 0.8105 - val_loss: 1.1893 - val_accuracy: 0.6364\n",
      "Epoch 2133/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5779 - accuracy: 0.8003 - val_loss: 1.1821 - val_accuracy: 0.6299\n",
      "Epoch 2134/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6138 - accuracy: 0.8101 - val_loss: 1.2082 - val_accuracy: 0.6299\n",
      "Epoch 2135/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5857 - accuracy: 0.7998 - val_loss: 1.2398 - val_accuracy: 0.6136\n",
      "Epoch 2136/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5977 - accuracy: 0.8045 - val_loss: 1.2934 - val_accuracy: 0.6169\n",
      "Epoch 2137/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5809 - accuracy: 0.8086 - val_loss: 1.3540 - val_accuracy: 0.6039\n",
      "Epoch 2138/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5960 - accuracy: 0.8154 - val_loss: 1.4205 - val_accuracy: 0.5877\n",
      "Epoch 2139/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6415 - accuracy: 0.7989 - val_loss: 1.4881 - val_accuracy: 0.5747\n",
      "Epoch 2140/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5531 - accuracy: 0.8320 - val_loss: 1.5249 - val_accuracy: 0.5714\n",
      "Epoch 2141/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5743 - accuracy: 0.8017 - val_loss: 1.5189 - val_accuracy: 0.5812\n",
      "Epoch 2142/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6282 - accuracy: 0.7849 - val_loss: 1.4935 - val_accuracy: 0.5909\n",
      "Epoch 2143/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5956 - accuracy: 0.8017 - val_loss: 1.4698 - val_accuracy: 0.6006\n",
      "Epoch 2144/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5687 - accuracy: 0.8291 - val_loss: 1.4186 - val_accuracy: 0.6104\n",
      "Epoch 2145/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5652 - accuracy: 0.8240 - val_loss: 1.3742 - val_accuracy: 0.5974\n",
      "Epoch 2146/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5710 - accuracy: 0.8115 - val_loss: 1.3366 - val_accuracy: 0.6006\n",
      "Epoch 2147/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6524 - accuracy: 0.7877 - val_loss: 1.2951 - val_accuracy: 0.6331\n",
      "Epoch 2148/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5227 - accuracy: 0.8242 - val_loss: 1.2822 - val_accuracy: 0.6299\n",
      "Epoch 2149/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6139 - accuracy: 0.8174 - val_loss: 1.2852 - val_accuracy: 0.6299\n",
      "Epoch 2150/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6164 - accuracy: 0.7905 - val_loss: 1.3050 - val_accuracy: 0.6169\n",
      "Epoch 2151/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6074 - accuracy: 0.8059 - val_loss: 1.3446 - val_accuracy: 0.6169\n",
      "Epoch 2152/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5975 - accuracy: 0.8101 - val_loss: 1.3746 - val_accuracy: 0.6266\n",
      "Epoch 2153/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6215 - accuracy: 0.7910 - val_loss: 1.3914 - val_accuracy: 0.6299\n",
      "Epoch 2154/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6038 - accuracy: 0.7905 - val_loss: 1.3549 - val_accuracy: 0.6234\n",
      "Epoch 2155/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5619 - accuracy: 0.8135 - val_loss: 1.3093 - val_accuracy: 0.6364\n",
      "Epoch 2156/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5903 - accuracy: 0.8086 - val_loss: 1.2815 - val_accuracy: 0.6266\n",
      "Epoch 2157/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5406 - accuracy: 0.8254 - val_loss: 1.2402 - val_accuracy: 0.6234\n",
      "Epoch 2158/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5867 - accuracy: 0.7835 - val_loss: 1.2156 - val_accuracy: 0.6299\n",
      "Epoch 2159/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5804 - accuracy: 0.8045 - val_loss: 1.2051 - val_accuracy: 0.6299\n",
      "Epoch 2160/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5845 - accuracy: 0.8154 - val_loss: 1.2089 - val_accuracy: 0.6396\n",
      "Epoch 2161/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6084 - accuracy: 0.7949 - val_loss: 1.2229 - val_accuracy: 0.6461\n",
      "Epoch 2162/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5747 - accuracy: 0.8047 - val_loss: 1.2411 - val_accuracy: 0.6526\n",
      "Epoch 2163/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6043 - accuracy: 0.8057 - val_loss: 1.2634 - val_accuracy: 0.6558\n",
      "Epoch 2164/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5659 - accuracy: 0.8128 - val_loss: 1.2724 - val_accuracy: 0.6526\n",
      "Epoch 2165/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6155 - accuracy: 0.8037 - val_loss: 1.3033 - val_accuracy: 0.6364\n",
      "Epoch 2166/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6488 - accuracy: 0.7849 - val_loss: 1.3414 - val_accuracy: 0.6104\n",
      "Epoch 2167/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5882 - accuracy: 0.8135 - val_loss: 1.3458 - val_accuracy: 0.6169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2168/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5220 - accuracy: 0.8352 - val_loss: 1.3075 - val_accuracy: 0.6136\n",
      "Epoch 2169/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6076 - accuracy: 0.7975 - val_loss: 1.2299 - val_accuracy: 0.6201\n",
      "Epoch 2170/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5848 - accuracy: 0.8087 - val_loss: 1.1578 - val_accuracy: 0.6201\n",
      "Epoch 2171/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6275 - accuracy: 0.8031 - val_loss: 1.1170 - val_accuracy: 0.6266\n",
      "Epoch 2172/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6047 - accuracy: 0.7989 - val_loss: 1.1024 - val_accuracy: 0.6429\n",
      "Epoch 2173/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5401 - accuracy: 0.8198 - val_loss: 1.1114 - val_accuracy: 0.6429\n",
      "Epoch 2174/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5502 - accuracy: 0.8174 - val_loss: 1.1282 - val_accuracy: 0.6429\n",
      "Epoch 2175/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6588 - accuracy: 0.7877 - val_loss: 1.1605 - val_accuracy: 0.6494\n",
      "Epoch 2176/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5638 - accuracy: 0.8115 - val_loss: 1.2051 - val_accuracy: 0.6396\n",
      "Epoch 2177/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5889 - accuracy: 0.8066 - val_loss: 1.2397 - val_accuracy: 0.6234\n",
      "Epoch 2178/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5501 - accuracy: 0.8203 - val_loss: 1.2812 - val_accuracy: 0.6104\n",
      "Epoch 2179/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6007 - accuracy: 0.8018 - val_loss: 1.3077 - val_accuracy: 0.6006\n",
      "Epoch 2180/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5703 - accuracy: 0.8128 - val_loss: 1.3172 - val_accuracy: 0.5974\n",
      "Epoch 2181/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5379 - accuracy: 0.8170 - val_loss: 1.2796 - val_accuracy: 0.6071\n",
      "Epoch 2182/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5308 - accuracy: 0.8296 - val_loss: 1.2358 - val_accuracy: 0.6104\n",
      "Epoch 2183/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5535 - accuracy: 0.8142 - val_loss: 1.2000 - val_accuracy: 0.6234\n",
      "Epoch 2184/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5664 - accuracy: 0.8170 - val_loss: 1.1791 - val_accuracy: 0.6461\n",
      "Epoch 2185/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5672 - accuracy: 0.8142 - val_loss: 1.1662 - val_accuracy: 0.6526\n",
      "Epoch 2186/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5489 - accuracy: 0.8271 - val_loss: 1.1547 - val_accuracy: 0.6526\n",
      "Epoch 2187/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5527 - accuracy: 0.8184 - val_loss: 1.1548 - val_accuracy: 0.6396\n",
      "Epoch 2188/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4816 - accuracy: 0.8492 - val_loss: 1.1574 - val_accuracy: 0.6396\n",
      "Epoch 2189/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5251 - accuracy: 0.8184 - val_loss: 1.1750 - val_accuracy: 0.6331\n",
      "Epoch 2190/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6566 - accuracy: 0.7783 - val_loss: 1.1951 - val_accuracy: 0.6461\n",
      "Epoch 2191/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5659 - accuracy: 0.8135 - val_loss: 1.2019 - val_accuracy: 0.6429\n",
      "Epoch 2192/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5798 - accuracy: 0.8045 - val_loss: 1.1902 - val_accuracy: 0.6429\n",
      "Epoch 2193/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6194 - accuracy: 0.8017 - val_loss: 1.1762 - val_accuracy: 0.6429\n",
      "Epoch 2194/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6435 - accuracy: 0.7919 - val_loss: 1.1629 - val_accuracy: 0.6429\n",
      "Epoch 2195/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5403 - accuracy: 0.8213 - val_loss: 1.1561 - val_accuracy: 0.6526\n",
      "Epoch 2196/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6026 - accuracy: 0.8125 - val_loss: 1.1646 - val_accuracy: 0.6558\n",
      "Epoch 2197/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5551 - accuracy: 0.8115 - val_loss: 1.1794 - val_accuracy: 0.6494\n",
      "Epoch 2198/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6114 - accuracy: 0.7949 - val_loss: 1.1878 - val_accuracy: 0.6494\n",
      "Epoch 2199/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5633 - accuracy: 0.7933 - val_loss: 1.2040 - val_accuracy: 0.6429\n",
      "Epoch 2200/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5806 - accuracy: 0.8156 - val_loss: 1.2403 - val_accuracy: 0.6201\n",
      "Epoch 2201/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5187 - accuracy: 0.8324 - val_loss: 1.2808 - val_accuracy: 0.6136\n",
      "Epoch 2202/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5258 - accuracy: 0.8125 - val_loss: 1.3076 - val_accuracy: 0.6006\n",
      "Epoch 2203/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5616 - accuracy: 0.8223 - val_loss: 1.3282 - val_accuracy: 0.6006\n",
      "Epoch 2204/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5527 - accuracy: 0.8105 - val_loss: 1.3487 - val_accuracy: 0.5812\n",
      "Epoch 2205/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5677 - accuracy: 0.8086 - val_loss: 1.3631 - val_accuracy: 0.5844\n",
      "Epoch 2206/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6053 - accuracy: 0.7959 - val_loss: 1.3446 - val_accuracy: 0.5812\n",
      "Epoch 2207/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5787 - accuracy: 0.8145 - val_loss: 1.3166 - val_accuracy: 0.5844\n",
      "Epoch 2208/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5841 - accuracy: 0.8198 - val_loss: 1.2742 - val_accuracy: 0.5844\n",
      "Epoch 2209/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6154 - accuracy: 0.7900 - val_loss: 1.2482 - val_accuracy: 0.5844\n",
      "Epoch 2210/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5960 - accuracy: 0.8037 - val_loss: 1.2459 - val_accuracy: 0.6039\n",
      "Epoch 2211/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5614 - accuracy: 0.8115 - val_loss: 1.2439 - val_accuracy: 0.6201\n",
      "Epoch 2212/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5583 - accuracy: 0.8017 - val_loss: 1.2581 - val_accuracy: 0.6331\n",
      "Epoch 2213/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5009 - accuracy: 0.8394 - val_loss: 1.2877 - val_accuracy: 0.6331\n",
      "Epoch 2214/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5549 - accuracy: 0.8045 - val_loss: 1.3119 - val_accuracy: 0.6201\n",
      "Epoch 2215/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5793 - accuracy: 0.8073 - val_loss: 1.3182 - val_accuracy: 0.6136\n",
      "Epoch 2216/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5926 - accuracy: 0.8037 - val_loss: 1.3024 - val_accuracy: 0.6169\n",
      "Epoch 2217/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6305 - accuracy: 0.7920 - val_loss: 1.2799 - val_accuracy: 0.6299\n",
      "Epoch 2218/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5754 - accuracy: 0.8128 - val_loss: 1.2435 - val_accuracy: 0.6494\n",
      "Epoch 2219/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5174 - accuracy: 0.8198 - val_loss: 1.2027 - val_accuracy: 0.6429\n",
      "Epoch 2220/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5504 - accuracy: 0.8115 - val_loss: 1.1834 - val_accuracy: 0.6396\n",
      "Epoch 2221/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6003 - accuracy: 0.8115 - val_loss: 1.1715 - val_accuracy: 0.6429\n",
      "Epoch 2222/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5549 - accuracy: 0.8198 - val_loss: 1.1663 - val_accuracy: 0.6364\n",
      "Epoch 2223/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5617 - accuracy: 0.8324 - val_loss: 1.1697 - val_accuracy: 0.6331\n",
      "Epoch 2224/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5642 - accuracy: 0.8170 - val_loss: 1.1920 - val_accuracy: 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2225/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5101 - accuracy: 0.8142 - val_loss: 1.2499 - val_accuracy: 0.6136\n",
      "Epoch 2226/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4849 - accuracy: 0.8478 - val_loss: 1.2883 - val_accuracy: 0.6104\n",
      "Epoch 2227/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5387 - accuracy: 0.8232 - val_loss: 1.3385 - val_accuracy: 0.5974\n",
      "Epoch 2228/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5934 - accuracy: 0.8125 - val_loss: 1.3534 - val_accuracy: 0.5877\n",
      "Epoch 2229/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5838 - accuracy: 0.7989 - val_loss: 1.3188 - val_accuracy: 0.6071\n",
      "Epoch 2230/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5160 - accuracy: 0.8311 - val_loss: 1.2892 - val_accuracy: 0.6039\n",
      "Epoch 2231/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5489 - accuracy: 0.8223 - val_loss: 1.2573 - val_accuracy: 0.6104\n",
      "Epoch 2232/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5361 - accuracy: 0.8059 - val_loss: 1.2483 - val_accuracy: 0.6104\n",
      "Epoch 2233/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5142 - accuracy: 0.8464 - val_loss: 1.2332 - val_accuracy: 0.6266\n",
      "Epoch 2234/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5677 - accuracy: 0.8059 - val_loss: 1.2281 - val_accuracy: 0.6234\n",
      "Epoch 2235/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5238 - accuracy: 0.8268 - val_loss: 1.2219 - val_accuracy: 0.6364\n",
      "Epoch 2236/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5651 - accuracy: 0.8096 - val_loss: 1.2287 - val_accuracy: 0.6494\n",
      "Epoch 2237/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5487 - accuracy: 0.8115 - val_loss: 1.2586 - val_accuracy: 0.6331\n",
      "Epoch 2238/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5877 - accuracy: 0.8115 - val_loss: 1.3033 - val_accuracy: 0.6136\n",
      "Epoch 2239/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5614 - accuracy: 0.8193 - val_loss: 1.3210 - val_accuracy: 0.6136\n",
      "Epoch 2240/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5592 - accuracy: 0.8156 - val_loss: 1.2919 - val_accuracy: 0.6266\n",
      "Epoch 2241/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5638 - accuracy: 0.8115 - val_loss: 1.2663 - val_accuracy: 0.6331\n",
      "Epoch 2242/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5383 - accuracy: 0.8242 - val_loss: 1.2527 - val_accuracy: 0.6234\n",
      "Epoch 2243/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5296 - accuracy: 0.8282 - val_loss: 1.2636 - val_accuracy: 0.6331\n",
      "Epoch 2244/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4746 - accuracy: 0.8438 - val_loss: 1.2713 - val_accuracy: 0.6169\n",
      "Epoch 2245/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5534 - accuracy: 0.8203 - val_loss: 1.2977 - val_accuracy: 0.6006\n",
      "Epoch 2246/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5777 - accuracy: 0.8145 - val_loss: 1.3332 - val_accuracy: 0.5942\n",
      "Epoch 2247/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5429 - accuracy: 0.8115 - val_loss: 1.3837 - val_accuracy: 0.5877\n",
      "Epoch 2248/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5589 - accuracy: 0.8037 - val_loss: 1.4336 - val_accuracy: 0.5747\n",
      "Epoch 2249/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5300 - accuracy: 0.8203 - val_loss: 1.4628 - val_accuracy: 0.5682\n",
      "Epoch 2250/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5445 - accuracy: 0.8059 - val_loss: 1.4469 - val_accuracy: 0.5844\n",
      "Epoch 2251/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5233 - accuracy: 0.8436 - val_loss: 1.3995 - val_accuracy: 0.5974\n",
      "Epoch 2252/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5398 - accuracy: 0.8170 - val_loss: 1.3484 - val_accuracy: 0.6039\n",
      "Epoch 2253/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5227 - accuracy: 0.8203 - val_loss: 1.2810 - val_accuracy: 0.6071\n",
      "Epoch 2254/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5990 - accuracy: 0.8073 - val_loss: 1.2283 - val_accuracy: 0.6299\n",
      "Epoch 2255/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5975 - accuracy: 0.7949 - val_loss: 1.2116 - val_accuracy: 0.6364\n",
      "Epoch 2256/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5542 - accuracy: 0.8105 - val_loss: 1.2012 - val_accuracy: 0.6299\n",
      "Epoch 2257/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5588 - accuracy: 0.8045 - val_loss: 1.1931 - val_accuracy: 0.6331\n",
      "Epoch 2258/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5525 - accuracy: 0.8115 - val_loss: 1.1874 - val_accuracy: 0.6364\n",
      "Epoch 2259/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5695 - accuracy: 0.8135 - val_loss: 1.2114 - val_accuracy: 0.6201\n",
      "Epoch 2260/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5635 - accuracy: 0.8059 - val_loss: 1.2949 - val_accuracy: 0.6104\n",
      "Epoch 2261/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5344 - accuracy: 0.8301 - val_loss: 1.3870 - val_accuracy: 0.5877\n",
      "Epoch 2262/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5217 - accuracy: 0.8296 - val_loss: 1.4364 - val_accuracy: 0.5779\n",
      "Epoch 2263/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5318 - accuracy: 0.8213 - val_loss: 1.4836 - val_accuracy: 0.5682\n",
      "Epoch 2264/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5720 - accuracy: 0.8142 - val_loss: 1.5297 - val_accuracy: 0.5552\n",
      "Epoch 2265/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5144 - accuracy: 0.8174 - val_loss: 1.5133 - val_accuracy: 0.5584\n",
      "Epoch 2266/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5974 - accuracy: 0.8045 - val_loss: 1.4938 - val_accuracy: 0.5714\n",
      "Epoch 2267/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4397 - accuracy: 0.8436 - val_loss: 1.4696 - val_accuracy: 0.5714\n",
      "Epoch 2268/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5167 - accuracy: 0.8352 - val_loss: 1.4368 - val_accuracy: 0.5747\n",
      "Epoch 2269/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5125 - accuracy: 0.8310 - val_loss: 1.3738 - val_accuracy: 0.5974\n",
      "Epoch 2270/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5324 - accuracy: 0.8045 - val_loss: 1.3220 - val_accuracy: 0.6039\n",
      "Epoch 2271/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5519 - accuracy: 0.8240 - val_loss: 1.2609 - val_accuracy: 0.6234\n",
      "Epoch 2272/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6152 - accuracy: 0.8096 - val_loss: 1.2323 - val_accuracy: 0.6234\n",
      "Epoch 2273/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5556 - accuracy: 0.8135 - val_loss: 1.2370 - val_accuracy: 0.6364\n",
      "Epoch 2274/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5035 - accuracy: 0.8422 - val_loss: 1.2688 - val_accuracy: 0.6201\n",
      "Epoch 2275/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5335 - accuracy: 0.8436 - val_loss: 1.3274 - val_accuracy: 0.6006\n",
      "Epoch 2276/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5770 - accuracy: 0.8086 - val_loss: 1.4008 - val_accuracy: 0.5812\n",
      "Epoch 2277/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5850 - accuracy: 0.7949 - val_loss: 1.4926 - val_accuracy: 0.5747\n",
      "Epoch 2278/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5539 - accuracy: 0.8174 - val_loss: 1.5633 - val_accuracy: 0.5682\n",
      "Epoch 2279/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5396 - accuracy: 0.8101 - val_loss: 1.5949 - val_accuracy: 0.5617\n",
      "Epoch 2280/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5109 - accuracy: 0.8320 - val_loss: 1.6235 - val_accuracy: 0.5519\n",
      "Epoch 2281/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5735 - accuracy: 0.8142 - val_loss: 1.5813 - val_accuracy: 0.5617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2282/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5545 - accuracy: 0.8213 - val_loss: 1.5443 - val_accuracy: 0.5584\n",
      "Epoch 2283/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5384 - accuracy: 0.8213 - val_loss: 1.4767 - val_accuracy: 0.5682\n",
      "Epoch 2284/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5254 - accuracy: 0.8198 - val_loss: 1.4425 - val_accuracy: 0.5682\n",
      "Epoch 2285/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5209 - accuracy: 0.8320 - val_loss: 1.3988 - val_accuracy: 0.5714\n",
      "Epoch 2286/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5201 - accuracy: 0.8338 - val_loss: 1.3720 - val_accuracy: 0.5649\n",
      "Epoch 2287/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5206 - accuracy: 0.8262 - val_loss: 1.3398 - val_accuracy: 0.5779\n",
      "Epoch 2288/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5367 - accuracy: 0.8240 - val_loss: 1.3128 - val_accuracy: 0.5844\n",
      "Epoch 2289/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5924 - accuracy: 0.8115 - val_loss: 1.2809 - val_accuracy: 0.5974\n",
      "Epoch 2290/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6194 - accuracy: 0.7877 - val_loss: 1.2769 - val_accuracy: 0.5974\n",
      "Epoch 2291/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5178 - accuracy: 0.8379 - val_loss: 1.2853 - val_accuracy: 0.6136\n",
      "Epoch 2292/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5509 - accuracy: 0.8226 - val_loss: 1.3106 - val_accuracy: 0.5974\n",
      "Epoch 2293/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5438 - accuracy: 0.8281 - val_loss: 1.3080 - val_accuracy: 0.5909\n",
      "Epoch 2294/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5740 - accuracy: 0.7919 - val_loss: 1.3069 - val_accuracy: 0.5974\n",
      "Epoch 2295/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5304 - accuracy: 0.8252 - val_loss: 1.3015 - val_accuracy: 0.5974\n",
      "Epoch 2296/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5717 - accuracy: 0.8017 - val_loss: 1.2706 - val_accuracy: 0.6071\n",
      "Epoch 2297/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5391 - accuracy: 0.8198 - val_loss: 1.2558 - val_accuracy: 0.6136\n",
      "Epoch 2298/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5647 - accuracy: 0.8170 - val_loss: 1.2725 - val_accuracy: 0.6071\n",
      "Epoch 2299/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5556 - accuracy: 0.8135 - val_loss: 1.2880 - val_accuracy: 0.6006\n",
      "Epoch 2300/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5562 - accuracy: 0.8212 - val_loss: 1.2952 - val_accuracy: 0.5942\n",
      "Epoch 2301/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5707 - accuracy: 0.8184 - val_loss: 1.2881 - val_accuracy: 0.6006\n",
      "Epoch 2302/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5703 - accuracy: 0.8135 - val_loss: 1.3061 - val_accuracy: 0.5974\n",
      "Epoch 2303/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5808 - accuracy: 0.8115 - val_loss: 1.3380 - val_accuracy: 0.6071\n",
      "Epoch 2304/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5508 - accuracy: 0.8101 - val_loss: 1.3437 - val_accuracy: 0.6039\n",
      "Epoch 2305/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5598 - accuracy: 0.8076 - val_loss: 1.3029 - val_accuracy: 0.6104\n",
      "Epoch 2306/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5120 - accuracy: 0.8359 - val_loss: 1.2783 - val_accuracy: 0.6169\n",
      "Epoch 2307/4000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5819 - accuracy: 0.8076 - val_loss: 1.2761 - val_accuracy: 0.6136\n",
      "Epoch 2308/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4979 - accuracy: 0.8291 - val_loss: 1.2654 - val_accuracy: 0.6266\n",
      "Epoch 2309/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5241 - accuracy: 0.8198 - val_loss: 1.2682 - val_accuracy: 0.6364\n",
      "Epoch 2310/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5570 - accuracy: 0.8174 - val_loss: 1.2632 - val_accuracy: 0.6364\n",
      "Epoch 2311/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5790 - accuracy: 0.8115 - val_loss: 1.2664 - val_accuracy: 0.6266\n",
      "Epoch 2312/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5501 - accuracy: 0.8226 - val_loss: 1.2718 - val_accuracy: 0.6364\n",
      "Epoch 2313/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5240 - accuracy: 0.8268 - val_loss: 1.2628 - val_accuracy: 0.6104\n",
      "Epoch 2314/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5121 - accuracy: 0.8291 - val_loss: 1.2788 - val_accuracy: 0.6136\n",
      "Epoch 2315/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5037 - accuracy: 0.8282 - val_loss: 1.2820 - val_accuracy: 0.6169\n",
      "Epoch 2316/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5294 - accuracy: 0.8193 - val_loss: 1.2715 - val_accuracy: 0.6169\n",
      "Epoch 2317/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5462 - accuracy: 0.8311 - val_loss: 1.2379 - val_accuracy: 0.6169\n",
      "Epoch 2318/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5654 - accuracy: 0.8164 - val_loss: 1.2316 - val_accuracy: 0.6234\n",
      "Epoch 2319/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5402 - accuracy: 0.8242 - val_loss: 1.2243 - val_accuracy: 0.6331\n",
      "Epoch 2320/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5635 - accuracy: 0.8018 - val_loss: 1.2245 - val_accuracy: 0.6299\n",
      "Epoch 2321/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5390 - accuracy: 0.8291 - val_loss: 1.2125 - val_accuracy: 0.6364\n",
      "Epoch 2322/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5600 - accuracy: 0.8271 - val_loss: 1.2069 - val_accuracy: 0.6331\n",
      "Epoch 2323/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5285 - accuracy: 0.8389 - val_loss: 1.2177 - val_accuracy: 0.6299\n",
      "Epoch 2324/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5205 - accuracy: 0.8324 - val_loss: 1.2546 - val_accuracy: 0.6039\n",
      "Epoch 2325/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5283 - accuracy: 0.8330 - val_loss: 1.3403 - val_accuracy: 0.5682\n",
      "Epoch 2326/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5431 - accuracy: 0.8296 - val_loss: 1.4677 - val_accuracy: 0.5519\n",
      "Epoch 2327/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5233 - accuracy: 0.8268 - val_loss: 1.6472 - val_accuracy: 0.5292\n",
      "Epoch 2328/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5937 - accuracy: 0.8047 - val_loss: 1.7508 - val_accuracy: 0.5032\n",
      "Epoch 2329/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5608 - accuracy: 0.8105 - val_loss: 1.7802 - val_accuracy: 0.5032\n",
      "Epoch 2330/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5541 - accuracy: 0.8135 - val_loss: 1.7482 - val_accuracy: 0.5130\n",
      "Epoch 2331/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5646 - accuracy: 0.8170 - val_loss: 1.6372 - val_accuracy: 0.5260\n",
      "Epoch 2332/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5232 - accuracy: 0.8320 - val_loss: 1.5057 - val_accuracy: 0.5714\n",
      "Epoch 2333/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5629 - accuracy: 0.8184 - val_loss: 1.3989 - val_accuracy: 0.5942\n",
      "Epoch 2334/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4580 - accuracy: 0.8366 - val_loss: 1.3089 - val_accuracy: 0.6006\n",
      "Epoch 2335/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5037 - accuracy: 0.8492 - val_loss: 1.2647 - val_accuracy: 0.6071\n",
      "Epoch 2336/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5322 - accuracy: 0.8262 - val_loss: 1.2351 - val_accuracy: 0.6234\n",
      "Epoch 2337/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5262 - accuracy: 0.8193 - val_loss: 1.2175 - val_accuracy: 0.6266\n",
      "Epoch 2338/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5233 - accuracy: 0.8320 - val_loss: 1.1968 - val_accuracy: 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2339/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5649 - accuracy: 0.8125 - val_loss: 1.1847 - val_accuracy: 0.6364\n",
      "Epoch 2340/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4812 - accuracy: 0.8324 - val_loss: 1.1695 - val_accuracy: 0.6494\n",
      "Epoch 2341/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4824 - accuracy: 0.8352 - val_loss: 1.1769 - val_accuracy: 0.6558\n",
      "Epoch 2342/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4664 - accuracy: 0.8350 - val_loss: 1.1943 - val_accuracy: 0.6591\n",
      "Epoch 2343/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5115 - accuracy: 0.8389 - val_loss: 1.2108 - val_accuracy: 0.6526\n",
      "Epoch 2344/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5138 - accuracy: 0.8422 - val_loss: 1.1980 - val_accuracy: 0.6461\n",
      "Epoch 2345/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5336 - accuracy: 0.8213 - val_loss: 1.1897 - val_accuracy: 0.6526\n",
      "Epoch 2346/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5212 - accuracy: 0.8156 - val_loss: 1.1842 - val_accuracy: 0.6494\n",
      "Epoch 2347/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5435 - accuracy: 0.8394 - val_loss: 1.1678 - val_accuracy: 0.6591\n",
      "Epoch 2348/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4533 - accuracy: 0.8575 - val_loss: 1.1630 - val_accuracy: 0.6558\n",
      "Epoch 2349/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5078 - accuracy: 0.8350 - val_loss: 1.1597 - val_accuracy: 0.6494\n",
      "Epoch 2350/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4895 - accuracy: 0.8467 - val_loss: 1.1699 - val_accuracy: 0.6494\n",
      "Epoch 2351/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5495 - accuracy: 0.8301 - val_loss: 1.1799 - val_accuracy: 0.6429\n",
      "Epoch 2352/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5114 - accuracy: 0.8340 - val_loss: 1.1971 - val_accuracy: 0.6266\n",
      "Epoch 2353/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5389 - accuracy: 0.8184 - val_loss: 1.2238 - val_accuracy: 0.6201\n",
      "Epoch 2354/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4849 - accuracy: 0.8359 - val_loss: 1.2473 - val_accuracy: 0.6201\n",
      "Epoch 2355/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4831 - accuracy: 0.8268 - val_loss: 1.2591 - val_accuracy: 0.6234\n",
      "Epoch 2356/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5479 - accuracy: 0.8226 - val_loss: 1.2362 - val_accuracy: 0.6234\n",
      "Epoch 2357/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5264 - accuracy: 0.8282 - val_loss: 1.1758 - val_accuracy: 0.6364\n",
      "Epoch 2358/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5238 - accuracy: 0.8359 - val_loss: 1.1285 - val_accuracy: 0.6526\n",
      "Epoch 2359/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5314 - accuracy: 0.8212 - val_loss: 1.1159 - val_accuracy: 0.6623\n",
      "Epoch 2360/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5333 - accuracy: 0.8170 - val_loss: 1.1189 - val_accuracy: 0.6558\n",
      "Epoch 2361/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5234 - accuracy: 0.8226 - val_loss: 1.1234 - val_accuracy: 0.6656\n",
      "Epoch 2362/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4845 - accuracy: 0.8589 - val_loss: 1.1191 - val_accuracy: 0.6688\n",
      "Epoch 2363/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5206 - accuracy: 0.8350 - val_loss: 1.1182 - val_accuracy: 0.6656\n",
      "Epoch 2364/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4897 - accuracy: 0.8369 - val_loss: 1.1206 - val_accuracy: 0.6753\n",
      "Epoch 2365/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5751 - accuracy: 0.8135 - val_loss: 1.1130 - val_accuracy: 0.6786\n",
      "Epoch 2366/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4907 - accuracy: 0.8506 - val_loss: 1.1069 - val_accuracy: 0.6753\n",
      "Epoch 2367/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5936 - accuracy: 0.8031 - val_loss: 1.1044 - val_accuracy: 0.6818\n",
      "Epoch 2368/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5381 - accuracy: 0.8125 - val_loss: 1.1104 - val_accuracy: 0.6688\n",
      "Epoch 2369/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5057 - accuracy: 0.8232 - val_loss: 1.1212 - val_accuracy: 0.6623\n",
      "Epoch 2370/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4705 - accuracy: 0.8492 - val_loss: 1.1495 - val_accuracy: 0.6526\n",
      "Epoch 2371/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5199 - accuracy: 0.8281 - val_loss: 1.1784 - val_accuracy: 0.6461\n",
      "Epoch 2372/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4884 - accuracy: 0.8320 - val_loss: 1.2173 - val_accuracy: 0.6396\n",
      "Epoch 2373/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5829 - accuracy: 0.7979 - val_loss: 1.2393 - val_accuracy: 0.6201\n",
      "Epoch 2374/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5386 - accuracy: 0.8282 - val_loss: 1.2638 - val_accuracy: 0.6006\n",
      "Epoch 2375/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5475 - accuracy: 0.8281 - val_loss: 1.3045 - val_accuracy: 0.6136\n",
      "Epoch 2376/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5001 - accuracy: 0.8320 - val_loss: 1.3290 - val_accuracy: 0.6104\n",
      "Epoch 2377/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4898 - accuracy: 0.8296 - val_loss: 1.3401 - val_accuracy: 0.6104\n",
      "Epoch 2378/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4971 - accuracy: 0.8418 - val_loss: 1.3288 - val_accuracy: 0.6006\n",
      "Epoch 2379/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4751 - accuracy: 0.8438 - val_loss: 1.3132 - val_accuracy: 0.6039\n",
      "Epoch 2380/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4812 - accuracy: 0.8310 - val_loss: 1.2898 - val_accuracy: 0.6299\n",
      "Epoch 2381/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5520 - accuracy: 0.8242 - val_loss: 1.2649 - val_accuracy: 0.6429\n",
      "Epoch 2382/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5063 - accuracy: 0.8240 - val_loss: 1.2274 - val_accuracy: 0.6623\n",
      "Epoch 2383/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6116 - accuracy: 0.8073 - val_loss: 1.1870 - val_accuracy: 0.6721\n",
      "Epoch 2384/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5083 - accuracy: 0.8128 - val_loss: 1.1559 - val_accuracy: 0.6721\n",
      "Epoch 2385/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5233 - accuracy: 0.8240 - val_loss: 1.1138 - val_accuracy: 0.6721\n",
      "Epoch 2386/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5104 - accuracy: 0.8428 - val_loss: 1.0952 - val_accuracy: 0.6818\n",
      "Epoch 2387/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5072 - accuracy: 0.8338 - val_loss: 1.0948 - val_accuracy: 0.6786\n",
      "Epoch 2388/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5341 - accuracy: 0.8164 - val_loss: 1.1116 - val_accuracy: 0.6688\n",
      "Epoch 2389/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4509 - accuracy: 0.8450 - val_loss: 1.1372 - val_accuracy: 0.6429\n",
      "Epoch 2390/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5338 - accuracy: 0.8184 - val_loss: 1.1710 - val_accuracy: 0.6396\n",
      "Epoch 2391/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4887 - accuracy: 0.8436 - val_loss: 1.2101 - val_accuracy: 0.6299\n",
      "Epoch 2392/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5486 - accuracy: 0.8135 - val_loss: 1.2174 - val_accuracy: 0.6234\n",
      "Epoch 2393/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5025 - accuracy: 0.8350 - val_loss: 1.2094 - val_accuracy: 0.6266\n",
      "Epoch 2394/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4916 - accuracy: 0.8301 - val_loss: 1.1871 - val_accuracy: 0.6364\n",
      "Epoch 2395/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6052 - accuracy: 0.7975 - val_loss: 1.1806 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2396/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5311 - accuracy: 0.8296 - val_loss: 1.1715 - val_accuracy: 0.6591\n",
      "Epoch 2397/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5233 - accuracy: 0.8324 - val_loss: 1.1927 - val_accuracy: 0.6396\n",
      "Epoch 2398/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5448 - accuracy: 0.8240 - val_loss: 1.2173 - val_accuracy: 0.6429\n",
      "Epoch 2399/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4967 - accuracy: 0.8252 - val_loss: 1.2370 - val_accuracy: 0.6299\n",
      "Epoch 2400/4000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5050 - accuracy: 0.8438 - val_loss: 1.2319 - val_accuracy: 0.6234\n",
      "Epoch 2401/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5676 - accuracy: 0.8164 - val_loss: 1.2334 - val_accuracy: 0.6299\n",
      "Epoch 2402/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5197 - accuracy: 0.8282 - val_loss: 1.2380 - val_accuracy: 0.6299\n",
      "Epoch 2403/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4910 - accuracy: 0.8359 - val_loss: 1.2270 - val_accuracy: 0.6396\n",
      "Epoch 2404/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4821 - accuracy: 0.8324 - val_loss: 1.2175 - val_accuracy: 0.6429\n",
      "Epoch 2405/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5221 - accuracy: 0.8350 - val_loss: 1.2054 - val_accuracy: 0.6429\n",
      "Epoch 2406/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4651 - accuracy: 0.8398 - val_loss: 1.1985 - val_accuracy: 0.6461\n",
      "Epoch 2407/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5633 - accuracy: 0.8145 - val_loss: 1.1882 - val_accuracy: 0.6331\n",
      "Epoch 2408/4000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4792 - accuracy: 0.8268 - val_loss: 1.1761 - val_accuracy: 0.6364\n",
      "Epoch 2409/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4957 - accuracy: 0.8380 - val_loss: 1.1752 - val_accuracy: 0.6461\n",
      "Epoch 2410/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5051 - accuracy: 0.8268 - val_loss: 1.1779 - val_accuracy: 0.6396\n",
      "Epoch 2411/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4857 - accuracy: 0.8252 - val_loss: 1.1845 - val_accuracy: 0.6396\n",
      "Epoch 2412/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4914 - accuracy: 0.8282 - val_loss: 1.1921 - val_accuracy: 0.6429\n",
      "Epoch 2413/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4944 - accuracy: 0.8271 - val_loss: 1.2096 - val_accuracy: 0.6461\n",
      "Epoch 2414/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5374 - accuracy: 0.8184 - val_loss: 1.2063 - val_accuracy: 0.6591\n",
      "Epoch 2415/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4565 - accuracy: 0.8478 - val_loss: 1.2076 - val_accuracy: 0.6494\n",
      "Epoch 2416/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5217 - accuracy: 0.8422 - val_loss: 1.2135 - val_accuracy: 0.6526\n",
      "Epoch 2417/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5172 - accuracy: 0.8394 - val_loss: 1.2293 - val_accuracy: 0.6429\n",
      "Epoch 2418/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4970 - accuracy: 0.8301 - val_loss: 1.2430 - val_accuracy: 0.6494\n",
      "Epoch 2419/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4214 - accuracy: 0.8575 - val_loss: 1.2516 - val_accuracy: 0.6494\n",
      "Epoch 2420/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5589 - accuracy: 0.8101 - val_loss: 1.2564 - val_accuracy: 0.6396\n",
      "Epoch 2421/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5428 - accuracy: 0.8156 - val_loss: 1.2519 - val_accuracy: 0.6396\n",
      "Epoch 2422/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4691 - accuracy: 0.8545 - val_loss: 1.2731 - val_accuracy: 0.6136\n",
      "Epoch 2423/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4869 - accuracy: 0.8379 - val_loss: 1.2767 - val_accuracy: 0.6104\n",
      "Epoch 2424/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4755 - accuracy: 0.8467 - val_loss: 1.2787 - val_accuracy: 0.6169\n",
      "Epoch 2425/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5126 - accuracy: 0.8324 - val_loss: 1.2905 - val_accuracy: 0.6039\n",
      "Epoch 2426/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5258 - accuracy: 0.8320 - val_loss: 1.2778 - val_accuracy: 0.6234\n",
      "Epoch 2427/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5755 - accuracy: 0.8184 - val_loss: 1.2562 - val_accuracy: 0.6201\n",
      "Epoch 2428/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5274 - accuracy: 0.8320 - val_loss: 1.2344 - val_accuracy: 0.6234\n",
      "Epoch 2429/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4281 - accuracy: 0.8561 - val_loss: 1.2156 - val_accuracy: 0.6429\n",
      "Epoch 2430/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5313 - accuracy: 0.8170 - val_loss: 1.2075 - val_accuracy: 0.6623\n",
      "Epoch 2431/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4562 - accuracy: 0.8408 - val_loss: 1.2043 - val_accuracy: 0.6558\n",
      "Epoch 2432/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4661 - accuracy: 0.8340 - val_loss: 1.2013 - val_accuracy: 0.6623\n",
      "Epoch 2433/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5005 - accuracy: 0.8428 - val_loss: 1.2094 - val_accuracy: 0.6558\n",
      "Epoch 2434/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5171 - accuracy: 0.8450 - val_loss: 1.2329 - val_accuracy: 0.6364\n",
      "Epoch 2435/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4678 - accuracy: 0.8254 - val_loss: 1.2523 - val_accuracy: 0.6494\n",
      "Epoch 2436/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5159 - accuracy: 0.8156 - val_loss: 1.2646 - val_accuracy: 0.6461\n",
      "Epoch 2437/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4901 - accuracy: 0.8492 - val_loss: 1.2640 - val_accuracy: 0.6494\n",
      "Epoch 2438/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5243 - accuracy: 0.8311 - val_loss: 1.2582 - val_accuracy: 0.6558\n",
      "Epoch 2439/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5281 - accuracy: 0.8242 - val_loss: 1.2461 - val_accuracy: 0.6558\n",
      "Epoch 2440/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4471 - accuracy: 0.8673 - val_loss: 1.2506 - val_accuracy: 0.6558\n",
      "Epoch 2441/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5212 - accuracy: 0.8291 - val_loss: 1.2597 - val_accuracy: 0.6591\n",
      "Epoch 2442/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5046 - accuracy: 0.8436 - val_loss: 1.2794 - val_accuracy: 0.6526\n",
      "Epoch 2443/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4892 - accuracy: 0.8408 - val_loss: 1.2735 - val_accuracy: 0.6656\n",
      "Epoch 2444/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5105 - accuracy: 0.8338 - val_loss: 1.2690 - val_accuracy: 0.6656\n",
      "Epoch 2445/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5298 - accuracy: 0.8408 - val_loss: 1.2543 - val_accuracy: 0.6721\n",
      "Epoch 2446/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5018 - accuracy: 0.8281 - val_loss: 1.2380 - val_accuracy: 0.6721\n",
      "Epoch 2447/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4956 - accuracy: 0.8447 - val_loss: 1.2337 - val_accuracy: 0.6688\n",
      "Epoch 2448/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5146 - accuracy: 0.8380 - val_loss: 1.2318 - val_accuracy: 0.6623\n",
      "Epoch 2449/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5282 - accuracy: 0.8226 - val_loss: 1.2313 - val_accuracy: 0.6623\n",
      "Epoch 2450/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5452 - accuracy: 0.8338 - val_loss: 1.2286 - val_accuracy: 0.6623\n",
      "Epoch 2451/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4778 - accuracy: 0.8366 - val_loss: 1.2278 - val_accuracy: 0.6461\n",
      "Epoch 2452/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5337 - accuracy: 0.8380 - val_loss: 1.2286 - val_accuracy: 0.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2453/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4962 - accuracy: 0.8380 - val_loss: 1.2363 - val_accuracy: 0.6396\n",
      "Epoch 2454/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4797 - accuracy: 0.8467 - val_loss: 1.2368 - val_accuracy: 0.6396\n",
      "Epoch 2455/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5115 - accuracy: 0.8311 - val_loss: 1.2243 - val_accuracy: 0.6429\n",
      "Epoch 2456/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5463 - accuracy: 0.8184 - val_loss: 1.2128 - val_accuracy: 0.6396\n",
      "Epoch 2457/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4623 - accuracy: 0.8477 - val_loss: 1.1953 - val_accuracy: 0.6364\n",
      "Epoch 2458/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4458 - accuracy: 0.8575 - val_loss: 1.1829 - val_accuracy: 0.6461\n",
      "Epoch 2459/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4904 - accuracy: 0.8340 - val_loss: 1.1830 - val_accuracy: 0.6396\n",
      "Epoch 2460/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4999 - accuracy: 0.8394 - val_loss: 1.1816 - val_accuracy: 0.6396\n",
      "Epoch 2461/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4943 - accuracy: 0.8324 - val_loss: 1.1772 - val_accuracy: 0.6461\n",
      "Epoch 2462/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5044 - accuracy: 0.8369 - val_loss: 1.1767 - val_accuracy: 0.6494\n",
      "Epoch 2463/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5397 - accuracy: 0.8212 - val_loss: 1.1842 - val_accuracy: 0.6591\n",
      "Epoch 2464/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4916 - accuracy: 0.8408 - val_loss: 1.1990 - val_accuracy: 0.6623\n",
      "Epoch 2465/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4668 - accuracy: 0.8589 - val_loss: 1.2105 - val_accuracy: 0.6461\n",
      "Epoch 2466/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4906 - accuracy: 0.8296 - val_loss: 1.2184 - val_accuracy: 0.6461\n",
      "Epoch 2467/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5377 - accuracy: 0.8301 - val_loss: 1.2266 - val_accuracy: 0.6494\n",
      "Epoch 2468/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4585 - accuracy: 0.8561 - val_loss: 1.2560 - val_accuracy: 0.6331\n",
      "Epoch 2469/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4956 - accuracy: 0.8340 - val_loss: 1.3103 - val_accuracy: 0.6234\n",
      "Epoch 2470/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4524 - accuracy: 0.8464 - val_loss: 1.3787 - val_accuracy: 0.6169\n",
      "Epoch 2471/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4768 - accuracy: 0.8408 - val_loss: 1.4372 - val_accuracy: 0.5942\n",
      "Epoch 2472/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4995 - accuracy: 0.8486 - val_loss: 1.4666 - val_accuracy: 0.5779\n",
      "Epoch 2473/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5151 - accuracy: 0.8226 - val_loss: 1.4780 - val_accuracy: 0.5682\n",
      "Epoch 2474/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4615 - accuracy: 0.8324 - val_loss: 1.4507 - val_accuracy: 0.5747\n",
      "Epoch 2475/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5125 - accuracy: 0.8193 - val_loss: 1.4143 - val_accuracy: 0.5877\n",
      "Epoch 2476/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4908 - accuracy: 0.8380 - val_loss: 1.3757 - val_accuracy: 0.6039\n",
      "Epoch 2477/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4490 - accuracy: 0.8603 - val_loss: 1.3399 - val_accuracy: 0.6299\n",
      "Epoch 2478/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4601 - accuracy: 0.8478 - val_loss: 1.3089 - val_accuracy: 0.6429\n",
      "Epoch 2479/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4635 - accuracy: 0.8547 - val_loss: 1.2819 - val_accuracy: 0.6558\n",
      "Epoch 2480/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4842 - accuracy: 0.8330 - val_loss: 1.2581 - val_accuracy: 0.6396\n",
      "Epoch 2481/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4873 - accuracy: 0.8447 - val_loss: 1.2438 - val_accuracy: 0.6494\n",
      "Epoch 2482/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4734 - accuracy: 0.8457 - val_loss: 1.2275 - val_accuracy: 0.6591\n",
      "Epoch 2483/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4497 - accuracy: 0.8408 - val_loss: 1.2228 - val_accuracy: 0.6591\n",
      "Epoch 2484/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4603 - accuracy: 0.8422 - val_loss: 1.2325 - val_accuracy: 0.6558\n",
      "Epoch 2485/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5265 - accuracy: 0.8212 - val_loss: 1.2555 - val_accuracy: 0.6623\n",
      "Epoch 2486/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4740 - accuracy: 0.8301 - val_loss: 1.2808 - val_accuracy: 0.6494\n",
      "Epoch 2487/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4742 - accuracy: 0.8496 - val_loss: 1.3002 - val_accuracy: 0.6331\n",
      "Epoch 2488/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4797 - accuracy: 0.8330 - val_loss: 1.3104 - val_accuracy: 0.6169\n",
      "Epoch 2489/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4832 - accuracy: 0.8478 - val_loss: 1.3027 - val_accuracy: 0.6136\n",
      "Epoch 2490/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5308 - accuracy: 0.8408 - val_loss: 1.2725 - val_accuracy: 0.6299\n",
      "Epoch 2491/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4916 - accuracy: 0.8436 - val_loss: 1.2437 - val_accuracy: 0.6461\n",
      "Epoch 2492/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4911 - accuracy: 0.8252 - val_loss: 1.2210 - val_accuracy: 0.6494\n",
      "Epoch 2493/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4800 - accuracy: 0.8350 - val_loss: 1.2053 - val_accuracy: 0.6396\n",
      "Epoch 2494/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4947 - accuracy: 0.8359 - val_loss: 1.1958 - val_accuracy: 0.6461\n",
      "Epoch 2495/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5141 - accuracy: 0.8193 - val_loss: 1.1940 - val_accuracy: 0.6526\n",
      "Epoch 2496/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4848 - accuracy: 0.8240 - val_loss: 1.2098 - val_accuracy: 0.6494\n",
      "Epoch 2497/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4782 - accuracy: 0.8436 - val_loss: 1.2408 - val_accuracy: 0.6461\n",
      "Epoch 2498/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4381 - accuracy: 0.8589 - val_loss: 1.2880 - val_accuracy: 0.6331\n",
      "Epoch 2499/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5364 - accuracy: 0.8262 - val_loss: 1.3212 - val_accuracy: 0.6266\n",
      "Epoch 2500/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4772 - accuracy: 0.8547 - val_loss: 1.3315 - val_accuracy: 0.6331\n",
      "Epoch 2501/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4979 - accuracy: 0.8359 - val_loss: 1.3203 - val_accuracy: 0.6364\n",
      "Epoch 2502/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4779 - accuracy: 0.8310 - val_loss: 1.3058 - val_accuracy: 0.6396\n",
      "Epoch 2503/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4516 - accuracy: 0.8603 - val_loss: 1.2706 - val_accuracy: 0.6331\n",
      "Epoch 2504/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4838 - accuracy: 0.8350 - val_loss: 1.2330 - val_accuracy: 0.6429\n",
      "Epoch 2505/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4929 - accuracy: 0.8436 - val_loss: 1.2110 - val_accuracy: 0.6526\n",
      "Epoch 2506/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4384 - accuracy: 0.8575 - val_loss: 1.2129 - val_accuracy: 0.6526\n",
      "Epoch 2507/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4436 - accuracy: 0.8457 - val_loss: 1.2099 - val_accuracy: 0.6591\n",
      "Epoch 2508/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4832 - accuracy: 0.8282 - val_loss: 1.2160 - val_accuracy: 0.6623\n",
      "Epoch 2509/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4506 - accuracy: 0.8506 - val_loss: 1.2261 - val_accuracy: 0.6494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2510/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5327 - accuracy: 0.8128 - val_loss: 1.2322 - val_accuracy: 0.6396\n",
      "Epoch 2511/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4756 - accuracy: 0.8436 - val_loss: 1.2480 - val_accuracy: 0.6396\n",
      "Epoch 2512/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4665 - accuracy: 0.8525 - val_loss: 1.2743 - val_accuracy: 0.6266\n",
      "Epoch 2513/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5438 - accuracy: 0.8240 - val_loss: 1.3103 - val_accuracy: 0.6039\n",
      "Epoch 2514/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5096 - accuracy: 0.8436 - val_loss: 1.3330 - val_accuracy: 0.5779\n",
      "Epoch 2515/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5176 - accuracy: 0.8156 - val_loss: 1.3176 - val_accuracy: 0.5942\n",
      "Epoch 2516/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4791 - accuracy: 0.8447 - val_loss: 1.2685 - val_accuracy: 0.6136\n",
      "Epoch 2517/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5262 - accuracy: 0.8324 - val_loss: 1.2310 - val_accuracy: 0.6461\n",
      "Epoch 2518/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4481 - accuracy: 0.8520 - val_loss: 1.2081 - val_accuracy: 0.6591\n",
      "Epoch 2519/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4300 - accuracy: 0.8652 - val_loss: 1.2009 - val_accuracy: 0.6721\n",
      "Epoch 2520/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4768 - accuracy: 0.8436 - val_loss: 1.2079 - val_accuracy: 0.6753\n",
      "Epoch 2521/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5008 - accuracy: 0.8428 - val_loss: 1.2324 - val_accuracy: 0.6623\n",
      "Epoch 2522/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4398 - accuracy: 0.8525 - val_loss: 1.2585 - val_accuracy: 0.6494\n",
      "Epoch 2523/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4771 - accuracy: 0.8389 - val_loss: 1.2955 - val_accuracy: 0.6169\n",
      "Epoch 2524/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4462 - accuracy: 0.8555 - val_loss: 1.3292 - val_accuracy: 0.6071\n",
      "Epoch 2525/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4914 - accuracy: 0.8418 - val_loss: 1.3828 - val_accuracy: 0.6006\n",
      "Epoch 2526/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4592 - accuracy: 0.8643 - val_loss: 1.4224 - val_accuracy: 0.6006\n",
      "Epoch 2527/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4662 - accuracy: 0.8394 - val_loss: 1.4638 - val_accuracy: 0.5877\n",
      "Epoch 2528/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4780 - accuracy: 0.8352 - val_loss: 1.4715 - val_accuracy: 0.5974\n",
      "Epoch 2529/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5093 - accuracy: 0.8359 - val_loss: 1.4490 - val_accuracy: 0.5942\n",
      "Epoch 2530/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4793 - accuracy: 0.8438 - val_loss: 1.4214 - val_accuracy: 0.6039\n",
      "Epoch 2531/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4499 - accuracy: 0.8467 - val_loss: 1.3828 - val_accuracy: 0.6104\n",
      "Epoch 2532/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4985 - accuracy: 0.8447 - val_loss: 1.3240 - val_accuracy: 0.6201\n",
      "Epoch 2533/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4654 - accuracy: 0.8436 - val_loss: 1.2891 - val_accuracy: 0.6299\n",
      "Epoch 2534/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4468 - accuracy: 0.8525 - val_loss: 1.2797 - val_accuracy: 0.6461\n",
      "Epoch 2535/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4231 - accuracy: 0.8715 - val_loss: 1.2913 - val_accuracy: 0.6429\n",
      "Epoch 2536/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5060 - accuracy: 0.8516 - val_loss: 1.3261 - val_accuracy: 0.6136\n",
      "Epoch 2537/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4617 - accuracy: 0.8457 - val_loss: 1.3883 - val_accuracy: 0.5942\n",
      "Epoch 2538/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4799 - accuracy: 0.8408 - val_loss: 1.4463 - val_accuracy: 0.5779\n",
      "Epoch 2539/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4985 - accuracy: 0.8366 - val_loss: 1.4809 - val_accuracy: 0.5714\n",
      "Epoch 2540/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4365 - accuracy: 0.8672 - val_loss: 1.4637 - val_accuracy: 0.5812\n",
      "Epoch 2541/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4845 - accuracy: 0.8394 - val_loss: 1.4392 - val_accuracy: 0.5877\n",
      "Epoch 2542/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5385 - accuracy: 0.8320 - val_loss: 1.4016 - val_accuracy: 0.5942\n",
      "Epoch 2543/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4030 - accuracy: 0.8673 - val_loss: 1.3688 - val_accuracy: 0.6039\n",
      "Epoch 2544/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5031 - accuracy: 0.8338 - val_loss: 1.3287 - val_accuracy: 0.6039\n",
      "Epoch 2545/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4762 - accuracy: 0.8428 - val_loss: 1.3124 - val_accuracy: 0.6201\n",
      "Epoch 2546/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5078 - accuracy: 0.8352 - val_loss: 1.3075 - val_accuracy: 0.6234\n",
      "Epoch 2547/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4541 - accuracy: 0.8492 - val_loss: 1.3121 - val_accuracy: 0.6169\n",
      "Epoch 2548/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4613 - accuracy: 0.8594 - val_loss: 1.3006 - val_accuracy: 0.6169\n",
      "Epoch 2549/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4928 - accuracy: 0.8520 - val_loss: 1.3052 - val_accuracy: 0.5974\n",
      "Epoch 2550/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4985 - accuracy: 0.8366 - val_loss: 1.3146 - val_accuracy: 0.5942\n",
      "Epoch 2551/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4433 - accuracy: 0.8555 - val_loss: 1.3336 - val_accuracy: 0.5714\n",
      "Epoch 2552/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5122 - accuracy: 0.8296 - val_loss: 1.3635 - val_accuracy: 0.5714\n",
      "Epoch 2553/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5078 - accuracy: 0.8330 - val_loss: 1.4170 - val_accuracy: 0.5617\n",
      "Epoch 2554/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4779 - accuracy: 0.8464 - val_loss: 1.4857 - val_accuracy: 0.5584\n",
      "Epoch 2555/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4996 - accuracy: 0.8492 - val_loss: 1.5389 - val_accuracy: 0.5487\n",
      "Epoch 2556/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5036 - accuracy: 0.8422 - val_loss: 1.5939 - val_accuracy: 0.5584\n",
      "Epoch 2557/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5601 - accuracy: 0.8156 - val_loss: 1.6384 - val_accuracy: 0.5455\n",
      "Epoch 2558/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4788 - accuracy: 0.8464 - val_loss: 1.6323 - val_accuracy: 0.5487\n",
      "Epoch 2559/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4607 - accuracy: 0.8438 - val_loss: 1.5879 - val_accuracy: 0.5649\n",
      "Epoch 2560/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4759 - accuracy: 0.8496 - val_loss: 1.5198 - val_accuracy: 0.5519\n",
      "Epoch 2561/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4829 - accuracy: 0.8418 - val_loss: 1.4451 - val_accuracy: 0.5877\n",
      "Epoch 2562/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4666 - accuracy: 0.8478 - val_loss: 1.3999 - val_accuracy: 0.6071\n",
      "Epoch 2563/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4676 - accuracy: 0.8438 - val_loss: 1.3841 - val_accuracy: 0.6136\n",
      "Epoch 2564/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5107 - accuracy: 0.8340 - val_loss: 1.4025 - val_accuracy: 0.6169\n",
      "Epoch 2565/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4333 - accuracy: 0.8682 - val_loss: 1.4195 - val_accuracy: 0.6201\n",
      "Epoch 2566/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4513 - accuracy: 0.8547 - val_loss: 1.4449 - val_accuracy: 0.6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2567/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5136 - accuracy: 0.8492 - val_loss: 1.4693 - val_accuracy: 0.6006\n",
      "Epoch 2568/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5034 - accuracy: 0.8408 - val_loss: 1.4816 - val_accuracy: 0.6071\n",
      "Epoch 2569/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4898 - accuracy: 0.8478 - val_loss: 1.4620 - val_accuracy: 0.6039\n",
      "Epoch 2570/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5032 - accuracy: 0.8408 - val_loss: 1.4235 - val_accuracy: 0.6266\n",
      "Epoch 2571/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5141 - accuracy: 0.8254 - val_loss: 1.3780 - val_accuracy: 0.6364\n",
      "Epoch 2572/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4457 - accuracy: 0.8520 - val_loss: 1.3485 - val_accuracy: 0.6364\n",
      "Epoch 2573/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4963 - accuracy: 0.8240 - val_loss: 1.3261 - val_accuracy: 0.6494\n",
      "Epoch 2574/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4770 - accuracy: 0.8516 - val_loss: 1.3042 - val_accuracy: 0.6558\n",
      "Epoch 2575/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4511 - accuracy: 0.8535 - val_loss: 1.2875 - val_accuracy: 0.6591\n",
      "Epoch 2576/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4425 - accuracy: 0.8613 - val_loss: 1.2833 - val_accuracy: 0.6591\n",
      "Epoch 2577/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5148 - accuracy: 0.8394 - val_loss: 1.2939 - val_accuracy: 0.6429\n",
      "Epoch 2578/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4732 - accuracy: 0.8398 - val_loss: 1.3275 - val_accuracy: 0.6169\n",
      "Epoch 2579/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5133 - accuracy: 0.8398 - val_loss: 1.3507 - val_accuracy: 0.6234\n",
      "Epoch 2580/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4786 - accuracy: 0.8291 - val_loss: 1.3644 - val_accuracy: 0.6071\n",
      "Epoch 2581/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4787 - accuracy: 0.8478 - val_loss: 1.3870 - val_accuracy: 0.6006\n",
      "Epoch 2582/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4779 - accuracy: 0.8545 - val_loss: 1.4225 - val_accuracy: 0.5974\n",
      "Epoch 2583/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4944 - accuracy: 0.8394 - val_loss: 1.4722 - val_accuracy: 0.5942\n",
      "Epoch 2584/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4081 - accuracy: 0.8633 - val_loss: 1.5003 - val_accuracy: 0.6104\n",
      "Epoch 2585/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4564 - accuracy: 0.8555 - val_loss: 1.4962 - val_accuracy: 0.6104\n",
      "Epoch 2586/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4249 - accuracy: 0.8574 - val_loss: 1.4773 - val_accuracy: 0.6169\n",
      "Epoch 2587/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4930 - accuracy: 0.8350 - val_loss: 1.4487 - val_accuracy: 0.6136\n",
      "Epoch 2588/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4460 - accuracy: 0.8535 - val_loss: 1.4047 - val_accuracy: 0.6234\n",
      "Epoch 2589/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4583 - accuracy: 0.8525 - val_loss: 1.3885 - val_accuracy: 0.6234\n",
      "Epoch 2590/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4158 - accuracy: 0.8643 - val_loss: 1.3756 - val_accuracy: 0.6201\n",
      "Epoch 2591/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4594 - accuracy: 0.8506 - val_loss: 1.3507 - val_accuracy: 0.6266\n",
      "Epoch 2592/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4569 - accuracy: 0.8535 - val_loss: 1.3356 - val_accuracy: 0.6266\n",
      "Epoch 2593/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4670 - accuracy: 0.8428 - val_loss: 1.3236 - val_accuracy: 0.6299\n",
      "Epoch 2594/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4949 - accuracy: 0.8450 - val_loss: 1.3491 - val_accuracy: 0.6331\n",
      "Epoch 2595/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4611 - accuracy: 0.8478 - val_loss: 1.3812 - val_accuracy: 0.6201\n",
      "Epoch 2596/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4720 - accuracy: 0.8324 - val_loss: 1.3869 - val_accuracy: 0.6136\n",
      "Epoch 2597/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4913 - accuracy: 0.8398 - val_loss: 1.3687 - val_accuracy: 0.6104\n",
      "Epoch 2598/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4272 - accuracy: 0.8604 - val_loss: 1.3287 - val_accuracy: 0.6201\n",
      "Epoch 2599/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5124 - accuracy: 0.8352 - val_loss: 1.2831 - val_accuracy: 0.6526\n",
      "Epoch 2600/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4810 - accuracy: 0.8320 - val_loss: 1.2552 - val_accuracy: 0.6494\n",
      "Epoch 2601/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4574 - accuracy: 0.8422 - val_loss: 1.2348 - val_accuracy: 0.6623\n",
      "Epoch 2602/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4844 - accuracy: 0.8464 - val_loss: 1.2351 - val_accuracy: 0.6591\n",
      "Epoch 2603/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3979 - accuracy: 0.8617 - val_loss: 1.2551 - val_accuracy: 0.6429\n",
      "Epoch 2604/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4853 - accuracy: 0.8296 - val_loss: 1.2866 - val_accuracy: 0.6234\n",
      "Epoch 2605/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4333 - accuracy: 0.8594 - val_loss: 1.3216 - val_accuracy: 0.6201\n",
      "Epoch 2606/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5185 - accuracy: 0.8389 - val_loss: 1.3444 - val_accuracy: 0.6136\n",
      "Epoch 2607/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4820 - accuracy: 0.8398 - val_loss: 1.3505 - val_accuracy: 0.6169\n",
      "Epoch 2608/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4837 - accuracy: 0.8457 - val_loss: 1.3719 - val_accuracy: 0.6169\n",
      "Epoch 2609/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4206 - accuracy: 0.8721 - val_loss: 1.3685 - val_accuracy: 0.6169\n",
      "Epoch 2610/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4232 - accuracy: 0.8721 - val_loss: 1.3501 - val_accuracy: 0.6234\n",
      "Epoch 2611/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4576 - accuracy: 0.8492 - val_loss: 1.3044 - val_accuracy: 0.6364\n",
      "Epoch 2612/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4419 - accuracy: 0.8617 - val_loss: 1.2900 - val_accuracy: 0.6364\n",
      "Epoch 2613/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4688 - accuracy: 0.8394 - val_loss: 1.2720 - val_accuracy: 0.6331\n",
      "Epoch 2614/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4710 - accuracy: 0.8408 - val_loss: 1.2756 - val_accuracy: 0.6169\n",
      "Epoch 2615/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4791 - accuracy: 0.8575 - val_loss: 1.2797 - val_accuracy: 0.6234\n",
      "Epoch 2616/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4851 - accuracy: 0.8447 - val_loss: 1.2818 - val_accuracy: 0.6331\n",
      "Epoch 2617/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4834 - accuracy: 0.8380 - val_loss: 1.2846 - val_accuracy: 0.6299\n",
      "Epoch 2618/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4646 - accuracy: 0.8478 - val_loss: 1.2806 - val_accuracy: 0.6331\n",
      "Epoch 2619/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4138 - accuracy: 0.8701 - val_loss: 1.2715 - val_accuracy: 0.6461\n",
      "Epoch 2620/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4138 - accuracy: 0.8631 - val_loss: 1.2567 - val_accuracy: 0.6494\n",
      "Epoch 2621/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4665 - accuracy: 0.8486 - val_loss: 1.2502 - val_accuracy: 0.6526\n",
      "Epoch 2622/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4611 - accuracy: 0.8584 - val_loss: 1.2525 - val_accuracy: 0.6656\n",
      "Epoch 2623/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4161 - accuracy: 0.8589 - val_loss: 1.2616 - val_accuracy: 0.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2624/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4405 - accuracy: 0.8486 - val_loss: 1.2774 - val_accuracy: 0.6753\n",
      "Epoch 2625/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4086 - accuracy: 0.8770 - val_loss: 1.2923 - val_accuracy: 0.6688\n",
      "Epoch 2626/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4414 - accuracy: 0.8535 - val_loss: 1.3004 - val_accuracy: 0.6656\n",
      "Epoch 2627/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5258 - accuracy: 0.8291 - val_loss: 1.3138 - val_accuracy: 0.6623\n",
      "Epoch 2628/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4615 - accuracy: 0.8408 - val_loss: 1.3342 - val_accuracy: 0.6494\n",
      "Epoch 2629/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4396 - accuracy: 0.8545 - val_loss: 1.3577 - val_accuracy: 0.6201\n",
      "Epoch 2630/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4172 - accuracy: 0.8575 - val_loss: 1.3810 - val_accuracy: 0.6266\n",
      "Epoch 2631/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4459 - accuracy: 0.8575 - val_loss: 1.3693 - val_accuracy: 0.6429\n",
      "Epoch 2632/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4419 - accuracy: 0.8492 - val_loss: 1.3581 - val_accuracy: 0.6461\n",
      "Epoch 2633/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4444 - accuracy: 0.8603 - val_loss: 1.3537 - val_accuracy: 0.6494\n",
      "Epoch 2634/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4536 - accuracy: 0.8464 - val_loss: 1.3531 - val_accuracy: 0.6364\n",
      "Epoch 2635/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4867 - accuracy: 0.8394 - val_loss: 1.3427 - val_accuracy: 0.6429\n",
      "Epoch 2636/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4638 - accuracy: 0.8506 - val_loss: 1.3300 - val_accuracy: 0.6364\n",
      "Epoch 2637/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4497 - accuracy: 0.8673 - val_loss: 1.3175 - val_accuracy: 0.6461\n",
      "Epoch 2638/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4647 - accuracy: 0.8438 - val_loss: 1.3094 - val_accuracy: 0.6429\n",
      "Epoch 2639/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4705 - accuracy: 0.8450 - val_loss: 1.2981 - val_accuracy: 0.6461\n",
      "Epoch 2640/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5400 - accuracy: 0.8240 - val_loss: 1.2958 - val_accuracy: 0.6461\n",
      "Epoch 2641/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4361 - accuracy: 0.8659 - val_loss: 1.2851 - val_accuracy: 0.6364\n",
      "Epoch 2642/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4395 - accuracy: 0.8428 - val_loss: 1.2734 - val_accuracy: 0.6461\n",
      "Epoch 2643/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4748 - accuracy: 0.8350 - val_loss: 1.2675 - val_accuracy: 0.6526\n",
      "Epoch 2644/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5057 - accuracy: 0.8226 - val_loss: 1.2744 - val_accuracy: 0.6558\n",
      "Epoch 2645/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4558 - accuracy: 0.8438 - val_loss: 1.2783 - val_accuracy: 0.6526\n",
      "Epoch 2646/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4585 - accuracy: 0.8516 - val_loss: 1.2835 - val_accuracy: 0.6461\n",
      "Epoch 2647/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4465 - accuracy: 0.8574 - val_loss: 1.2837 - val_accuracy: 0.6526\n",
      "Epoch 2648/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5160 - accuracy: 0.8389 - val_loss: 1.2807 - val_accuracy: 0.6591\n",
      "Epoch 2649/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4924 - accuracy: 0.8296 - val_loss: 1.2848 - val_accuracy: 0.6558\n",
      "Epoch 2650/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4624 - accuracy: 0.8564 - val_loss: 1.2895 - val_accuracy: 0.6494\n",
      "Epoch 2651/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5112 - accuracy: 0.8310 - val_loss: 1.2974 - val_accuracy: 0.6429\n",
      "Epoch 2652/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4190 - accuracy: 0.8687 - val_loss: 1.3077 - val_accuracy: 0.6299\n",
      "Epoch 2653/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5386 - accuracy: 0.8242 - val_loss: 1.3154 - val_accuracy: 0.6234\n",
      "Epoch 2654/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4393 - accuracy: 0.8631 - val_loss: 1.3107 - val_accuracy: 0.6201\n",
      "Epoch 2655/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4236 - accuracy: 0.8701 - val_loss: 1.2984 - val_accuracy: 0.6234\n",
      "Epoch 2656/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4424 - accuracy: 0.8477 - val_loss: 1.2885 - val_accuracy: 0.6266\n",
      "Epoch 2657/4000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4240 - accuracy: 0.8617 - val_loss: 1.2984 - val_accuracy: 0.6299\n",
      "Epoch 2658/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4786 - accuracy: 0.8478 - val_loss: 1.3030 - val_accuracy: 0.6169\n",
      "Epoch 2659/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4109 - accuracy: 0.8740 - val_loss: 1.2980 - val_accuracy: 0.6201\n",
      "Epoch 2660/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4520 - accuracy: 0.8535 - val_loss: 1.3030 - val_accuracy: 0.6104\n",
      "Epoch 2661/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4940 - accuracy: 0.8281 - val_loss: 1.3090 - val_accuracy: 0.6071\n",
      "Epoch 2662/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4613 - accuracy: 0.8428 - val_loss: 1.3025 - val_accuracy: 0.6136\n",
      "Epoch 2663/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4733 - accuracy: 0.8408 - val_loss: 1.3136 - val_accuracy: 0.6104\n",
      "Epoch 2664/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4656 - accuracy: 0.8555 - val_loss: 1.3252 - val_accuracy: 0.6136\n",
      "Epoch 2665/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4351 - accuracy: 0.8594 - val_loss: 1.3223 - val_accuracy: 0.6169\n",
      "Epoch 2666/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4938 - accuracy: 0.8310 - val_loss: 1.3615 - val_accuracy: 0.6039\n",
      "Epoch 2667/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4439 - accuracy: 0.8701 - val_loss: 1.3586 - val_accuracy: 0.6136\n",
      "Epoch 2668/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5048 - accuracy: 0.8422 - val_loss: 1.3486 - val_accuracy: 0.6234\n",
      "Epoch 2669/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4685 - accuracy: 0.8408 - val_loss: 1.3513 - val_accuracy: 0.6169\n",
      "Epoch 2670/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4827 - accuracy: 0.8422 - val_loss: 1.3437 - val_accuracy: 0.6201\n",
      "Epoch 2671/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4180 - accuracy: 0.8545 - val_loss: 1.3519 - val_accuracy: 0.6169\n",
      "Epoch 2672/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4444 - accuracy: 0.8516 - val_loss: 1.3818 - val_accuracy: 0.6006\n",
      "Epoch 2673/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4561 - accuracy: 0.8418 - val_loss: 1.4271 - val_accuracy: 0.5812\n",
      "Epoch 2674/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5270 - accuracy: 0.8226 - val_loss: 1.4650 - val_accuracy: 0.5812\n",
      "Epoch 2675/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4754 - accuracy: 0.8408 - val_loss: 1.5007 - val_accuracy: 0.5779\n",
      "Epoch 2676/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4379 - accuracy: 0.8478 - val_loss: 1.4927 - val_accuracy: 0.5779\n",
      "Epoch 2677/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5035 - accuracy: 0.8389 - val_loss: 1.4451 - val_accuracy: 0.5812\n",
      "Epoch 2678/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5311 - accuracy: 0.8366 - val_loss: 1.3678 - val_accuracy: 0.5844\n",
      "Epoch 2679/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4405 - accuracy: 0.8575 - val_loss: 1.3217 - val_accuracy: 0.6136\n",
      "Epoch 2680/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4309 - accuracy: 0.8643 - val_loss: 1.2986 - val_accuracy: 0.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2681/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4631 - accuracy: 0.8574 - val_loss: 1.3020 - val_accuracy: 0.6364\n",
      "Epoch 2682/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4683 - accuracy: 0.8436 - val_loss: 1.2952 - val_accuracy: 0.6364\n",
      "Epoch 2683/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5166 - accuracy: 0.8213 - val_loss: 1.2943 - val_accuracy: 0.6396\n",
      "Epoch 2684/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4849 - accuracy: 0.8447 - val_loss: 1.3079 - val_accuracy: 0.6364\n",
      "Epoch 2685/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4972 - accuracy: 0.8436 - val_loss: 1.3271 - val_accuracy: 0.6331\n",
      "Epoch 2686/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4042 - accuracy: 0.8730 - val_loss: 1.3454 - val_accuracy: 0.6299\n",
      "Epoch 2687/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4021 - accuracy: 0.8547 - val_loss: 1.3426 - val_accuracy: 0.6266\n",
      "Epoch 2688/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4713 - accuracy: 0.8398 - val_loss: 1.3399 - val_accuracy: 0.6201\n",
      "Epoch 2689/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4608 - accuracy: 0.8520 - val_loss: 1.3303 - val_accuracy: 0.6201\n",
      "Epoch 2690/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4899 - accuracy: 0.8477 - val_loss: 1.3389 - val_accuracy: 0.6201\n",
      "Epoch 2691/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4372 - accuracy: 0.8555 - val_loss: 1.3661 - val_accuracy: 0.6071\n",
      "Epoch 2692/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4664 - accuracy: 0.8422 - val_loss: 1.4041 - val_accuracy: 0.5877\n",
      "Epoch 2693/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4501 - accuracy: 0.8534 - val_loss: 1.4104 - val_accuracy: 0.5942\n",
      "Epoch 2694/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4438 - accuracy: 0.8617 - val_loss: 1.4087 - val_accuracy: 0.5877\n",
      "Epoch 2695/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4610 - accuracy: 0.8301 - val_loss: 1.4030 - val_accuracy: 0.5877\n",
      "Epoch 2696/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4225 - accuracy: 0.8604 - val_loss: 1.4121 - val_accuracy: 0.5974\n",
      "Epoch 2697/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4374 - accuracy: 0.8492 - val_loss: 1.4193 - val_accuracy: 0.6039\n",
      "Epoch 2698/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4133 - accuracy: 0.8691 - val_loss: 1.4433 - val_accuracy: 0.6039\n",
      "Epoch 2699/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3561 - accuracy: 0.8771 - val_loss: 1.4677 - val_accuracy: 0.6006\n",
      "Epoch 2700/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3696 - accuracy: 0.8771 - val_loss: 1.4748 - val_accuracy: 0.6104\n",
      "Epoch 2701/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4517 - accuracy: 0.8604 - val_loss: 1.4409 - val_accuracy: 0.6104\n",
      "Epoch 2702/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4383 - accuracy: 0.8477 - val_loss: 1.4229 - val_accuracy: 0.6201\n",
      "Epoch 2703/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4444 - accuracy: 0.8447 - val_loss: 1.3891 - val_accuracy: 0.6266\n",
      "Epoch 2704/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4238 - accuracy: 0.8564 - val_loss: 1.3565 - val_accuracy: 0.6266\n",
      "Epoch 2705/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4870 - accuracy: 0.8436 - val_loss: 1.3529 - val_accuracy: 0.6234\n",
      "Epoch 2706/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5044 - accuracy: 0.8478 - val_loss: 1.3414 - val_accuracy: 0.6266\n",
      "Epoch 2707/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4833 - accuracy: 0.8547 - val_loss: 1.3361 - val_accuracy: 0.6331\n",
      "Epoch 2708/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4289 - accuracy: 0.8534 - val_loss: 1.3240 - val_accuracy: 0.6461\n",
      "Epoch 2709/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5129 - accuracy: 0.8268 - val_loss: 1.3037 - val_accuracy: 0.6558\n",
      "Epoch 2710/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4048 - accuracy: 0.8604 - val_loss: 1.3065 - val_accuracy: 0.6331\n",
      "Epoch 2711/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4053 - accuracy: 0.8594 - val_loss: 1.3146 - val_accuracy: 0.6266\n",
      "Epoch 2712/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3880 - accuracy: 0.8743 - val_loss: 1.3114 - val_accuracy: 0.6136\n",
      "Epoch 2713/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4423 - accuracy: 0.8516 - val_loss: 1.3173 - val_accuracy: 0.6039\n",
      "Epoch 2714/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4410 - accuracy: 0.8631 - val_loss: 1.3271 - val_accuracy: 0.6104\n",
      "Epoch 2715/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4581 - accuracy: 0.8645 - val_loss: 1.3375 - val_accuracy: 0.6234\n",
      "Epoch 2716/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4240 - accuracy: 0.8652 - val_loss: 1.3381 - val_accuracy: 0.6266\n",
      "Epoch 2717/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4389 - accuracy: 0.8496 - val_loss: 1.3561 - val_accuracy: 0.6266\n",
      "Epoch 2718/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4698 - accuracy: 0.8525 - val_loss: 1.3796 - val_accuracy: 0.6136\n",
      "Epoch 2719/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4105 - accuracy: 0.8617 - val_loss: 1.3887 - val_accuracy: 0.6071\n",
      "Epoch 2720/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4138 - accuracy: 0.8631 - val_loss: 1.3992 - val_accuracy: 0.6039\n",
      "Epoch 2721/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4635 - accuracy: 0.8574 - val_loss: 1.4069 - val_accuracy: 0.6039\n",
      "Epoch 2722/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4541 - accuracy: 0.8477 - val_loss: 1.4044 - val_accuracy: 0.6071\n",
      "Epoch 2723/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4852 - accuracy: 0.8447 - val_loss: 1.3978 - val_accuracy: 0.6071\n",
      "Epoch 2724/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4496 - accuracy: 0.8496 - val_loss: 1.3936 - val_accuracy: 0.6136\n",
      "Epoch 2725/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4537 - accuracy: 0.8478 - val_loss: 1.3935 - val_accuracy: 0.6071\n",
      "Epoch 2726/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4077 - accuracy: 0.8743 - val_loss: 1.3856 - val_accuracy: 0.6169\n",
      "Epoch 2727/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4631 - accuracy: 0.8428 - val_loss: 1.3805 - val_accuracy: 0.6201\n",
      "Epoch 2728/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4464 - accuracy: 0.8561 - val_loss: 1.3790 - val_accuracy: 0.6169\n",
      "Epoch 2729/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4458 - accuracy: 0.8467 - val_loss: 1.3819 - val_accuracy: 0.6169\n",
      "Epoch 2730/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4476 - accuracy: 0.8555 - val_loss: 1.3748 - val_accuracy: 0.6039\n",
      "Epoch 2731/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4989 - accuracy: 0.8464 - val_loss: 1.3369 - val_accuracy: 0.6039\n",
      "Epoch 2732/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4768 - accuracy: 0.8438 - val_loss: 1.3132 - val_accuracy: 0.6201\n",
      "Epoch 2733/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4619 - accuracy: 0.8478 - val_loss: 1.3074 - val_accuracy: 0.6266\n",
      "Epoch 2734/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4759 - accuracy: 0.8394 - val_loss: 1.3140 - val_accuracy: 0.6136\n",
      "Epoch 2735/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4640 - accuracy: 0.8525 - val_loss: 1.3410 - val_accuracy: 0.6266\n",
      "Epoch 2736/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4674 - accuracy: 0.8330 - val_loss: 1.3489 - val_accuracy: 0.6234\n",
      "Epoch 2737/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4438 - accuracy: 0.8631 - val_loss: 1.3832 - val_accuracy: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2738/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4682 - accuracy: 0.8398 - val_loss: 1.4121 - val_accuracy: 0.5942\n",
      "Epoch 2739/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4117 - accuracy: 0.8659 - val_loss: 1.4427 - val_accuracy: 0.5779\n",
      "Epoch 2740/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4766 - accuracy: 0.8516 - val_loss: 1.4931 - val_accuracy: 0.5714\n",
      "Epoch 2741/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4373 - accuracy: 0.8525 - val_loss: 1.5273 - val_accuracy: 0.5844\n",
      "Epoch 2742/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4449 - accuracy: 0.8506 - val_loss: 1.5313 - val_accuracy: 0.5844\n",
      "Epoch 2743/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4155 - accuracy: 0.8750 - val_loss: 1.5134 - val_accuracy: 0.5812\n",
      "Epoch 2744/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4541 - accuracy: 0.8310 - val_loss: 1.4885 - val_accuracy: 0.5747\n",
      "Epoch 2745/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4520 - accuracy: 0.8506 - val_loss: 1.4987 - val_accuracy: 0.6006\n",
      "Epoch 2746/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4618 - accuracy: 0.8520 - val_loss: 1.5274 - val_accuracy: 0.6136\n",
      "Epoch 2747/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4611 - accuracy: 0.8520 - val_loss: 1.5777 - val_accuracy: 0.6169\n",
      "Epoch 2748/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4911 - accuracy: 0.8492 - val_loss: 1.5442 - val_accuracy: 0.5974\n",
      "Epoch 2749/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4247 - accuracy: 0.8561 - val_loss: 1.4824 - val_accuracy: 0.6104\n",
      "Epoch 2750/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4236 - accuracy: 0.8652 - val_loss: 1.4270 - val_accuracy: 0.6136\n",
      "Epoch 2751/4000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4195 - accuracy: 0.8643 - val_loss: 1.3904 - val_accuracy: 0.6136\n",
      "Epoch 2752/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4594 - accuracy: 0.8561 - val_loss: 1.3568 - val_accuracy: 0.6266\n",
      "Epoch 2753/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4387 - accuracy: 0.8603 - val_loss: 1.3279 - val_accuracy: 0.6071\n",
      "Epoch 2754/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4569 - accuracy: 0.8492 - val_loss: 1.3131 - val_accuracy: 0.6266\n",
      "Epoch 2755/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4487 - accuracy: 0.8506 - val_loss: 1.2619 - val_accuracy: 0.6364\n",
      "Epoch 2756/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4196 - accuracy: 0.8547 - val_loss: 1.2272 - val_accuracy: 0.6656\n",
      "Epoch 2757/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4086 - accuracy: 0.8631 - val_loss: 1.2388 - val_accuracy: 0.6494\n",
      "Epoch 2758/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4391 - accuracy: 0.8408 - val_loss: 1.2745 - val_accuracy: 0.6396\n",
      "Epoch 2759/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4159 - accuracy: 0.8715 - val_loss: 1.3087 - val_accuracy: 0.6331\n",
      "Epoch 2760/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4233 - accuracy: 0.8603 - val_loss: 1.3340 - val_accuracy: 0.6331\n",
      "Epoch 2761/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4546 - accuracy: 0.8545 - val_loss: 1.3523 - val_accuracy: 0.6364\n",
      "Epoch 2762/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4123 - accuracy: 0.8603 - val_loss: 1.3523 - val_accuracy: 0.6299\n",
      "Epoch 2763/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4128 - accuracy: 0.8534 - val_loss: 1.3562 - val_accuracy: 0.6169\n",
      "Epoch 2764/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4232 - accuracy: 0.8584 - val_loss: 1.3496 - val_accuracy: 0.6104\n",
      "Epoch 2765/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4396 - accuracy: 0.8486 - val_loss: 1.3411 - val_accuracy: 0.6136\n",
      "Epoch 2766/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4650 - accuracy: 0.8547 - val_loss: 1.3257 - val_accuracy: 0.6266\n",
      "Epoch 2767/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4605 - accuracy: 0.8564 - val_loss: 1.3137 - val_accuracy: 0.6136\n",
      "Epoch 2768/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4485 - accuracy: 0.8477 - val_loss: 1.2931 - val_accuracy: 0.6201\n",
      "Epoch 2769/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4537 - accuracy: 0.8418 - val_loss: 1.2823 - val_accuracy: 0.6364\n",
      "Epoch 2770/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5020 - accuracy: 0.8311 - val_loss: 1.2950 - val_accuracy: 0.6299\n",
      "Epoch 2771/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4492 - accuracy: 0.8520 - val_loss: 1.2975 - val_accuracy: 0.6201\n",
      "Epoch 2772/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4569 - accuracy: 0.8492 - val_loss: 1.2992 - val_accuracy: 0.6299\n",
      "Epoch 2773/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4257 - accuracy: 0.8574 - val_loss: 1.3025 - val_accuracy: 0.6364\n",
      "Epoch 2774/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5048 - accuracy: 0.8350 - val_loss: 1.3084 - val_accuracy: 0.6169\n",
      "Epoch 2775/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4393 - accuracy: 0.8589 - val_loss: 1.3209 - val_accuracy: 0.6136\n",
      "Epoch 2776/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4644 - accuracy: 0.8525 - val_loss: 1.3339 - val_accuracy: 0.6104\n",
      "Epoch 2777/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4266 - accuracy: 0.8584 - val_loss: 1.3232 - val_accuracy: 0.6169\n",
      "Epoch 2778/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4158 - accuracy: 0.8408 - val_loss: 1.3089 - val_accuracy: 0.6299\n",
      "Epoch 2779/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4329 - accuracy: 0.8574 - val_loss: 1.2895 - val_accuracy: 0.6461\n",
      "Epoch 2780/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3722 - accuracy: 0.8701 - val_loss: 1.2869 - val_accuracy: 0.6494\n",
      "Epoch 2781/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4604 - accuracy: 0.8457 - val_loss: 1.2842 - val_accuracy: 0.6591\n",
      "Epoch 2782/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4573 - accuracy: 0.8464 - val_loss: 1.2886 - val_accuracy: 0.6461\n",
      "Epoch 2783/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4081 - accuracy: 0.8604 - val_loss: 1.2911 - val_accuracy: 0.6331\n",
      "Epoch 2784/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4444 - accuracy: 0.8535 - val_loss: 1.2961 - val_accuracy: 0.6331\n",
      "Epoch 2785/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4393 - accuracy: 0.8535 - val_loss: 1.3045 - val_accuracy: 0.6234\n",
      "Epoch 2786/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4202 - accuracy: 0.8584 - val_loss: 1.3018 - val_accuracy: 0.6396\n",
      "Epoch 2787/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4244 - accuracy: 0.8687 - val_loss: 1.3164 - val_accuracy: 0.6429\n",
      "Epoch 2788/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4789 - accuracy: 0.8369 - val_loss: 1.3481 - val_accuracy: 0.6266\n",
      "Epoch 2789/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4324 - accuracy: 0.8594 - val_loss: 1.4055 - val_accuracy: 0.6169\n",
      "Epoch 2790/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4442 - accuracy: 0.8672 - val_loss: 1.5056 - val_accuracy: 0.5747\n",
      "Epoch 2791/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4058 - accuracy: 0.8701 - val_loss: 1.5967 - val_accuracy: 0.5617\n",
      "Epoch 2792/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3942 - accuracy: 0.8771 - val_loss: 1.6847 - val_accuracy: 0.5487\n",
      "Epoch 2793/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4211 - accuracy: 0.8631 - val_loss: 1.7298 - val_accuracy: 0.5455\n",
      "Epoch 2794/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4228 - accuracy: 0.8687 - val_loss: 1.7478 - val_accuracy: 0.5455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2795/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3802 - accuracy: 0.8799 - val_loss: 1.7329 - val_accuracy: 0.5519\n",
      "Epoch 2796/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3708 - accuracy: 0.8757 - val_loss: 1.6743 - val_accuracy: 0.5584\n",
      "Epoch 2797/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4517 - accuracy: 0.8555 - val_loss: 1.6047 - val_accuracy: 0.5649\n",
      "Epoch 2798/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4244 - accuracy: 0.8652 - val_loss: 1.5397 - val_accuracy: 0.5747\n",
      "Epoch 2799/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4864 - accuracy: 0.8350 - val_loss: 1.5082 - val_accuracy: 0.5747\n",
      "Epoch 2800/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4495 - accuracy: 0.8457 - val_loss: 1.4762 - val_accuracy: 0.5942\n",
      "Epoch 2801/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4200 - accuracy: 0.8450 - val_loss: 1.4521 - val_accuracy: 0.5942\n",
      "Epoch 2802/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4123 - accuracy: 0.8760 - val_loss: 1.4331 - val_accuracy: 0.5942\n",
      "Epoch 2803/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4991 - accuracy: 0.8464 - val_loss: 1.4127 - val_accuracy: 0.5974\n",
      "Epoch 2804/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4058 - accuracy: 0.8652 - val_loss: 1.4030 - val_accuracy: 0.5877\n",
      "Epoch 2805/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4685 - accuracy: 0.8547 - val_loss: 1.4087 - val_accuracy: 0.5942\n",
      "Epoch 2806/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4196 - accuracy: 0.8525 - val_loss: 1.4098 - val_accuracy: 0.5942\n",
      "Epoch 2807/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3769 - accuracy: 0.8750 - val_loss: 1.4020 - val_accuracy: 0.5942\n",
      "Epoch 2808/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5258 - accuracy: 0.8408 - val_loss: 1.3888 - val_accuracy: 0.6071\n",
      "Epoch 2809/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4392 - accuracy: 0.8545 - val_loss: 1.3562 - val_accuracy: 0.6201\n",
      "Epoch 2810/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4326 - accuracy: 0.8574 - val_loss: 1.3243 - val_accuracy: 0.6266\n",
      "Epoch 2811/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4264 - accuracy: 0.8467 - val_loss: 1.2873 - val_accuracy: 0.6429\n",
      "Epoch 2812/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4325 - accuracy: 0.8631 - val_loss: 1.2594 - val_accuracy: 0.6429\n",
      "Epoch 2813/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4214 - accuracy: 0.8682 - val_loss: 1.2439 - val_accuracy: 0.6526\n",
      "Epoch 2814/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4263 - accuracy: 0.8547 - val_loss: 1.2348 - val_accuracy: 0.6591\n",
      "Epoch 2815/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4157 - accuracy: 0.8613 - val_loss: 1.2293 - val_accuracy: 0.6591\n",
      "Epoch 2816/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4707 - accuracy: 0.8422 - val_loss: 1.2286 - val_accuracy: 0.6591\n",
      "Epoch 2817/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4499 - accuracy: 0.8525 - val_loss: 1.2406 - val_accuracy: 0.6558\n",
      "Epoch 2818/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4180 - accuracy: 0.8757 - val_loss: 1.2490 - val_accuracy: 0.6461\n",
      "Epoch 2819/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4723 - accuracy: 0.8561 - val_loss: 1.2675 - val_accuracy: 0.6461\n",
      "Epoch 2820/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3818 - accuracy: 0.8623 - val_loss: 1.2867 - val_accuracy: 0.6299\n",
      "Epoch 2821/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4571 - accuracy: 0.8506 - val_loss: 1.2917 - val_accuracy: 0.6266\n",
      "Epoch 2822/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4471 - accuracy: 0.8438 - val_loss: 1.2957 - val_accuracy: 0.6234\n",
      "Epoch 2823/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4477 - accuracy: 0.8659 - val_loss: 1.2870 - val_accuracy: 0.6201\n",
      "Epoch 2824/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4184 - accuracy: 0.8701 - val_loss: 1.2980 - val_accuracy: 0.6169\n",
      "Epoch 2825/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4038 - accuracy: 0.8691 - val_loss: 1.3079 - val_accuracy: 0.6169\n",
      "Epoch 2826/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4265 - accuracy: 0.8604 - val_loss: 1.3008 - val_accuracy: 0.6266\n",
      "Epoch 2827/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3786 - accuracy: 0.8603 - val_loss: 1.2931 - val_accuracy: 0.6266\n",
      "Epoch 2828/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3971 - accuracy: 0.8711 - val_loss: 1.2795 - val_accuracy: 0.6201\n",
      "Epoch 2829/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4895 - accuracy: 0.8398 - val_loss: 1.2914 - val_accuracy: 0.6201\n",
      "Epoch 2830/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4145 - accuracy: 0.8691 - val_loss: 1.3133 - val_accuracy: 0.6201\n",
      "Epoch 2831/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4389 - accuracy: 0.8520 - val_loss: 1.3260 - val_accuracy: 0.6234\n",
      "Epoch 2832/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4545 - accuracy: 0.8564 - val_loss: 1.3415 - val_accuracy: 0.6104\n",
      "Epoch 2833/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4109 - accuracy: 0.8799 - val_loss: 1.3612 - val_accuracy: 0.6104\n",
      "Epoch 2834/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4049 - accuracy: 0.8603 - val_loss: 1.3879 - val_accuracy: 0.6104\n",
      "Epoch 2835/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4331 - accuracy: 0.8478 - val_loss: 1.4179 - val_accuracy: 0.5974\n",
      "Epoch 2836/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4658 - accuracy: 0.8547 - val_loss: 1.4219 - val_accuracy: 0.6071\n",
      "Epoch 2837/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4292 - accuracy: 0.8682 - val_loss: 1.4149 - val_accuracy: 0.6039\n",
      "Epoch 2838/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4484 - accuracy: 0.8534 - val_loss: 1.4174 - val_accuracy: 0.6071\n",
      "Epoch 2839/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3932 - accuracy: 0.8564 - val_loss: 1.4245 - val_accuracy: 0.6169\n",
      "Epoch 2840/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4612 - accuracy: 0.8575 - val_loss: 1.4249 - val_accuracy: 0.6104\n",
      "Epoch 2841/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4266 - accuracy: 0.8534 - val_loss: 1.4196 - val_accuracy: 0.6136\n",
      "Epoch 2842/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4049 - accuracy: 0.8574 - val_loss: 1.4160 - val_accuracy: 0.6104\n",
      "Epoch 2843/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4229 - accuracy: 0.8672 - val_loss: 1.3779 - val_accuracy: 0.6169\n",
      "Epoch 2844/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4637 - accuracy: 0.8520 - val_loss: 1.3278 - val_accuracy: 0.6299\n",
      "Epoch 2845/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4610 - accuracy: 0.8506 - val_loss: 1.2829 - val_accuracy: 0.6429\n",
      "Epoch 2846/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4204 - accuracy: 0.8534 - val_loss: 1.2668 - val_accuracy: 0.6461\n",
      "Epoch 2847/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4339 - accuracy: 0.8564 - val_loss: 1.2506 - val_accuracy: 0.6461\n",
      "Epoch 2848/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4199 - accuracy: 0.8673 - val_loss: 1.2553 - val_accuracy: 0.6429\n",
      "Epoch 2849/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3874 - accuracy: 0.8715 - val_loss: 1.2900 - val_accuracy: 0.6201\n",
      "Epoch 2850/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3983 - accuracy: 0.8721 - val_loss: 1.3386 - val_accuracy: 0.6006\n",
      "Epoch 2851/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4543 - accuracy: 0.8545 - val_loss: 1.3732 - val_accuracy: 0.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2852/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4229 - accuracy: 0.8555 - val_loss: 1.3804 - val_accuracy: 0.5877\n",
      "Epoch 2853/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4040 - accuracy: 0.8740 - val_loss: 1.3517 - val_accuracy: 0.5909\n",
      "Epoch 2854/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4845 - accuracy: 0.8506 - val_loss: 1.3132 - val_accuracy: 0.6234\n",
      "Epoch 2855/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4399 - accuracy: 0.8506 - val_loss: 1.2710 - val_accuracy: 0.6364\n",
      "Epoch 2856/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3589 - accuracy: 0.8771 - val_loss: 1.2379 - val_accuracy: 0.6461\n",
      "Epoch 2857/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4189 - accuracy: 0.8575 - val_loss: 1.2350 - val_accuracy: 0.6461\n",
      "Epoch 2858/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4014 - accuracy: 0.8589 - val_loss: 1.2566 - val_accuracy: 0.6429\n",
      "Epoch 2859/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4456 - accuracy: 0.8594 - val_loss: 1.2995 - val_accuracy: 0.6331\n",
      "Epoch 2860/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4360 - accuracy: 0.8603 - val_loss: 1.3511 - val_accuracy: 0.6104\n",
      "Epoch 2861/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4439 - accuracy: 0.8545 - val_loss: 1.4010 - val_accuracy: 0.6039\n",
      "Epoch 2862/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4833 - accuracy: 0.8496 - val_loss: 1.4557 - val_accuracy: 0.5942\n",
      "Epoch 2863/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4084 - accuracy: 0.8589 - val_loss: 1.5060 - val_accuracy: 0.5877\n",
      "Epoch 2864/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4234 - accuracy: 0.8623 - val_loss: 1.5711 - val_accuracy: 0.5812\n",
      "Epoch 2865/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4207 - accuracy: 0.8662 - val_loss: 1.6725 - val_accuracy: 0.5552\n",
      "Epoch 2866/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4076 - accuracy: 0.8575 - val_loss: 1.7522 - val_accuracy: 0.5390\n",
      "Epoch 2867/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4178 - accuracy: 0.8575 - val_loss: 1.7925 - val_accuracy: 0.5325\n",
      "Epoch 2868/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4500 - accuracy: 0.8555 - val_loss: 1.7820 - val_accuracy: 0.5195\n",
      "Epoch 2869/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4683 - accuracy: 0.8506 - val_loss: 1.7472 - val_accuracy: 0.5390\n",
      "Epoch 2870/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3844 - accuracy: 0.8799 - val_loss: 1.6875 - val_accuracy: 0.5292\n",
      "Epoch 2871/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4297 - accuracy: 0.8673 - val_loss: 1.6174 - val_accuracy: 0.5422\n",
      "Epoch 2872/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4437 - accuracy: 0.8545 - val_loss: 1.5677 - val_accuracy: 0.5617\n",
      "Epoch 2873/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3900 - accuracy: 0.8779 - val_loss: 1.5191 - val_accuracy: 0.5714\n",
      "Epoch 2874/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3946 - accuracy: 0.8721 - val_loss: 1.4777 - val_accuracy: 0.5747\n",
      "Epoch 2875/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4019 - accuracy: 0.8740 - val_loss: 1.4692 - val_accuracy: 0.5747\n",
      "Epoch 2876/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4549 - accuracy: 0.8574 - val_loss: 1.4986 - val_accuracy: 0.5844\n",
      "Epoch 2877/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4559 - accuracy: 0.8594 - val_loss: 1.5370 - val_accuracy: 0.5909\n",
      "Epoch 2878/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4415 - accuracy: 0.8645 - val_loss: 1.5520 - val_accuracy: 0.5844\n",
      "Epoch 2879/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4260 - accuracy: 0.8545 - val_loss: 1.5291 - val_accuracy: 0.5844\n",
      "Epoch 2880/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3996 - accuracy: 0.8682 - val_loss: 1.4947 - val_accuracy: 0.5877\n",
      "Epoch 2881/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4279 - accuracy: 0.8617 - val_loss: 1.4502 - val_accuracy: 0.5909\n",
      "Epoch 2882/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4161 - accuracy: 0.8673 - val_loss: 1.3887 - val_accuracy: 0.6136\n",
      "Epoch 2883/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4109 - accuracy: 0.8701 - val_loss: 1.3311 - val_accuracy: 0.6299\n",
      "Epoch 2884/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4460 - accuracy: 0.8547 - val_loss: 1.2859 - val_accuracy: 0.6364\n",
      "Epoch 2885/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3576 - accuracy: 0.8809 - val_loss: 1.2665 - val_accuracy: 0.6299\n",
      "Epoch 2886/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4449 - accuracy: 0.8478 - val_loss: 1.2632 - val_accuracy: 0.6494\n",
      "Epoch 2887/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3974 - accuracy: 0.8701 - val_loss: 1.2657 - val_accuracy: 0.6526\n",
      "Epoch 2888/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3904 - accuracy: 0.8715 - val_loss: 1.2658 - val_accuracy: 0.6558\n",
      "Epoch 2889/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4062 - accuracy: 0.8701 - val_loss: 1.2790 - val_accuracy: 0.6721\n",
      "Epoch 2890/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4430 - accuracy: 0.8547 - val_loss: 1.3059 - val_accuracy: 0.6526\n",
      "Epoch 2891/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3865 - accuracy: 0.8659 - val_loss: 1.3174 - val_accuracy: 0.6429\n",
      "Epoch 2892/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3775 - accuracy: 0.8809 - val_loss: 1.3200 - val_accuracy: 0.6494\n",
      "Epoch 2893/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4129 - accuracy: 0.8687 - val_loss: 1.3140 - val_accuracy: 0.6396\n",
      "Epoch 2894/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3767 - accuracy: 0.8750 - val_loss: 1.2997 - val_accuracy: 0.6299\n",
      "Epoch 2895/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3911 - accuracy: 0.8662 - val_loss: 1.2907 - val_accuracy: 0.6494\n",
      "Epoch 2896/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4118 - accuracy: 0.8662 - val_loss: 1.2794 - val_accuracy: 0.6461\n",
      "Epoch 2897/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3866 - accuracy: 0.8652 - val_loss: 1.2755 - val_accuracy: 0.6396\n",
      "Epoch 2898/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4226 - accuracy: 0.8617 - val_loss: 1.2811 - val_accuracy: 0.6429\n",
      "Epoch 2899/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4293 - accuracy: 0.8478 - val_loss: 1.2796 - val_accuracy: 0.6461\n",
      "Epoch 2900/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4270 - accuracy: 0.8561 - val_loss: 1.2823 - val_accuracy: 0.6461\n",
      "Epoch 2901/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4152 - accuracy: 0.8771 - val_loss: 1.2760 - val_accuracy: 0.6494\n",
      "Epoch 2902/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4205 - accuracy: 0.8687 - val_loss: 1.2884 - val_accuracy: 0.6526\n",
      "Epoch 2903/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4077 - accuracy: 0.8623 - val_loss: 1.3027 - val_accuracy: 0.6494\n",
      "Epoch 2904/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4026 - accuracy: 0.8672 - val_loss: 1.3086 - val_accuracy: 0.6429\n",
      "Epoch 2905/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3860 - accuracy: 0.8730 - val_loss: 1.3096 - val_accuracy: 0.6331\n",
      "Epoch 2906/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3741 - accuracy: 0.8682 - val_loss: 1.3166 - val_accuracy: 0.6299\n",
      "Epoch 2907/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4206 - accuracy: 0.8711 - val_loss: 1.3387 - val_accuracy: 0.6396\n",
      "Epoch 2908/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3886 - accuracy: 0.8603 - val_loss: 1.3597 - val_accuracy: 0.6234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2909/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4280 - accuracy: 0.8672 - val_loss: 1.3869 - val_accuracy: 0.6234\n",
      "Epoch 2910/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3724 - accuracy: 0.8953 - val_loss: 1.4244 - val_accuracy: 0.6136\n",
      "Epoch 2911/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3650 - accuracy: 0.8848 - val_loss: 1.4462 - val_accuracy: 0.6071\n",
      "Epoch 2912/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3955 - accuracy: 0.8687 - val_loss: 1.4702 - val_accuracy: 0.6006\n",
      "Epoch 2913/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4176 - accuracy: 0.8547 - val_loss: 1.4939 - val_accuracy: 0.5974\n",
      "Epoch 2914/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4232 - accuracy: 0.8760 - val_loss: 1.5037 - val_accuracy: 0.5974\n",
      "Epoch 2915/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3857 - accuracy: 0.8743 - val_loss: 1.5005 - val_accuracy: 0.5844\n",
      "Epoch 2916/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4259 - accuracy: 0.8564 - val_loss: 1.4525 - val_accuracy: 0.5844\n",
      "Epoch 2917/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4143 - accuracy: 0.8659 - val_loss: 1.3930 - val_accuracy: 0.6071\n",
      "Epoch 2918/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3884 - accuracy: 0.8757 - val_loss: 1.3374 - val_accuracy: 0.6104\n",
      "Epoch 2919/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3828 - accuracy: 0.8838 - val_loss: 1.2989 - val_accuracy: 0.6299\n",
      "Epoch 2920/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4473 - accuracy: 0.8672 - val_loss: 1.2671 - val_accuracy: 0.6396\n",
      "Epoch 2921/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4156 - accuracy: 0.8594 - val_loss: 1.2588 - val_accuracy: 0.6396\n",
      "Epoch 2922/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3954 - accuracy: 0.8574 - val_loss: 1.2556 - val_accuracy: 0.6364\n",
      "Epoch 2923/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4093 - accuracy: 0.8701 - val_loss: 1.2731 - val_accuracy: 0.6364\n",
      "Epoch 2924/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3994 - accuracy: 0.8711 - val_loss: 1.2734 - val_accuracy: 0.6396\n",
      "Epoch 2925/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4023 - accuracy: 0.8711 - val_loss: 1.2729 - val_accuracy: 0.6461\n",
      "Epoch 2926/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3820 - accuracy: 0.8770 - val_loss: 1.2686 - val_accuracy: 0.6526\n",
      "Epoch 2927/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4266 - accuracy: 0.8652 - val_loss: 1.2661 - val_accuracy: 0.6526\n",
      "Epoch 2928/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4433 - accuracy: 0.8617 - val_loss: 1.2788 - val_accuracy: 0.6526\n",
      "Epoch 2929/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4454 - accuracy: 0.8659 - val_loss: 1.3002 - val_accuracy: 0.6331\n",
      "Epoch 2930/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4653 - accuracy: 0.8477 - val_loss: 1.3437 - val_accuracy: 0.6234\n",
      "Epoch 2931/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3946 - accuracy: 0.8715 - val_loss: 1.3426 - val_accuracy: 0.6266\n",
      "Epoch 2932/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4327 - accuracy: 0.8631 - val_loss: 1.3210 - val_accuracy: 0.6331\n",
      "Epoch 2933/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4220 - accuracy: 0.8589 - val_loss: 1.2914 - val_accuracy: 0.6364\n",
      "Epoch 2934/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4020 - accuracy: 0.8757 - val_loss: 1.2588 - val_accuracy: 0.6494\n",
      "Epoch 2935/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4610 - accuracy: 0.8492 - val_loss: 1.2374 - val_accuracy: 0.6623\n",
      "Epoch 2936/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3835 - accuracy: 0.8883 - val_loss: 1.2259 - val_accuracy: 0.6623\n",
      "Epoch 2937/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3713 - accuracy: 0.8828 - val_loss: 1.2225 - val_accuracy: 0.6688\n",
      "Epoch 2938/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3706 - accuracy: 0.8789 - val_loss: 1.2157 - val_accuracy: 0.6786\n",
      "Epoch 2939/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4226 - accuracy: 0.8687 - val_loss: 1.2211 - val_accuracy: 0.6753\n",
      "Epoch 2940/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4271 - accuracy: 0.8645 - val_loss: 1.2280 - val_accuracy: 0.6753\n",
      "Epoch 2941/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4316 - accuracy: 0.8617 - val_loss: 1.2388 - val_accuracy: 0.6721\n",
      "Epoch 2942/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3902 - accuracy: 0.8730 - val_loss: 1.2464 - val_accuracy: 0.6656\n",
      "Epoch 2943/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4009 - accuracy: 0.8750 - val_loss: 1.2589 - val_accuracy: 0.6558\n",
      "Epoch 2944/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3830 - accuracy: 0.8682 - val_loss: 1.2773 - val_accuracy: 0.6558\n",
      "Epoch 2945/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3768 - accuracy: 0.8785 - val_loss: 1.2916 - val_accuracy: 0.6461\n",
      "Epoch 2946/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4086 - accuracy: 0.8729 - val_loss: 1.3047 - val_accuracy: 0.6396\n",
      "Epoch 2947/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4110 - accuracy: 0.8673 - val_loss: 1.3037 - val_accuracy: 0.6364\n",
      "Epoch 2948/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4480 - accuracy: 0.8506 - val_loss: 1.3046 - val_accuracy: 0.6396\n",
      "Epoch 2949/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4223 - accuracy: 0.8589 - val_loss: 1.3254 - val_accuracy: 0.6429\n",
      "Epoch 2950/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3826 - accuracy: 0.8673 - val_loss: 1.3252 - val_accuracy: 0.6429\n",
      "Epoch 2951/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3689 - accuracy: 0.8740 - val_loss: 1.3330 - val_accuracy: 0.6461\n",
      "Epoch 2952/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3998 - accuracy: 0.8771 - val_loss: 1.3261 - val_accuracy: 0.6396\n",
      "Epoch 2953/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4044 - accuracy: 0.8617 - val_loss: 1.3070 - val_accuracy: 0.6396\n",
      "Epoch 2954/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3997 - accuracy: 0.8729 - val_loss: 1.2889 - val_accuracy: 0.6526\n",
      "Epoch 2955/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4150 - accuracy: 0.8575 - val_loss: 1.2761 - val_accuracy: 0.6461\n",
      "Epoch 2956/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3929 - accuracy: 0.8617 - val_loss: 1.2728 - val_accuracy: 0.6591\n",
      "Epoch 2957/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4349 - accuracy: 0.8574 - val_loss: 1.2705 - val_accuracy: 0.6558\n",
      "Epoch 2958/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4342 - accuracy: 0.8715 - val_loss: 1.2734 - val_accuracy: 0.6558\n",
      "Epoch 2959/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3517 - accuracy: 0.8730 - val_loss: 1.2817 - val_accuracy: 0.6558\n",
      "Epoch 2960/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3832 - accuracy: 0.8711 - val_loss: 1.3060 - val_accuracy: 0.6461\n",
      "Epoch 2961/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4475 - accuracy: 0.8645 - val_loss: 1.3158 - val_accuracy: 0.6461\n",
      "Epoch 2962/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3601 - accuracy: 0.8896 - val_loss: 1.3269 - val_accuracy: 0.6526\n",
      "Epoch 2963/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3885 - accuracy: 0.8652 - val_loss: 1.3391 - val_accuracy: 0.6429\n",
      "Epoch 2964/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4113 - accuracy: 0.8633 - val_loss: 1.3455 - val_accuracy: 0.6429\n",
      "Epoch 2965/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4636 - accuracy: 0.8450 - val_loss: 1.3453 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2966/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4110 - accuracy: 0.8623 - val_loss: 1.3279 - val_accuracy: 0.6266\n",
      "Epoch 2967/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3519 - accuracy: 0.8827 - val_loss: 1.3105 - val_accuracy: 0.6331\n",
      "Epoch 2968/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3516 - accuracy: 0.8841 - val_loss: 1.3018 - val_accuracy: 0.6429\n",
      "Epoch 2969/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4041 - accuracy: 0.8743 - val_loss: 1.2862 - val_accuracy: 0.6591\n",
      "Epoch 2970/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4416 - accuracy: 0.8589 - val_loss: 1.2760 - val_accuracy: 0.6494\n",
      "Epoch 2971/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3694 - accuracy: 0.8701 - val_loss: 1.2634 - val_accuracy: 0.6656\n",
      "Epoch 2972/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4091 - accuracy: 0.8631 - val_loss: 1.2644 - val_accuracy: 0.6656\n",
      "Epoch 2973/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3955 - accuracy: 0.8623 - val_loss: 1.2739 - val_accuracy: 0.6656\n",
      "Epoch 2974/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4491 - accuracy: 0.8687 - val_loss: 1.2877 - val_accuracy: 0.6526\n",
      "Epoch 2975/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4014 - accuracy: 0.8682 - val_loss: 1.3155 - val_accuracy: 0.6591\n",
      "Epoch 2976/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3928 - accuracy: 0.8673 - val_loss: 1.3501 - val_accuracy: 0.6396\n",
      "Epoch 2977/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3723 - accuracy: 0.8828 - val_loss: 1.3870 - val_accuracy: 0.6364\n",
      "Epoch 2978/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4011 - accuracy: 0.8643 - val_loss: 1.4150 - val_accuracy: 0.6299\n",
      "Epoch 2979/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3825 - accuracy: 0.8779 - val_loss: 1.4320 - val_accuracy: 0.6396\n",
      "Epoch 2980/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4337 - accuracy: 0.8525 - val_loss: 1.4405 - val_accuracy: 0.6331\n",
      "Epoch 2981/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4330 - accuracy: 0.8574 - val_loss: 1.4386 - val_accuracy: 0.6169\n",
      "Epoch 2982/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3800 - accuracy: 0.8740 - val_loss: 1.4083 - val_accuracy: 0.6104\n",
      "Epoch 2983/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3999 - accuracy: 0.8729 - val_loss: 1.3698 - val_accuracy: 0.6071\n",
      "Epoch 2984/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4230 - accuracy: 0.8603 - val_loss: 1.3383 - val_accuracy: 0.6201\n",
      "Epoch 2985/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3824 - accuracy: 0.8855 - val_loss: 1.3115 - val_accuracy: 0.6266\n",
      "Epoch 2986/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3641 - accuracy: 0.8813 - val_loss: 1.2961 - val_accuracy: 0.6396\n",
      "Epoch 2987/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3512 - accuracy: 0.8828 - val_loss: 1.2890 - val_accuracy: 0.6558\n",
      "Epoch 2988/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3772 - accuracy: 0.8715 - val_loss: 1.2931 - val_accuracy: 0.6558\n",
      "Epoch 2989/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3804 - accuracy: 0.8584 - val_loss: 1.3040 - val_accuracy: 0.6429\n",
      "Epoch 2990/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3900 - accuracy: 0.8687 - val_loss: 1.3181 - val_accuracy: 0.6461\n",
      "Epoch 2991/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4626 - accuracy: 0.8506 - val_loss: 1.3105 - val_accuracy: 0.6364\n",
      "Epoch 2992/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4086 - accuracy: 0.8623 - val_loss: 1.3048 - val_accuracy: 0.6364\n",
      "Epoch 2993/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4171 - accuracy: 0.8691 - val_loss: 1.3159 - val_accuracy: 0.6266\n",
      "Epoch 2994/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3853 - accuracy: 0.8633 - val_loss: 1.3116 - val_accuracy: 0.6364\n",
      "Epoch 2995/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4059 - accuracy: 0.8547 - val_loss: 1.2946 - val_accuracy: 0.6623\n",
      "Epoch 2996/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3576 - accuracy: 0.8809 - val_loss: 1.2779 - val_accuracy: 0.6623\n",
      "Epoch 2997/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4187 - accuracy: 0.8561 - val_loss: 1.2677 - val_accuracy: 0.6786\n",
      "Epoch 2998/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3775 - accuracy: 0.8980 - val_loss: 1.2671 - val_accuracy: 0.6786\n",
      "Epoch 2999/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3802 - accuracy: 0.8789 - val_loss: 1.2722 - val_accuracy: 0.6623\n",
      "Epoch 3000/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3850 - accuracy: 0.8643 - val_loss: 1.2994 - val_accuracy: 0.6429\n",
      "Epoch 3001/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3419 - accuracy: 0.8857 - val_loss: 1.3464 - val_accuracy: 0.6461\n",
      "Epoch 3002/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3917 - accuracy: 0.8757 - val_loss: 1.3990 - val_accuracy: 0.6364\n",
      "Epoch 3003/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3466 - accuracy: 0.8799 - val_loss: 1.4336 - val_accuracy: 0.6201\n",
      "Epoch 3004/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4114 - accuracy: 0.8743 - val_loss: 1.4544 - val_accuracy: 0.6104\n",
      "Epoch 3005/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3629 - accuracy: 0.8916 - val_loss: 1.4653 - val_accuracy: 0.6071\n",
      "Epoch 3006/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4212 - accuracy: 0.8643 - val_loss: 1.4482 - val_accuracy: 0.6006\n",
      "Epoch 3007/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3524 - accuracy: 0.8877 - val_loss: 1.4132 - val_accuracy: 0.6136\n",
      "Epoch 3008/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3171 - accuracy: 0.8939 - val_loss: 1.3706 - val_accuracy: 0.6169\n",
      "Epoch 3009/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4320 - accuracy: 0.8687 - val_loss: 1.3281 - val_accuracy: 0.6299\n",
      "Epoch 3010/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4262 - accuracy: 0.8682 - val_loss: 1.3133 - val_accuracy: 0.6591\n",
      "Epoch 3011/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3548 - accuracy: 0.8841 - val_loss: 1.3071 - val_accuracy: 0.6656\n",
      "Epoch 3012/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4037 - accuracy: 0.8645 - val_loss: 1.3036 - val_accuracy: 0.6494\n",
      "Epoch 3013/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3305 - accuracy: 0.8925 - val_loss: 1.3003 - val_accuracy: 0.6526\n",
      "Epoch 3014/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3551 - accuracy: 0.8779 - val_loss: 1.2941 - val_accuracy: 0.6753\n",
      "Epoch 3015/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4106 - accuracy: 0.8662 - val_loss: 1.2932 - val_accuracy: 0.6656\n",
      "Epoch 3016/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3924 - accuracy: 0.8750 - val_loss: 1.2893 - val_accuracy: 0.6721\n",
      "Epoch 3017/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3635 - accuracy: 0.8687 - val_loss: 1.2947 - val_accuracy: 0.6591\n",
      "Epoch 3018/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4068 - accuracy: 0.8687 - val_loss: 1.3214 - val_accuracy: 0.6591\n",
      "Epoch 3019/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3685 - accuracy: 0.8721 - val_loss: 1.3520 - val_accuracy: 0.6494\n",
      "Epoch 3020/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4139 - accuracy: 0.8659 - val_loss: 1.3762 - val_accuracy: 0.6201\n",
      "Epoch 3021/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3765 - accuracy: 0.8673 - val_loss: 1.4243 - val_accuracy: 0.6169\n",
      "Epoch 3022/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4191 - accuracy: 0.8506 - val_loss: 1.4736 - val_accuracy: 0.5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3023/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4645 - accuracy: 0.8477 - val_loss: 1.5167 - val_accuracy: 0.5877\n",
      "Epoch 3024/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3779 - accuracy: 0.8730 - val_loss: 1.5251 - val_accuracy: 0.5779\n",
      "Epoch 3025/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3795 - accuracy: 0.8631 - val_loss: 1.5139 - val_accuracy: 0.5747\n",
      "Epoch 3026/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4142 - accuracy: 0.8643 - val_loss: 1.4867 - val_accuracy: 0.5812\n",
      "Epoch 3027/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4829 - accuracy: 0.8506 - val_loss: 1.4425 - val_accuracy: 0.5877\n",
      "Epoch 3028/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4304 - accuracy: 0.8631 - val_loss: 1.4085 - val_accuracy: 0.5974\n",
      "Epoch 3029/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3466 - accuracy: 0.8869 - val_loss: 1.3721 - val_accuracy: 0.6169\n",
      "Epoch 3030/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3868 - accuracy: 0.8760 - val_loss: 1.3571 - val_accuracy: 0.6201\n",
      "Epoch 3031/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4114 - accuracy: 0.8687 - val_loss: 1.3582 - val_accuracy: 0.6136\n",
      "Epoch 3032/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3496 - accuracy: 0.8809 - val_loss: 1.3923 - val_accuracy: 0.5974\n",
      "Epoch 3033/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3588 - accuracy: 0.8838 - val_loss: 1.4239 - val_accuracy: 0.5909\n",
      "Epoch 3034/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4419 - accuracy: 0.8574 - val_loss: 1.4626 - val_accuracy: 0.5877\n",
      "Epoch 3035/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3549 - accuracy: 0.8867 - val_loss: 1.5141 - val_accuracy: 0.5779\n",
      "Epoch 3036/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4025 - accuracy: 0.8687 - val_loss: 1.5719 - val_accuracy: 0.5649\n",
      "Epoch 3037/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3937 - accuracy: 0.8659 - val_loss: 1.5934 - val_accuracy: 0.5584\n",
      "Epoch 3038/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3674 - accuracy: 0.8771 - val_loss: 1.6017 - val_accuracy: 0.5487\n",
      "Epoch 3039/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3840 - accuracy: 0.8779 - val_loss: 1.5521 - val_accuracy: 0.5682\n",
      "Epoch 3040/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3439 - accuracy: 0.8896 - val_loss: 1.4813 - val_accuracy: 0.5909\n",
      "Epoch 3041/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3503 - accuracy: 0.8911 - val_loss: 1.4105 - val_accuracy: 0.6104\n",
      "Epoch 3042/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3657 - accuracy: 0.8867 - val_loss: 1.3328 - val_accuracy: 0.6396\n",
      "Epoch 3043/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3652 - accuracy: 0.8869 - val_loss: 1.2804 - val_accuracy: 0.6558\n",
      "Epoch 3044/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4067 - accuracy: 0.8643 - val_loss: 1.2381 - val_accuracy: 0.6688\n",
      "Epoch 3045/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3950 - accuracy: 0.8760 - val_loss: 1.2207 - val_accuracy: 0.6656\n",
      "Epoch 3046/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4868 - accuracy: 0.8506 - val_loss: 1.2155 - val_accuracy: 0.6623\n",
      "Epoch 3047/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3466 - accuracy: 0.8687 - val_loss: 1.2154 - val_accuracy: 0.6721\n",
      "Epoch 3048/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3733 - accuracy: 0.8701 - val_loss: 1.2144 - val_accuracy: 0.6688\n",
      "Epoch 3049/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3682 - accuracy: 0.8813 - val_loss: 1.2259 - val_accuracy: 0.6623\n",
      "Epoch 3050/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3584 - accuracy: 0.8869 - val_loss: 1.2422 - val_accuracy: 0.6558\n",
      "Epoch 3051/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4055 - accuracy: 0.8604 - val_loss: 1.2572 - val_accuracy: 0.6396\n",
      "Epoch 3052/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4064 - accuracy: 0.8799 - val_loss: 1.2747 - val_accuracy: 0.6429\n",
      "Epoch 3053/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3962 - accuracy: 0.8760 - val_loss: 1.3164 - val_accuracy: 0.6396\n",
      "Epoch 3054/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3701 - accuracy: 0.8682 - val_loss: 1.3491 - val_accuracy: 0.6234\n",
      "Epoch 3055/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4202 - accuracy: 0.8617 - val_loss: 1.3497 - val_accuracy: 0.6136\n",
      "Epoch 3056/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4088 - accuracy: 0.8652 - val_loss: 1.3502 - val_accuracy: 0.6201\n",
      "Epoch 3057/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3863 - accuracy: 0.8771 - val_loss: 1.3393 - val_accuracy: 0.6169\n",
      "Epoch 3058/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3507 - accuracy: 0.8813 - val_loss: 1.3331 - val_accuracy: 0.6234\n",
      "Epoch 3059/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3448 - accuracy: 0.8841 - val_loss: 1.3295 - val_accuracy: 0.6299\n",
      "Epoch 3060/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3838 - accuracy: 0.8813 - val_loss: 1.3205 - val_accuracy: 0.6364\n",
      "Epoch 3061/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3993 - accuracy: 0.8813 - val_loss: 1.3327 - val_accuracy: 0.6364\n",
      "Epoch 3062/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4079 - accuracy: 0.8757 - val_loss: 1.3821 - val_accuracy: 0.6234\n",
      "Epoch 3063/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3804 - accuracy: 0.8887 - val_loss: 1.4438 - val_accuracy: 0.6071\n",
      "Epoch 3064/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3619 - accuracy: 0.8809 - val_loss: 1.5164 - val_accuracy: 0.5942\n",
      "Epoch 3065/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4216 - accuracy: 0.8589 - val_loss: 1.6022 - val_accuracy: 0.5844\n",
      "Epoch 3066/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3769 - accuracy: 0.8818 - val_loss: 1.6926 - val_accuracy: 0.5682\n",
      "Epoch 3067/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3966 - accuracy: 0.8721 - val_loss: 1.7337 - val_accuracy: 0.5714\n",
      "Epoch 3068/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3846 - accuracy: 0.8740 - val_loss: 1.7359 - val_accuracy: 0.5747\n",
      "Epoch 3069/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3896 - accuracy: 0.8743 - val_loss: 1.6865 - val_accuracy: 0.5812\n",
      "Epoch 3070/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4045 - accuracy: 0.8534 - val_loss: 1.5789 - val_accuracy: 0.5974\n",
      "Epoch 3071/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3780 - accuracy: 0.8779 - val_loss: 1.4904 - val_accuracy: 0.6006\n",
      "Epoch 3072/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3721 - accuracy: 0.8715 - val_loss: 1.4252 - val_accuracy: 0.6104\n",
      "Epoch 3073/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4152 - accuracy: 0.8672 - val_loss: 1.3850 - val_accuracy: 0.6039\n",
      "Epoch 3074/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3990 - accuracy: 0.8743 - val_loss: 1.3704 - val_accuracy: 0.6071\n",
      "Epoch 3075/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3958 - accuracy: 0.8672 - val_loss: 1.3570 - val_accuracy: 0.6266\n",
      "Epoch 3076/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3899 - accuracy: 0.8750 - val_loss: 1.3531 - val_accuracy: 0.6364\n",
      "Epoch 3077/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3941 - accuracy: 0.8789 - val_loss: 1.3660 - val_accuracy: 0.6364\n",
      "Epoch 3078/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3557 - accuracy: 0.8838 - val_loss: 1.3774 - val_accuracy: 0.6396\n",
      "Epoch 3079/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3878 - accuracy: 0.8711 - val_loss: 1.4037 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3080/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3447 - accuracy: 0.8877 - val_loss: 1.4214 - val_accuracy: 0.6331\n",
      "Epoch 3081/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4036 - accuracy: 0.8631 - val_loss: 1.4438 - val_accuracy: 0.6201\n",
      "Epoch 3082/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3864 - accuracy: 0.8771 - val_loss: 1.4694 - val_accuracy: 0.6039\n",
      "Epoch 3083/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3994 - accuracy: 0.8659 - val_loss: 1.4803 - val_accuracy: 0.5974\n",
      "Epoch 3084/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3989 - accuracy: 0.8729 - val_loss: 1.4888 - val_accuracy: 0.6039\n",
      "Epoch 3085/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4013 - accuracy: 0.8633 - val_loss: 1.4807 - val_accuracy: 0.6039\n",
      "Epoch 3086/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4238 - accuracy: 0.8701 - val_loss: 1.4588 - val_accuracy: 0.5974\n",
      "Epoch 3087/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4632 - accuracy: 0.8436 - val_loss: 1.4497 - val_accuracy: 0.6104\n",
      "Epoch 3088/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3976 - accuracy: 0.8740 - val_loss: 1.4416 - val_accuracy: 0.6234\n",
      "Epoch 3089/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4026 - accuracy: 0.8673 - val_loss: 1.4164 - val_accuracy: 0.6396\n",
      "Epoch 3090/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4030 - accuracy: 0.8771 - val_loss: 1.3884 - val_accuracy: 0.6526\n",
      "Epoch 3091/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3613 - accuracy: 0.8841 - val_loss: 1.3653 - val_accuracy: 0.6558\n",
      "Epoch 3092/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3829 - accuracy: 0.8701 - val_loss: 1.3373 - val_accuracy: 0.6526\n",
      "Epoch 3093/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4091 - accuracy: 0.8631 - val_loss: 1.3149 - val_accuracy: 0.6688\n",
      "Epoch 3094/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4441 - accuracy: 0.8631 - val_loss: 1.3041 - val_accuracy: 0.6656\n",
      "Epoch 3095/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3826 - accuracy: 0.8672 - val_loss: 1.2908 - val_accuracy: 0.6656\n",
      "Epoch 3096/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3307 - accuracy: 0.8869 - val_loss: 1.2965 - val_accuracy: 0.6688\n",
      "Epoch 3097/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3654 - accuracy: 0.8779 - val_loss: 1.2997 - val_accuracy: 0.6721\n",
      "Epoch 3098/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3888 - accuracy: 0.8730 - val_loss: 1.3007 - val_accuracy: 0.6688\n",
      "Epoch 3099/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3817 - accuracy: 0.8701 - val_loss: 1.2981 - val_accuracy: 0.6591\n",
      "Epoch 3100/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3551 - accuracy: 0.8869 - val_loss: 1.2981 - val_accuracy: 0.6526\n",
      "Epoch 3101/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3854 - accuracy: 0.8701 - val_loss: 1.2793 - val_accuracy: 0.6461\n",
      "Epoch 3102/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3056 - accuracy: 0.8925 - val_loss: 1.2654 - val_accuracy: 0.6558\n",
      "Epoch 3103/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3191 - accuracy: 0.8925 - val_loss: 1.2618 - val_accuracy: 0.6558\n",
      "Epoch 3104/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3334 - accuracy: 0.8925 - val_loss: 1.2648 - val_accuracy: 0.6656\n",
      "Epoch 3105/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4216 - accuracy: 0.8729 - val_loss: 1.2662 - val_accuracy: 0.6558\n",
      "Epoch 3106/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3879 - accuracy: 0.8673 - val_loss: 1.2714 - val_accuracy: 0.6591\n",
      "Epoch 3107/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4161 - accuracy: 0.8617 - val_loss: 1.2822 - val_accuracy: 0.6623\n",
      "Epoch 3108/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3572 - accuracy: 0.8848 - val_loss: 1.2977 - val_accuracy: 0.6656\n",
      "Epoch 3109/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3055 - accuracy: 0.8994 - val_loss: 1.3033 - val_accuracy: 0.6623\n",
      "Epoch 3110/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3562 - accuracy: 0.8841 - val_loss: 1.3081 - val_accuracy: 0.6623\n",
      "Epoch 3111/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3915 - accuracy: 0.8779 - val_loss: 1.3104 - val_accuracy: 0.6721\n",
      "Epoch 3112/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3507 - accuracy: 0.8965 - val_loss: 1.3098 - val_accuracy: 0.6688\n",
      "Epoch 3113/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3980 - accuracy: 0.8701 - val_loss: 1.3160 - val_accuracy: 0.6591\n",
      "Epoch 3114/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3566 - accuracy: 0.8785 - val_loss: 1.3326 - val_accuracy: 0.6526\n",
      "Epoch 3115/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3626 - accuracy: 0.8779 - val_loss: 1.3500 - val_accuracy: 0.6461\n",
      "Epoch 3116/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4270 - accuracy: 0.8603 - val_loss: 1.3437 - val_accuracy: 0.6494\n",
      "Epoch 3117/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3909 - accuracy: 0.8520 - val_loss: 1.3362 - val_accuracy: 0.6396\n",
      "Epoch 3118/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3217 - accuracy: 0.8916 - val_loss: 1.3304 - val_accuracy: 0.6429\n",
      "Epoch 3119/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3639 - accuracy: 0.8818 - val_loss: 1.3227 - val_accuracy: 0.6461\n",
      "Epoch 3120/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4323 - accuracy: 0.8589 - val_loss: 1.3159 - val_accuracy: 0.6429\n",
      "Epoch 3121/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4179 - accuracy: 0.8617 - val_loss: 1.3175 - val_accuracy: 0.6591\n",
      "Epoch 3122/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3259 - accuracy: 0.8916 - val_loss: 1.3214 - val_accuracy: 0.6558\n",
      "Epoch 3123/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3400 - accuracy: 0.8841 - val_loss: 1.3280 - val_accuracy: 0.6558\n",
      "Epoch 3124/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3938 - accuracy: 0.8659 - val_loss: 1.3289 - val_accuracy: 0.6591\n",
      "Epoch 3125/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4044 - accuracy: 0.8701 - val_loss: 1.3233 - val_accuracy: 0.6591\n",
      "Epoch 3126/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4146 - accuracy: 0.8813 - val_loss: 1.3206 - val_accuracy: 0.6623\n",
      "Epoch 3127/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3495 - accuracy: 0.8883 - val_loss: 1.3189 - val_accuracy: 0.6623\n",
      "Epoch 3128/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3856 - accuracy: 0.8813 - val_loss: 1.3235 - val_accuracy: 0.6526\n",
      "Epoch 3129/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3782 - accuracy: 0.8715 - val_loss: 1.3307 - val_accuracy: 0.6461\n",
      "Epoch 3130/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3331 - accuracy: 0.8955 - val_loss: 1.3729 - val_accuracy: 0.6396\n",
      "Epoch 3131/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4004 - accuracy: 0.8785 - val_loss: 1.4216 - val_accuracy: 0.6104\n",
      "Epoch 3132/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3786 - accuracy: 0.8770 - val_loss: 1.4629 - val_accuracy: 0.6039\n",
      "Epoch 3133/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4080 - accuracy: 0.8673 - val_loss: 1.5047 - val_accuracy: 0.5877\n",
      "Epoch 3134/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4055 - accuracy: 0.8691 - val_loss: 1.5124 - val_accuracy: 0.5812\n",
      "Epoch 3135/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3829 - accuracy: 0.8813 - val_loss: 1.4910 - val_accuracy: 0.5877\n",
      "Epoch 3136/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4083 - accuracy: 0.8701 - val_loss: 1.4573 - val_accuracy: 0.5877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3137/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4130 - accuracy: 0.8730 - val_loss: 1.3968 - val_accuracy: 0.6039\n",
      "Epoch 3138/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4101 - accuracy: 0.8645 - val_loss: 1.3443 - val_accuracy: 0.6331\n",
      "Epoch 3139/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3759 - accuracy: 0.8841 - val_loss: 1.3137 - val_accuracy: 0.6494\n",
      "Epoch 3140/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3953 - accuracy: 0.8730 - val_loss: 1.3006 - val_accuracy: 0.6494\n",
      "Epoch 3141/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4191 - accuracy: 0.8659 - val_loss: 1.3008 - val_accuracy: 0.6591\n",
      "Epoch 3142/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3900 - accuracy: 0.8691 - val_loss: 1.2969 - val_accuracy: 0.6656\n",
      "Epoch 3143/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3651 - accuracy: 0.8906 - val_loss: 1.2839 - val_accuracy: 0.6656\n",
      "Epoch 3144/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3543 - accuracy: 0.8757 - val_loss: 1.2690 - val_accuracy: 0.6591\n",
      "Epoch 3145/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4051 - accuracy: 0.8613 - val_loss: 1.2590 - val_accuracy: 0.6494\n",
      "Epoch 3146/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4062 - accuracy: 0.8617 - val_loss: 1.2545 - val_accuracy: 0.6688\n",
      "Epoch 3147/4000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3589 - accuracy: 0.8838 - val_loss: 1.2526 - val_accuracy: 0.6721\n",
      "Epoch 3148/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3632 - accuracy: 0.8617 - val_loss: 1.2588 - val_accuracy: 0.6721\n",
      "Epoch 3149/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3474 - accuracy: 0.8945 - val_loss: 1.2671 - val_accuracy: 0.6688\n",
      "Epoch 3150/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3965 - accuracy: 0.8771 - val_loss: 1.2586 - val_accuracy: 0.6623\n",
      "Epoch 3151/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3653 - accuracy: 0.8730 - val_loss: 1.2570 - val_accuracy: 0.6656\n",
      "Epoch 3152/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4169 - accuracy: 0.8662 - val_loss: 1.2590 - val_accuracy: 0.6558\n",
      "Epoch 3153/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3662 - accuracy: 0.8867 - val_loss: 1.2558 - val_accuracy: 0.6558\n",
      "Epoch 3154/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3255 - accuracy: 0.8953 - val_loss: 1.2486 - val_accuracy: 0.6494\n",
      "Epoch 3155/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3735 - accuracy: 0.8757 - val_loss: 1.2504 - val_accuracy: 0.6526\n",
      "Epoch 3156/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3994 - accuracy: 0.8662 - val_loss: 1.2521 - val_accuracy: 0.6591\n",
      "Epoch 3157/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3657 - accuracy: 0.8857 - val_loss: 1.2589 - val_accuracy: 0.6558\n",
      "Epoch 3158/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3694 - accuracy: 0.8672 - val_loss: 1.2703 - val_accuracy: 0.6558\n",
      "Epoch 3159/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4053 - accuracy: 0.8789 - val_loss: 1.2858 - val_accuracy: 0.6461\n",
      "Epoch 3160/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3589 - accuracy: 0.8799 - val_loss: 1.3050 - val_accuracy: 0.6526\n",
      "Epoch 3161/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3832 - accuracy: 0.8721 - val_loss: 1.3195 - val_accuracy: 0.6461\n",
      "Epoch 3162/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3655 - accuracy: 0.8785 - val_loss: 1.3210 - val_accuracy: 0.6623\n",
      "Epoch 3163/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3801 - accuracy: 0.8701 - val_loss: 1.3246 - val_accuracy: 0.6526\n",
      "Epoch 3164/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4168 - accuracy: 0.8687 - val_loss: 1.3315 - val_accuracy: 0.6429\n",
      "Epoch 3165/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3701 - accuracy: 0.8827 - val_loss: 1.3339 - val_accuracy: 0.6331\n",
      "Epoch 3166/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3801 - accuracy: 0.8750 - val_loss: 1.3422 - val_accuracy: 0.6364\n",
      "Epoch 3167/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3925 - accuracy: 0.8715 - val_loss: 1.3538 - val_accuracy: 0.6299\n",
      "Epoch 3168/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3642 - accuracy: 0.8789 - val_loss: 1.3553 - val_accuracy: 0.6201\n",
      "Epoch 3169/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4339 - accuracy: 0.8574 - val_loss: 1.3524 - val_accuracy: 0.6006\n",
      "Epoch 3170/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4253 - accuracy: 0.8574 - val_loss: 1.3503 - val_accuracy: 0.6104\n",
      "Epoch 3171/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3875 - accuracy: 0.8729 - val_loss: 1.3410 - val_accuracy: 0.6266\n",
      "Epoch 3172/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3790 - accuracy: 0.8743 - val_loss: 1.3312 - val_accuracy: 0.6234\n",
      "Epoch 3173/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3843 - accuracy: 0.8757 - val_loss: 1.3159 - val_accuracy: 0.6266\n",
      "Epoch 3174/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3519 - accuracy: 0.8857 - val_loss: 1.2951 - val_accuracy: 0.6364\n",
      "Epoch 3175/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3806 - accuracy: 0.8785 - val_loss: 1.2879 - val_accuracy: 0.6266\n",
      "Epoch 3176/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3806 - accuracy: 0.8809 - val_loss: 1.2942 - val_accuracy: 0.6396\n",
      "Epoch 3177/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3494 - accuracy: 0.8785 - val_loss: 1.3080 - val_accuracy: 0.6396\n",
      "Epoch 3178/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3888 - accuracy: 0.8645 - val_loss: 1.3166 - val_accuracy: 0.6461\n",
      "Epoch 3179/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3803 - accuracy: 0.8673 - val_loss: 1.3220 - val_accuracy: 0.6299\n",
      "Epoch 3180/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4267 - accuracy: 0.8673 - val_loss: 1.3314 - val_accuracy: 0.6331\n",
      "Epoch 3181/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3626 - accuracy: 0.8813 - val_loss: 1.3395 - val_accuracy: 0.6331\n",
      "Epoch 3182/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2939 - accuracy: 0.9050 - val_loss: 1.3462 - val_accuracy: 0.6396\n",
      "Epoch 3183/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4042 - accuracy: 0.8682 - val_loss: 1.3353 - val_accuracy: 0.6429\n",
      "Epoch 3184/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3857 - accuracy: 0.8730 - val_loss: 1.3096 - val_accuracy: 0.6461\n",
      "Epoch 3185/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3651 - accuracy: 0.8779 - val_loss: 1.2824 - val_accuracy: 0.6526\n",
      "Epoch 3186/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4119 - accuracy: 0.8760 - val_loss: 1.2652 - val_accuracy: 0.6591\n",
      "Epoch 3187/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3943 - accuracy: 0.8506 - val_loss: 1.2477 - val_accuracy: 0.6656\n",
      "Epoch 3188/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3418 - accuracy: 0.8855 - val_loss: 1.2438 - val_accuracy: 0.6721\n",
      "Epoch 3189/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4199 - accuracy: 0.8603 - val_loss: 1.2574 - val_accuracy: 0.6753\n",
      "Epoch 3190/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4399 - accuracy: 0.8645 - val_loss: 1.2556 - val_accuracy: 0.6753\n",
      "Epoch 3191/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3818 - accuracy: 0.8897 - val_loss: 1.2486 - val_accuracy: 0.6753\n",
      "Epoch 3192/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4048 - accuracy: 0.8561 - val_loss: 1.2381 - val_accuracy: 0.6818\n",
      "Epoch 3193/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3902 - accuracy: 0.8771 - val_loss: 1.2338 - val_accuracy: 0.6721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3194/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3880 - accuracy: 0.8687 - val_loss: 1.2385 - val_accuracy: 0.6656\n",
      "Epoch 3195/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3722 - accuracy: 0.8857 - val_loss: 1.2492 - val_accuracy: 0.6656\n",
      "Epoch 3196/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3688 - accuracy: 0.8771 - val_loss: 1.2575 - val_accuracy: 0.6688\n",
      "Epoch 3197/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3867 - accuracy: 0.8757 - val_loss: 1.2396 - val_accuracy: 0.6753\n",
      "Epoch 3198/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3212 - accuracy: 0.9022 - val_loss: 1.2214 - val_accuracy: 0.6786\n",
      "Epoch 3199/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3929 - accuracy: 0.8721 - val_loss: 1.2139 - val_accuracy: 0.6721\n",
      "Epoch 3200/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3487 - accuracy: 0.8896 - val_loss: 1.2130 - val_accuracy: 0.6688\n",
      "Epoch 3201/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3477 - accuracy: 0.8828 - val_loss: 1.2165 - val_accuracy: 0.6721\n",
      "Epoch 3202/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3985 - accuracy: 0.8701 - val_loss: 1.2199 - val_accuracy: 0.6688\n",
      "Epoch 3203/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3244 - accuracy: 0.8966 - val_loss: 1.2228 - val_accuracy: 0.6688\n",
      "Epoch 3204/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3791 - accuracy: 0.8687 - val_loss: 1.2242 - val_accuracy: 0.6656\n",
      "Epoch 3205/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4342 - accuracy: 0.8673 - val_loss: 1.2256 - val_accuracy: 0.6591\n",
      "Epoch 3206/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3992 - accuracy: 0.8757 - val_loss: 1.2187 - val_accuracy: 0.6558\n",
      "Epoch 3207/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3684 - accuracy: 0.8818 - val_loss: 1.2133 - val_accuracy: 0.6558\n",
      "Epoch 3208/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3900 - accuracy: 0.8789 - val_loss: 1.2128 - val_accuracy: 0.6558\n",
      "Epoch 3209/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3448 - accuracy: 0.8883 - val_loss: 1.2201 - val_accuracy: 0.6526\n",
      "Epoch 3210/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3726 - accuracy: 0.8799 - val_loss: 1.2252 - val_accuracy: 0.6494\n",
      "Epoch 3211/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3786 - accuracy: 0.8682 - val_loss: 1.2306 - val_accuracy: 0.6558\n",
      "Epoch 3212/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3624 - accuracy: 0.8730 - val_loss: 1.2412 - val_accuracy: 0.6623\n",
      "Epoch 3213/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3416 - accuracy: 0.8955 - val_loss: 1.2543 - val_accuracy: 0.6558\n",
      "Epoch 3214/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3518 - accuracy: 0.8828 - val_loss: 1.2581 - val_accuracy: 0.6429\n",
      "Epoch 3215/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3391 - accuracy: 0.8841 - val_loss: 1.2584 - val_accuracy: 0.6558\n",
      "Epoch 3216/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3751 - accuracy: 0.8730 - val_loss: 1.2674 - val_accuracy: 0.6558\n",
      "Epoch 3217/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3188 - accuracy: 0.8855 - val_loss: 1.2739 - val_accuracy: 0.6591\n",
      "Epoch 3218/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3964 - accuracy: 0.8740 - val_loss: 1.2818 - val_accuracy: 0.6558\n",
      "Epoch 3219/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3635 - accuracy: 0.8770 - val_loss: 1.2884 - val_accuracy: 0.6558\n",
      "Epoch 3220/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4151 - accuracy: 0.8740 - val_loss: 1.2899 - val_accuracy: 0.6591\n",
      "Epoch 3221/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4144 - accuracy: 0.8584 - val_loss: 1.2932 - val_accuracy: 0.6688\n",
      "Epoch 3222/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4222 - accuracy: 0.8643 - val_loss: 1.2916 - val_accuracy: 0.6656\n",
      "Epoch 3223/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3480 - accuracy: 0.9050 - val_loss: 1.3003 - val_accuracy: 0.6558\n",
      "Epoch 3224/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4226 - accuracy: 0.8682 - val_loss: 1.3099 - val_accuracy: 0.6494\n",
      "Epoch 3225/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3813 - accuracy: 0.8828 - val_loss: 1.3193 - val_accuracy: 0.6494\n",
      "Epoch 3226/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3436 - accuracy: 0.8813 - val_loss: 1.3291 - val_accuracy: 0.6494\n",
      "Epoch 3227/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4018 - accuracy: 0.8631 - val_loss: 1.3398 - val_accuracy: 0.6623\n",
      "Epoch 3228/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3468 - accuracy: 0.8936 - val_loss: 1.3453 - val_accuracy: 0.6494\n",
      "Epoch 3229/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3757 - accuracy: 0.8770 - val_loss: 1.3515 - val_accuracy: 0.6558\n",
      "Epoch 3230/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4255 - accuracy: 0.8652 - val_loss: 1.3576 - val_accuracy: 0.6558\n",
      "Epoch 3231/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3892 - accuracy: 0.8740 - val_loss: 1.3683 - val_accuracy: 0.6461\n",
      "Epoch 3232/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3314 - accuracy: 0.8789 - val_loss: 1.3739 - val_accuracy: 0.6461\n",
      "Epoch 3233/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3209 - accuracy: 0.8945 - val_loss: 1.3806 - val_accuracy: 0.6429\n",
      "Epoch 3234/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3713 - accuracy: 0.8701 - val_loss: 1.3836 - val_accuracy: 0.6396\n",
      "Epoch 3235/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3206 - accuracy: 0.8877 - val_loss: 1.3902 - val_accuracy: 0.6396\n",
      "Epoch 3236/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3179 - accuracy: 0.8896 - val_loss: 1.3997 - val_accuracy: 0.6331\n",
      "Epoch 3237/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3465 - accuracy: 0.8911 - val_loss: 1.4065 - val_accuracy: 0.6299\n",
      "Epoch 3238/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3331 - accuracy: 0.8906 - val_loss: 1.4094 - val_accuracy: 0.6169\n",
      "Epoch 3239/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3848 - accuracy: 0.8750 - val_loss: 1.4087 - val_accuracy: 0.6104\n",
      "Epoch 3240/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3275 - accuracy: 0.8743 - val_loss: 1.3968 - val_accuracy: 0.6169\n",
      "Epoch 3241/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3831 - accuracy: 0.8827 - val_loss: 1.3855 - val_accuracy: 0.6136\n",
      "Epoch 3242/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3559 - accuracy: 0.8945 - val_loss: 1.3775 - val_accuracy: 0.6266\n",
      "Epoch 3243/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4081 - accuracy: 0.8760 - val_loss: 1.3740 - val_accuracy: 0.6364\n",
      "Epoch 3244/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4028 - accuracy: 0.8779 - val_loss: 1.3718 - val_accuracy: 0.6364\n",
      "Epoch 3245/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3396 - accuracy: 0.9008 - val_loss: 1.3750 - val_accuracy: 0.6331\n",
      "Epoch 3246/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3547 - accuracy: 0.8867 - val_loss: 1.3736 - val_accuracy: 0.6364\n",
      "Epoch 3247/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3404 - accuracy: 0.8857 - val_loss: 1.3538 - val_accuracy: 0.6429\n",
      "Epoch 3248/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3627 - accuracy: 0.8729 - val_loss: 1.3297 - val_accuracy: 0.6526\n",
      "Epoch 3249/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3672 - accuracy: 0.8770 - val_loss: 1.3145 - val_accuracy: 0.6461\n",
      "Epoch 3250/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4184 - accuracy: 0.8645 - val_loss: 1.2977 - val_accuracy: 0.6558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3251/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3023 - accuracy: 0.8955 - val_loss: 1.2896 - val_accuracy: 0.6656\n",
      "Epoch 3252/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3396 - accuracy: 0.8945 - val_loss: 1.2794 - val_accuracy: 0.6753\n",
      "Epoch 3253/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2475 - accuracy: 0.9162 - val_loss: 1.2735 - val_accuracy: 0.6818\n",
      "Epoch 3254/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3586 - accuracy: 0.8841 - val_loss: 1.2826 - val_accuracy: 0.6656\n",
      "Epoch 3255/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4156 - accuracy: 0.8645 - val_loss: 1.2955 - val_accuracy: 0.6494\n",
      "Epoch 3256/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3562 - accuracy: 0.8848 - val_loss: 1.3287 - val_accuracy: 0.6331\n",
      "Epoch 3257/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4006 - accuracy: 0.8750 - val_loss: 1.3619 - val_accuracy: 0.6136\n",
      "Epoch 3258/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3551 - accuracy: 0.8799 - val_loss: 1.3838 - val_accuracy: 0.6136\n",
      "Epoch 3259/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3447 - accuracy: 0.8936 - val_loss: 1.3985 - val_accuracy: 0.6071\n",
      "Epoch 3260/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3701 - accuracy: 0.8813 - val_loss: 1.4017 - val_accuracy: 0.6104\n",
      "Epoch 3261/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3962 - accuracy: 0.8869 - val_loss: 1.3965 - val_accuracy: 0.6104\n",
      "Epoch 3262/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3734 - accuracy: 0.8813 - val_loss: 1.3630 - val_accuracy: 0.6201\n",
      "Epoch 3263/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3379 - accuracy: 0.8770 - val_loss: 1.3327 - val_accuracy: 0.6039\n",
      "Epoch 3264/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3495 - accuracy: 0.8877 - val_loss: 1.3215 - val_accuracy: 0.6104\n",
      "Epoch 3265/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3881 - accuracy: 0.8673 - val_loss: 1.3190 - val_accuracy: 0.6169\n",
      "Epoch 3266/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3816 - accuracy: 0.8743 - val_loss: 1.3249 - val_accuracy: 0.6136\n",
      "Epoch 3267/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3734 - accuracy: 0.8743 - val_loss: 1.3369 - val_accuracy: 0.6104\n",
      "Epoch 3268/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3294 - accuracy: 0.8883 - val_loss: 1.3720 - val_accuracy: 0.6071\n",
      "Epoch 3269/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4105 - accuracy: 0.8770 - val_loss: 1.4287 - val_accuracy: 0.6039\n",
      "Epoch 3270/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3673 - accuracy: 0.8682 - val_loss: 1.4812 - val_accuracy: 0.5909\n",
      "Epoch 3271/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3420 - accuracy: 0.8906 - val_loss: 1.5179 - val_accuracy: 0.5877\n",
      "Epoch 3272/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3576 - accuracy: 0.8911 - val_loss: 1.5244 - val_accuracy: 0.5844\n",
      "Epoch 3273/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4256 - accuracy: 0.8631 - val_loss: 1.5249 - val_accuracy: 0.5747\n",
      "Epoch 3274/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3581 - accuracy: 0.8906 - val_loss: 1.4964 - val_accuracy: 0.6006\n",
      "Epoch 3275/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3675 - accuracy: 0.8799 - val_loss: 1.4719 - val_accuracy: 0.6039\n",
      "Epoch 3276/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4095 - accuracy: 0.8534 - val_loss: 1.4782 - val_accuracy: 0.5942\n",
      "Epoch 3277/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3075 - accuracy: 0.8877 - val_loss: 1.4748 - val_accuracy: 0.5844\n",
      "Epoch 3278/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3735 - accuracy: 0.8877 - val_loss: 1.4640 - val_accuracy: 0.5909\n",
      "Epoch 3279/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3848 - accuracy: 0.8799 - val_loss: 1.4565 - val_accuracy: 0.6104\n",
      "Epoch 3280/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3055 - accuracy: 0.9148 - val_loss: 1.4772 - val_accuracy: 0.6039\n",
      "Epoch 3281/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3506 - accuracy: 0.8925 - val_loss: 1.4838 - val_accuracy: 0.6104\n",
      "Epoch 3282/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3562 - accuracy: 0.8848 - val_loss: 1.4979 - val_accuracy: 0.6039\n",
      "Epoch 3283/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3442 - accuracy: 0.8838 - val_loss: 1.5118 - val_accuracy: 0.6071\n",
      "Epoch 3284/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3828 - accuracy: 0.8729 - val_loss: 1.5260 - val_accuracy: 0.6071\n",
      "Epoch 3285/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3700 - accuracy: 0.8691 - val_loss: 1.5400 - val_accuracy: 0.6006\n",
      "Epoch 3286/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3847 - accuracy: 0.8799 - val_loss: 1.5692 - val_accuracy: 0.5942\n",
      "Epoch 3287/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3434 - accuracy: 0.8827 - val_loss: 1.5943 - val_accuracy: 0.5844\n",
      "Epoch 3288/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3508 - accuracy: 0.8715 - val_loss: 1.6092 - val_accuracy: 0.5649\n",
      "Epoch 3289/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3316 - accuracy: 0.8771 - val_loss: 1.6186 - val_accuracy: 0.5617\n",
      "Epoch 3290/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3125 - accuracy: 0.8911 - val_loss: 1.5951 - val_accuracy: 0.5649\n",
      "Epoch 3291/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3338 - accuracy: 0.8953 - val_loss: 1.5559 - val_accuracy: 0.5714\n",
      "Epoch 3292/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3361 - accuracy: 0.9064 - val_loss: 1.5201 - val_accuracy: 0.5877\n",
      "Epoch 3293/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3387 - accuracy: 0.8809 - val_loss: 1.4984 - val_accuracy: 0.5942\n",
      "Epoch 3294/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3460 - accuracy: 0.8911 - val_loss: 1.5008 - val_accuracy: 0.5942\n",
      "Epoch 3295/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3319 - accuracy: 0.9033 - val_loss: 1.5163 - val_accuracy: 0.5909\n",
      "Epoch 3296/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3559 - accuracy: 0.8841 - val_loss: 1.5237 - val_accuracy: 0.5942\n",
      "Epoch 3297/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3580 - accuracy: 0.8867 - val_loss: 1.5196 - val_accuracy: 0.5942\n",
      "Epoch 3298/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3932 - accuracy: 0.8701 - val_loss: 1.5123 - val_accuracy: 0.6039\n",
      "Epoch 3299/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3442 - accuracy: 0.8799 - val_loss: 1.4981 - val_accuracy: 0.6071\n",
      "Epoch 3300/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3021 - accuracy: 0.8965 - val_loss: 1.4775 - val_accuracy: 0.6201\n",
      "Epoch 3301/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3426 - accuracy: 0.8799 - val_loss: 1.4533 - val_accuracy: 0.6201\n",
      "Epoch 3302/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3306 - accuracy: 0.8966 - val_loss: 1.4506 - val_accuracy: 0.6234\n",
      "Epoch 3303/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3204 - accuracy: 0.8936 - val_loss: 1.4534 - val_accuracy: 0.6299\n",
      "Epoch 3304/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3199 - accuracy: 0.9043 - val_loss: 1.4538 - val_accuracy: 0.6201\n",
      "Epoch 3305/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3682 - accuracy: 0.8818 - val_loss: 1.4526 - val_accuracy: 0.6201\n",
      "Epoch 3306/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3528 - accuracy: 0.8869 - val_loss: 1.4568 - val_accuracy: 0.6104\n",
      "Epoch 3307/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3708 - accuracy: 0.8721 - val_loss: 1.4774 - val_accuracy: 0.6039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3308/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3115 - accuracy: 0.8841 - val_loss: 1.4935 - val_accuracy: 0.6039\n",
      "Epoch 3309/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3402 - accuracy: 0.8869 - val_loss: 1.5001 - val_accuracy: 0.5974\n",
      "Epoch 3310/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3352 - accuracy: 0.8799 - val_loss: 1.4909 - val_accuracy: 0.5942\n",
      "Epoch 3311/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3906 - accuracy: 0.8757 - val_loss: 1.4666 - val_accuracy: 0.6039\n",
      "Epoch 3312/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3545 - accuracy: 0.8841 - val_loss: 1.4229 - val_accuracy: 0.6299\n",
      "Epoch 3313/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3687 - accuracy: 0.8813 - val_loss: 1.4062 - val_accuracy: 0.6429\n",
      "Epoch 3314/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3562 - accuracy: 0.8857 - val_loss: 1.4008 - val_accuracy: 0.6364\n",
      "Epoch 3315/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3484 - accuracy: 0.8838 - val_loss: 1.4074 - val_accuracy: 0.6364\n",
      "Epoch 3316/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4069 - accuracy: 0.8799 - val_loss: 1.4078 - val_accuracy: 0.6526\n",
      "Epoch 3317/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3694 - accuracy: 0.8750 - val_loss: 1.4116 - val_accuracy: 0.6429\n",
      "Epoch 3318/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3013 - accuracy: 0.8936 - val_loss: 1.4123 - val_accuracy: 0.6364\n",
      "Epoch 3319/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3510 - accuracy: 0.8897 - val_loss: 1.4151 - val_accuracy: 0.6396\n",
      "Epoch 3320/4000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3436 - accuracy: 0.8828 - val_loss: 1.4105 - val_accuracy: 0.6429\n",
      "Epoch 3321/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3151 - accuracy: 0.8945 - val_loss: 1.4050 - val_accuracy: 0.6429\n",
      "Epoch 3322/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4240 - accuracy: 0.8643 - val_loss: 1.3887 - val_accuracy: 0.6526\n",
      "Epoch 3323/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3457 - accuracy: 0.8953 - val_loss: 1.3708 - val_accuracy: 0.6494\n",
      "Epoch 3324/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4139 - accuracy: 0.8740 - val_loss: 1.3515 - val_accuracy: 0.6494\n",
      "Epoch 3325/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3321 - accuracy: 0.8687 - val_loss: 1.3349 - val_accuracy: 0.6526\n",
      "Epoch 3326/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3907 - accuracy: 0.8673 - val_loss: 1.3267 - val_accuracy: 0.6494\n",
      "Epoch 3327/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3485 - accuracy: 0.8906 - val_loss: 1.3169 - val_accuracy: 0.6461\n",
      "Epoch 3328/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3319 - accuracy: 0.8916 - val_loss: 1.3136 - val_accuracy: 0.6396\n",
      "Epoch 3329/4000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3143 - accuracy: 0.8896 - val_loss: 1.3135 - val_accuracy: 0.6461\n",
      "Epoch 3330/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3600 - accuracy: 0.8827 - val_loss: 1.3196 - val_accuracy: 0.6494\n",
      "Epoch 3331/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3689 - accuracy: 0.8799 - val_loss: 1.3214 - val_accuracy: 0.6558\n",
      "Epoch 3332/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4428 - accuracy: 0.8687 - val_loss: 1.3365 - val_accuracy: 0.6623\n",
      "Epoch 3333/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2992 - accuracy: 0.8911 - val_loss: 1.3705 - val_accuracy: 0.6623\n",
      "Epoch 3334/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3443 - accuracy: 0.8869 - val_loss: 1.3934 - val_accuracy: 0.6721\n",
      "Epoch 3335/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3351 - accuracy: 0.8887 - val_loss: 1.4084 - val_accuracy: 0.6753\n",
      "Epoch 3336/4000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3720 - accuracy: 0.8701 - val_loss: 1.4091 - val_accuracy: 0.6688\n",
      "Epoch 3337/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3640 - accuracy: 0.8809 - val_loss: 1.4051 - val_accuracy: 0.6656\n",
      "Epoch 3338/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3579 - accuracy: 0.8911 - val_loss: 1.3963 - val_accuracy: 0.6429\n",
      "Epoch 3339/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3540 - accuracy: 0.8818 - val_loss: 1.3964 - val_accuracy: 0.6494\n",
      "Epoch 3340/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3444 - accuracy: 0.8939 - val_loss: 1.4050 - val_accuracy: 0.6494\n",
      "Epoch 3341/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3389 - accuracy: 0.8994 - val_loss: 1.4196 - val_accuracy: 0.6591\n",
      "Epoch 3342/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3443 - accuracy: 0.8887 - val_loss: 1.4439 - val_accuracy: 0.6558\n",
      "Epoch 3343/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3620 - accuracy: 0.8848 - val_loss: 1.4980 - val_accuracy: 0.6266\n",
      "Epoch 3344/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3083 - accuracy: 0.8877 - val_loss: 1.5577 - val_accuracy: 0.6006\n",
      "Epoch 3345/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3640 - accuracy: 0.8691 - val_loss: 1.6231 - val_accuracy: 0.5909\n",
      "Epoch 3346/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3792 - accuracy: 0.8813 - val_loss: 1.6493 - val_accuracy: 0.5779\n",
      "Epoch 3347/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3270 - accuracy: 0.8867 - val_loss: 1.6309 - val_accuracy: 0.5844\n",
      "Epoch 3348/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3206 - accuracy: 0.9023 - val_loss: 1.5876 - val_accuracy: 0.6006\n",
      "Epoch 3349/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3393 - accuracy: 0.8916 - val_loss: 1.5558 - val_accuracy: 0.6136\n",
      "Epoch 3350/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3801 - accuracy: 0.8743 - val_loss: 1.5292 - val_accuracy: 0.6201\n",
      "Epoch 3351/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3462 - accuracy: 0.8867 - val_loss: 1.5242 - val_accuracy: 0.6266\n",
      "Epoch 3352/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3265 - accuracy: 0.8965 - val_loss: 1.5275 - val_accuracy: 0.6201\n",
      "Epoch 3353/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3081 - accuracy: 0.9008 - val_loss: 1.5332 - val_accuracy: 0.6201\n",
      "Epoch 3354/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3312 - accuracy: 0.8953 - val_loss: 1.5367 - val_accuracy: 0.6104\n",
      "Epoch 3355/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3817 - accuracy: 0.8757 - val_loss: 1.5213 - val_accuracy: 0.6136\n",
      "Epoch 3356/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3239 - accuracy: 0.9062 - val_loss: 1.5128 - val_accuracy: 0.6071\n",
      "Epoch 3357/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3723 - accuracy: 0.8848 - val_loss: 1.5082 - val_accuracy: 0.6104\n",
      "Epoch 3358/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3350 - accuracy: 0.8980 - val_loss: 1.5179 - val_accuracy: 0.6234\n",
      "Epoch 3359/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3229 - accuracy: 0.8883 - val_loss: 1.5171 - val_accuracy: 0.6201\n",
      "Epoch 3360/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3437 - accuracy: 0.8729 - val_loss: 1.5166 - val_accuracy: 0.6169\n",
      "Epoch 3361/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3262 - accuracy: 0.8980 - val_loss: 1.5086 - val_accuracy: 0.6169\n",
      "Epoch 3362/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4185 - accuracy: 0.8715 - val_loss: 1.5294 - val_accuracy: 0.6104\n",
      "Epoch 3363/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3732 - accuracy: 0.8785 - val_loss: 1.5487 - val_accuracy: 0.6039\n",
      "Epoch 3364/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3554 - accuracy: 0.8877 - val_loss: 1.5704 - val_accuracy: 0.5877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3365/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3855 - accuracy: 0.8757 - val_loss: 1.6028 - val_accuracy: 0.5909\n",
      "Epoch 3366/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3364 - accuracy: 0.8813 - val_loss: 1.6377 - val_accuracy: 0.5812\n",
      "Epoch 3367/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3679 - accuracy: 0.8799 - val_loss: 1.6663 - val_accuracy: 0.5844\n",
      "Epoch 3368/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3745 - accuracy: 0.8906 - val_loss: 1.6905 - val_accuracy: 0.5779\n",
      "Epoch 3369/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3704 - accuracy: 0.8897 - val_loss: 1.7260 - val_accuracy: 0.5714\n",
      "Epoch 3370/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2913 - accuracy: 0.9072 - val_loss: 1.7637 - val_accuracy: 0.5584\n",
      "Epoch 3371/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3195 - accuracy: 0.8936 - val_loss: 1.7769 - val_accuracy: 0.5682\n",
      "Epoch 3372/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3350 - accuracy: 0.8906 - val_loss: 1.7921 - val_accuracy: 0.5487\n",
      "Epoch 3373/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3462 - accuracy: 0.8939 - val_loss: 1.7769 - val_accuracy: 0.5617\n",
      "Epoch 3374/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3611 - accuracy: 0.8916 - val_loss: 1.7321 - val_accuracy: 0.5844\n",
      "Epoch 3375/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3424 - accuracy: 0.9064 - val_loss: 1.6745 - val_accuracy: 0.5942\n",
      "Epoch 3376/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3510 - accuracy: 0.8883 - val_loss: 1.5667 - val_accuracy: 0.6136\n",
      "Epoch 3377/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3163 - accuracy: 0.8906 - val_loss: 1.5103 - val_accuracy: 0.6331\n",
      "Epoch 3378/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3346 - accuracy: 0.8883 - val_loss: 1.4714 - val_accuracy: 0.6461\n",
      "Epoch 3379/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2953 - accuracy: 0.8939 - val_loss: 1.4613 - val_accuracy: 0.6526\n",
      "Epoch 3380/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3744 - accuracy: 0.8809 - val_loss: 1.4667 - val_accuracy: 0.6558\n",
      "Epoch 3381/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3584 - accuracy: 0.8857 - val_loss: 1.4635 - val_accuracy: 0.6494\n",
      "Epoch 3382/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2958 - accuracy: 0.9082 - val_loss: 1.4595 - val_accuracy: 0.6461\n",
      "Epoch 3383/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3401 - accuracy: 0.8906 - val_loss: 1.4480 - val_accuracy: 0.6396\n",
      "Epoch 3384/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3267 - accuracy: 0.9036 - val_loss: 1.4357 - val_accuracy: 0.6364\n",
      "Epoch 3385/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3511 - accuracy: 0.8887 - val_loss: 1.4315 - val_accuracy: 0.6364\n",
      "Epoch 3386/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3549 - accuracy: 0.8789 - val_loss: 1.4312 - val_accuracy: 0.6364\n",
      "Epoch 3387/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3554 - accuracy: 0.8799 - val_loss: 1.4341 - val_accuracy: 0.6331\n",
      "Epoch 3388/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3496 - accuracy: 0.8785 - val_loss: 1.4535 - val_accuracy: 0.6266\n",
      "Epoch 3389/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3614 - accuracy: 0.8855 - val_loss: 1.4755 - val_accuracy: 0.6201\n",
      "Epoch 3390/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3255 - accuracy: 0.8867 - val_loss: 1.4802 - val_accuracy: 0.6169\n",
      "Epoch 3391/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3728 - accuracy: 0.8770 - val_loss: 1.4705 - val_accuracy: 0.6169\n",
      "Epoch 3392/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3784 - accuracy: 0.8785 - val_loss: 1.4436 - val_accuracy: 0.6234\n",
      "Epoch 3393/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3349 - accuracy: 0.8896 - val_loss: 1.4027 - val_accuracy: 0.6429\n",
      "Epoch 3394/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3406 - accuracy: 0.8827 - val_loss: 1.3766 - val_accuracy: 0.6429\n",
      "Epoch 3395/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3288 - accuracy: 0.8980 - val_loss: 1.3623 - val_accuracy: 0.6331\n",
      "Epoch 3396/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3119 - accuracy: 0.9004 - val_loss: 1.3577 - val_accuracy: 0.6299\n",
      "Epoch 3397/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3959 - accuracy: 0.8659 - val_loss: 1.3522 - val_accuracy: 0.6331\n",
      "Epoch 3398/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3713 - accuracy: 0.8743 - val_loss: 1.3438 - val_accuracy: 0.6396\n",
      "Epoch 3399/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3375 - accuracy: 0.8682 - val_loss: 1.3361 - val_accuracy: 0.6494\n",
      "Epoch 3400/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3097 - accuracy: 0.8925 - val_loss: 1.3371 - val_accuracy: 0.6526\n",
      "Epoch 3401/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2950 - accuracy: 0.8911 - val_loss: 1.3451 - val_accuracy: 0.6591\n",
      "Epoch 3402/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3803 - accuracy: 0.8799 - val_loss: 1.3680 - val_accuracy: 0.6526\n",
      "Epoch 3403/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3429 - accuracy: 0.8939 - val_loss: 1.3867 - val_accuracy: 0.6331\n",
      "Epoch 3404/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3336 - accuracy: 0.8925 - val_loss: 1.4079 - val_accuracy: 0.6299\n",
      "Epoch 3405/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3742 - accuracy: 0.8848 - val_loss: 1.4292 - val_accuracy: 0.6396\n",
      "Epoch 3406/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4136 - accuracy: 0.8729 - val_loss: 1.4541 - val_accuracy: 0.6331\n",
      "Epoch 3407/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3270 - accuracy: 0.8936 - val_loss: 1.4792 - val_accuracy: 0.6266\n",
      "Epoch 3408/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4008 - accuracy: 0.8662 - val_loss: 1.5142 - val_accuracy: 0.6266\n",
      "Epoch 3409/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3060 - accuracy: 0.8994 - val_loss: 1.5544 - val_accuracy: 0.6234\n",
      "Epoch 3410/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3260 - accuracy: 0.8911 - val_loss: 1.6107 - val_accuracy: 0.5909\n",
      "Epoch 3411/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3367 - accuracy: 0.8729 - val_loss: 1.6822 - val_accuracy: 0.5779\n",
      "Epoch 3412/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4086 - accuracy: 0.8813 - val_loss: 1.7352 - val_accuracy: 0.5779\n",
      "Epoch 3413/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3471 - accuracy: 0.8809 - val_loss: 1.7512 - val_accuracy: 0.5519\n",
      "Epoch 3414/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3810 - accuracy: 0.8770 - val_loss: 1.7685 - val_accuracy: 0.5617\n",
      "Epoch 3415/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3123 - accuracy: 0.9078 - val_loss: 1.7764 - val_accuracy: 0.5649\n",
      "Epoch 3416/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3197 - accuracy: 0.8869 - val_loss: 1.7683 - val_accuracy: 0.5519\n",
      "Epoch 3417/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2795 - accuracy: 0.9064 - val_loss: 1.7322 - val_accuracy: 0.5649\n",
      "Epoch 3418/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3444 - accuracy: 0.8966 - val_loss: 1.7113 - val_accuracy: 0.5747\n",
      "Epoch 3419/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3117 - accuracy: 0.8867 - val_loss: 1.6752 - val_accuracy: 0.5844\n",
      "Epoch 3420/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3670 - accuracy: 0.8980 - val_loss: 1.6213 - val_accuracy: 0.5877\n",
      "Epoch 3421/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3834 - accuracy: 0.8770 - val_loss: 1.5768 - val_accuracy: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3422/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3691 - accuracy: 0.8828 - val_loss: 1.5660 - val_accuracy: 0.6104\n",
      "Epoch 3423/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3565 - accuracy: 0.8857 - val_loss: 1.5627 - val_accuracy: 0.6104\n",
      "Epoch 3424/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3330 - accuracy: 0.8966 - val_loss: 1.5472 - val_accuracy: 0.6201\n",
      "Epoch 3425/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3269 - accuracy: 0.9033 - val_loss: 1.5432 - val_accuracy: 0.6136\n",
      "Epoch 3426/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3339 - accuracy: 0.8925 - val_loss: 1.5350 - val_accuracy: 0.6071\n",
      "Epoch 3427/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3335 - accuracy: 0.8916 - val_loss: 1.5400 - val_accuracy: 0.5974\n",
      "Epoch 3428/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3345 - accuracy: 0.8966 - val_loss: 1.5286 - val_accuracy: 0.5974\n",
      "Epoch 3429/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3650 - accuracy: 0.8848 - val_loss: 1.5195 - val_accuracy: 0.5844\n",
      "Epoch 3430/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3223 - accuracy: 0.8925 - val_loss: 1.5421 - val_accuracy: 0.5812\n",
      "Epoch 3431/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3556 - accuracy: 0.8779 - val_loss: 1.5797 - val_accuracy: 0.5909\n",
      "Epoch 3432/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3213 - accuracy: 0.9036 - val_loss: 1.6185 - val_accuracy: 0.5779\n",
      "Epoch 3433/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3111 - accuracy: 0.9022 - val_loss: 1.6407 - val_accuracy: 0.5844\n",
      "Epoch 3434/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4220 - accuracy: 0.8743 - val_loss: 1.6271 - val_accuracy: 0.5844\n",
      "Epoch 3435/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3486 - accuracy: 0.8897 - val_loss: 1.6004 - val_accuracy: 0.5877\n",
      "Epoch 3436/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3953 - accuracy: 0.8750 - val_loss: 1.5563 - val_accuracy: 0.6169\n",
      "Epoch 3437/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3536 - accuracy: 0.8897 - val_loss: 1.5207 - val_accuracy: 0.6364\n",
      "Epoch 3438/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3344 - accuracy: 0.8994 - val_loss: 1.4920 - val_accuracy: 0.6396\n",
      "Epoch 3439/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2895 - accuracy: 0.9064 - val_loss: 1.4643 - val_accuracy: 0.6364\n",
      "Epoch 3440/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3426 - accuracy: 0.8883 - val_loss: 1.4307 - val_accuracy: 0.6494\n",
      "Epoch 3441/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3716 - accuracy: 0.8673 - val_loss: 1.4193 - val_accuracy: 0.6429\n",
      "Epoch 3442/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3609 - accuracy: 0.8883 - val_loss: 1.4193 - val_accuracy: 0.6558\n",
      "Epoch 3443/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3441 - accuracy: 0.8887 - val_loss: 1.4211 - val_accuracy: 0.6591\n",
      "Epoch 3444/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3620 - accuracy: 0.8721 - val_loss: 1.4381 - val_accuracy: 0.6623\n",
      "Epoch 3445/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3553 - accuracy: 0.8813 - val_loss: 1.4641 - val_accuracy: 0.6526\n",
      "Epoch 3446/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3859 - accuracy: 0.8757 - val_loss: 1.4824 - val_accuracy: 0.6299\n",
      "Epoch 3447/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3073 - accuracy: 0.8936 - val_loss: 1.5031 - val_accuracy: 0.6299\n",
      "Epoch 3448/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3610 - accuracy: 0.8939 - val_loss: 1.5039 - val_accuracy: 0.6169\n",
      "Epoch 3449/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3201 - accuracy: 0.9004 - val_loss: 1.4981 - val_accuracy: 0.6071\n",
      "Epoch 3450/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3160 - accuracy: 0.8911 - val_loss: 1.4980 - val_accuracy: 0.6104\n",
      "Epoch 3451/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3435 - accuracy: 0.8897 - val_loss: 1.5410 - val_accuracy: 0.5812\n",
      "Epoch 3452/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3381 - accuracy: 0.8848 - val_loss: 1.5705 - val_accuracy: 0.5812\n",
      "Epoch 3453/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3618 - accuracy: 0.8855 - val_loss: 1.5915 - val_accuracy: 0.5779\n",
      "Epoch 3454/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3595 - accuracy: 0.8897 - val_loss: 1.5903 - val_accuracy: 0.5779\n",
      "Epoch 3455/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3135 - accuracy: 0.8994 - val_loss: 1.6342 - val_accuracy: 0.5682\n",
      "Epoch 3456/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3416 - accuracy: 0.8855 - val_loss: 1.6752 - val_accuracy: 0.5617\n",
      "Epoch 3457/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3411 - accuracy: 0.8916 - val_loss: 1.6922 - val_accuracy: 0.5552\n",
      "Epoch 3458/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3401 - accuracy: 0.8841 - val_loss: 1.7004 - val_accuracy: 0.5552\n",
      "Epoch 3459/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3101 - accuracy: 0.9078 - val_loss: 1.6672 - val_accuracy: 0.5747\n",
      "Epoch 3460/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3813 - accuracy: 0.8771 - val_loss: 1.6440 - val_accuracy: 0.5812\n",
      "Epoch 3461/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3325 - accuracy: 0.8926 - val_loss: 1.6178 - val_accuracy: 0.5812\n",
      "Epoch 3462/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3379 - accuracy: 0.9023 - val_loss: 1.5906 - val_accuracy: 0.5779\n",
      "Epoch 3463/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3018 - accuracy: 0.8966 - val_loss: 1.5723 - val_accuracy: 0.5812\n",
      "Epoch 3464/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3558 - accuracy: 0.8916 - val_loss: 1.5355 - val_accuracy: 0.5877\n",
      "Epoch 3465/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3253 - accuracy: 0.8867 - val_loss: 1.4949 - val_accuracy: 0.6006\n",
      "Epoch 3466/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3111 - accuracy: 0.9008 - val_loss: 1.4847 - val_accuracy: 0.5942\n",
      "Epoch 3467/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3717 - accuracy: 0.8721 - val_loss: 1.4608 - val_accuracy: 0.6006\n",
      "Epoch 3468/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2933 - accuracy: 0.9062 - val_loss: 1.4485 - val_accuracy: 0.6136\n",
      "Epoch 3469/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3311 - accuracy: 0.8897 - val_loss: 1.4333 - val_accuracy: 0.6136\n",
      "Epoch 3470/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3094 - accuracy: 0.8925 - val_loss: 1.4109 - val_accuracy: 0.6234\n",
      "Epoch 3471/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3076 - accuracy: 0.9008 - val_loss: 1.3886 - val_accuracy: 0.6266\n",
      "Epoch 3472/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2991 - accuracy: 0.8955 - val_loss: 1.3684 - val_accuracy: 0.6266\n",
      "Epoch 3473/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3739 - accuracy: 0.8672 - val_loss: 1.3493 - val_accuracy: 0.6396\n",
      "Epoch 3474/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3673 - accuracy: 0.8828 - val_loss: 1.3416 - val_accuracy: 0.6461\n",
      "Epoch 3475/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3265 - accuracy: 0.8869 - val_loss: 1.3315 - val_accuracy: 0.6526\n",
      "Epoch 3476/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2793 - accuracy: 0.9189 - val_loss: 1.3275 - val_accuracy: 0.6558\n",
      "Epoch 3477/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3383 - accuracy: 0.8877 - val_loss: 1.3229 - val_accuracy: 0.6526\n",
      "Epoch 3478/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2928 - accuracy: 0.8966 - val_loss: 1.3250 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3479/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3267 - accuracy: 0.8936 - val_loss: 1.3348 - val_accuracy: 0.6429\n",
      "Epoch 3480/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3459 - accuracy: 0.8779 - val_loss: 1.3609 - val_accuracy: 0.6396\n",
      "Epoch 3481/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3418 - accuracy: 0.8906 - val_loss: 1.3891 - val_accuracy: 0.6299\n",
      "Epoch 3482/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3428 - accuracy: 0.8916 - val_loss: 1.4194 - val_accuracy: 0.6266\n",
      "Epoch 3483/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3922 - accuracy: 0.8687 - val_loss: 1.4488 - val_accuracy: 0.6266\n",
      "Epoch 3484/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3407 - accuracy: 0.8867 - val_loss: 1.4849 - val_accuracy: 0.6169\n",
      "Epoch 3485/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3162 - accuracy: 0.8911 - val_loss: 1.5150 - val_accuracy: 0.6104\n",
      "Epoch 3486/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3097 - accuracy: 0.9008 - val_loss: 1.5253 - val_accuracy: 0.6071\n",
      "Epoch 3487/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3503 - accuracy: 0.8771 - val_loss: 1.4947 - val_accuracy: 0.6071\n",
      "Epoch 3488/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2901 - accuracy: 0.9148 - val_loss: 1.4681 - val_accuracy: 0.6234\n",
      "Epoch 3489/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3297 - accuracy: 0.8926 - val_loss: 1.4550 - val_accuracy: 0.6104\n",
      "Epoch 3490/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3718 - accuracy: 0.8828 - val_loss: 1.4531 - val_accuracy: 0.6104\n",
      "Epoch 3491/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3208 - accuracy: 0.8877 - val_loss: 1.4569 - val_accuracy: 0.6104\n",
      "Epoch 3492/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3263 - accuracy: 0.8906 - val_loss: 1.4531 - val_accuracy: 0.6136\n",
      "Epoch 3493/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3579 - accuracy: 0.8828 - val_loss: 1.4361 - val_accuracy: 0.6136\n",
      "Epoch 3494/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2910 - accuracy: 0.9008 - val_loss: 1.4161 - val_accuracy: 0.6201\n",
      "Epoch 3495/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3372 - accuracy: 0.8994 - val_loss: 1.4075 - val_accuracy: 0.6331\n",
      "Epoch 3496/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3523 - accuracy: 0.8897 - val_loss: 1.4082 - val_accuracy: 0.6299\n",
      "Epoch 3497/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3528 - accuracy: 0.8818 - val_loss: 1.4059 - val_accuracy: 0.6331\n",
      "Epoch 3498/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3263 - accuracy: 0.8953 - val_loss: 1.3995 - val_accuracy: 0.6299\n",
      "Epoch 3499/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3099 - accuracy: 0.8896 - val_loss: 1.4027 - val_accuracy: 0.6299\n",
      "Epoch 3500/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3525 - accuracy: 0.8799 - val_loss: 1.4176 - val_accuracy: 0.6266\n",
      "Epoch 3501/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3224 - accuracy: 0.8869 - val_loss: 1.4401 - val_accuracy: 0.6299\n",
      "Epoch 3502/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2984 - accuracy: 0.9050 - val_loss: 1.4742 - val_accuracy: 0.6169\n",
      "Epoch 3503/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3323 - accuracy: 0.8926 - val_loss: 1.4917 - val_accuracy: 0.5974\n",
      "Epoch 3504/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3268 - accuracy: 0.8945 - val_loss: 1.5049 - val_accuracy: 0.5974\n",
      "Epoch 3505/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3094 - accuracy: 0.8813 - val_loss: 1.5097 - val_accuracy: 0.5909\n",
      "Epoch 3506/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2889 - accuracy: 0.8939 - val_loss: 1.5387 - val_accuracy: 0.5942\n",
      "Epoch 3507/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3524 - accuracy: 0.8887 - val_loss: 1.5374 - val_accuracy: 0.5974\n",
      "Epoch 3508/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3421 - accuracy: 0.8925 - val_loss: 1.5754 - val_accuracy: 0.5779\n",
      "Epoch 3509/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3860 - accuracy: 0.8779 - val_loss: 1.6057 - val_accuracy: 0.5682\n",
      "Epoch 3510/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3570 - accuracy: 0.8809 - val_loss: 1.6308 - val_accuracy: 0.5779\n",
      "Epoch 3511/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3439 - accuracy: 0.8887 - val_loss: 1.6360 - val_accuracy: 0.5844\n",
      "Epoch 3512/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3356 - accuracy: 0.8994 - val_loss: 1.6604 - val_accuracy: 0.5779\n",
      "Epoch 3513/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2984 - accuracy: 0.9050 - val_loss: 1.6746 - val_accuracy: 0.5779\n",
      "Epoch 3514/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3406 - accuracy: 0.8883 - val_loss: 1.6540 - val_accuracy: 0.5909\n",
      "Epoch 3515/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3915 - accuracy: 0.8701 - val_loss: 1.6169 - val_accuracy: 0.6006\n",
      "Epoch 3516/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3138 - accuracy: 0.8911 - val_loss: 1.5823 - val_accuracy: 0.6039\n",
      "Epoch 3517/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2825 - accuracy: 0.9050 - val_loss: 1.5330 - val_accuracy: 0.6104\n",
      "Epoch 3518/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3062 - accuracy: 0.9023 - val_loss: 1.4800 - val_accuracy: 0.6396\n",
      "Epoch 3519/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3369 - accuracy: 0.8945 - val_loss: 1.4493 - val_accuracy: 0.6396\n",
      "Epoch 3520/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3626 - accuracy: 0.8896 - val_loss: 1.4183 - val_accuracy: 0.6461\n",
      "Epoch 3521/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3305 - accuracy: 0.8925 - val_loss: 1.3889 - val_accuracy: 0.6461\n",
      "Epoch 3522/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3033 - accuracy: 0.9022 - val_loss: 1.3747 - val_accuracy: 0.6494\n",
      "Epoch 3523/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3122 - accuracy: 0.8966 - val_loss: 1.3676 - val_accuracy: 0.6591\n",
      "Epoch 3524/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3170 - accuracy: 0.8953 - val_loss: 1.3748 - val_accuracy: 0.6558\n",
      "Epoch 3525/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3303 - accuracy: 0.8945 - val_loss: 1.4013 - val_accuracy: 0.6429\n",
      "Epoch 3526/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3279 - accuracy: 0.8953 - val_loss: 1.4364 - val_accuracy: 0.6396\n",
      "Epoch 3527/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3178 - accuracy: 0.8887 - val_loss: 1.4499 - val_accuracy: 0.6364\n",
      "Epoch 3528/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3331 - accuracy: 0.8953 - val_loss: 1.4718 - val_accuracy: 0.6396\n",
      "Epoch 3529/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3349 - accuracy: 0.8818 - val_loss: 1.4695 - val_accuracy: 0.6494\n",
      "Epoch 3530/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3146 - accuracy: 0.8818 - val_loss: 1.4666 - val_accuracy: 0.6364\n",
      "Epoch 3531/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3009 - accuracy: 0.9092 - val_loss: 1.4586 - val_accuracy: 0.6331\n",
      "Epoch 3532/4000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3107 - accuracy: 0.8883 - val_loss: 1.4590 - val_accuracy: 0.6331\n",
      "Epoch 3533/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3494 - accuracy: 0.8911 - val_loss: 1.4613 - val_accuracy: 0.6396\n",
      "Epoch 3534/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3715 - accuracy: 0.8869 - val_loss: 1.4661 - val_accuracy: 0.6331\n",
      "Epoch 3535/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3184 - accuracy: 0.8887 - val_loss: 1.4629 - val_accuracy: 0.6331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3536/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3390 - accuracy: 0.8906 - val_loss: 1.4586 - val_accuracy: 0.6299\n",
      "Epoch 3537/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2816 - accuracy: 0.9072 - val_loss: 1.4419 - val_accuracy: 0.6396\n",
      "Epoch 3538/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3169 - accuracy: 0.8906 - val_loss: 1.4334 - val_accuracy: 0.6364\n",
      "Epoch 3539/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3382 - accuracy: 0.8848 - val_loss: 1.4197 - val_accuracy: 0.6429\n",
      "Epoch 3540/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3168 - accuracy: 0.9014 - val_loss: 1.4154 - val_accuracy: 0.6429\n",
      "Epoch 3541/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3379 - accuracy: 0.8939 - val_loss: 1.3926 - val_accuracy: 0.6494\n",
      "Epoch 3542/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3647 - accuracy: 0.8818 - val_loss: 1.3957 - val_accuracy: 0.6429\n",
      "Epoch 3543/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3336 - accuracy: 0.8760 - val_loss: 1.3953 - val_accuracy: 0.6461\n",
      "Epoch 3544/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3061 - accuracy: 0.8945 - val_loss: 1.4023 - val_accuracy: 0.6429\n",
      "Epoch 3545/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3592 - accuracy: 0.8779 - val_loss: 1.4108 - val_accuracy: 0.6429\n",
      "Epoch 3546/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3626 - accuracy: 0.8916 - val_loss: 1.4173 - val_accuracy: 0.6331\n",
      "Epoch 3547/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2872 - accuracy: 0.9162 - val_loss: 1.4296 - val_accuracy: 0.6429\n",
      "Epoch 3548/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3339 - accuracy: 0.8926 - val_loss: 1.4339 - val_accuracy: 0.6396\n",
      "Epoch 3549/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3665 - accuracy: 0.8911 - val_loss: 1.4314 - val_accuracy: 0.6461\n",
      "Epoch 3550/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3400 - accuracy: 0.8857 - val_loss: 1.4386 - val_accuracy: 0.6494\n",
      "Epoch 3551/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3241 - accuracy: 0.8841 - val_loss: 1.4431 - val_accuracy: 0.6656\n",
      "Epoch 3552/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3215 - accuracy: 0.8953 - val_loss: 1.4561 - val_accuracy: 0.6591\n",
      "Epoch 3553/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3414 - accuracy: 0.8867 - val_loss: 1.4642 - val_accuracy: 0.6558\n",
      "Epoch 3554/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2655 - accuracy: 0.9150 - val_loss: 1.4683 - val_accuracy: 0.6558\n",
      "Epoch 3555/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2963 - accuracy: 0.9064 - val_loss: 1.4661 - val_accuracy: 0.6656\n",
      "Epoch 3556/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3277 - accuracy: 0.8975 - val_loss: 1.4561 - val_accuracy: 0.6656\n",
      "Epoch 3557/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3509 - accuracy: 0.8838 - val_loss: 1.4433 - val_accuracy: 0.6721\n",
      "Epoch 3558/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2795 - accuracy: 0.9036 - val_loss: 1.4199 - val_accuracy: 0.6721\n",
      "Epoch 3559/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3394 - accuracy: 0.8828 - val_loss: 1.4109 - val_accuracy: 0.6753\n",
      "Epoch 3560/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3284 - accuracy: 0.8975 - val_loss: 1.4037 - val_accuracy: 0.6753\n",
      "Epoch 3561/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3232 - accuracy: 0.9036 - val_loss: 1.4032 - val_accuracy: 0.6786\n",
      "Epoch 3562/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3029 - accuracy: 0.8994 - val_loss: 1.4142 - val_accuracy: 0.6656\n",
      "Epoch 3563/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3002 - accuracy: 0.8945 - val_loss: 1.4215 - val_accuracy: 0.6753\n",
      "Epoch 3564/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3112 - accuracy: 0.8925 - val_loss: 1.4241 - val_accuracy: 0.6656\n",
      "Epoch 3565/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3334 - accuracy: 0.8966 - val_loss: 1.4214 - val_accuracy: 0.6656\n",
      "Epoch 3566/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2802 - accuracy: 0.8984 - val_loss: 1.4290 - val_accuracy: 0.6591\n",
      "Epoch 3567/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3042 - accuracy: 0.9082 - val_loss: 1.4226 - val_accuracy: 0.6526\n",
      "Epoch 3568/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3161 - accuracy: 0.8936 - val_loss: 1.4341 - val_accuracy: 0.6526\n",
      "Epoch 3569/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3235 - accuracy: 0.8906 - val_loss: 1.4306 - val_accuracy: 0.6461\n",
      "Epoch 3570/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2844 - accuracy: 0.8953 - val_loss: 1.4366 - val_accuracy: 0.6558\n",
      "Epoch 3571/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3133 - accuracy: 0.9036 - val_loss: 1.4390 - val_accuracy: 0.6526\n",
      "Epoch 3572/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3287 - accuracy: 0.8994 - val_loss: 1.4481 - val_accuracy: 0.6526\n",
      "Epoch 3573/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3332 - accuracy: 0.8818 - val_loss: 1.4754 - val_accuracy: 0.6429\n",
      "Epoch 3574/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3637 - accuracy: 0.8799 - val_loss: 1.4984 - val_accuracy: 0.6429\n",
      "Epoch 3575/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2931 - accuracy: 0.9033 - val_loss: 1.5235 - val_accuracy: 0.6396\n",
      "Epoch 3576/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3015 - accuracy: 0.8984 - val_loss: 1.5387 - val_accuracy: 0.6494\n",
      "Epoch 3577/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3840 - accuracy: 0.8760 - val_loss: 1.5588 - val_accuracy: 0.6429\n",
      "Epoch 3578/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3443 - accuracy: 0.8897 - val_loss: 1.5773 - val_accuracy: 0.6169\n",
      "Epoch 3579/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3360 - accuracy: 0.8887 - val_loss: 1.5941 - val_accuracy: 0.6006\n",
      "Epoch 3580/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3382 - accuracy: 0.8887 - val_loss: 1.5942 - val_accuracy: 0.6104\n",
      "Epoch 3581/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3280 - accuracy: 0.8916 - val_loss: 1.5679 - val_accuracy: 0.6136\n",
      "Epoch 3582/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3302 - accuracy: 0.8926 - val_loss: 1.5412 - val_accuracy: 0.6169\n",
      "Epoch 3583/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3391 - accuracy: 0.8897 - val_loss: 1.5189 - val_accuracy: 0.6234\n",
      "Epoch 3584/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2992 - accuracy: 0.9092 - val_loss: 1.4965 - val_accuracy: 0.6429\n",
      "Epoch 3585/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2696 - accuracy: 0.9023 - val_loss: 1.4751 - val_accuracy: 0.6429\n",
      "Epoch 3586/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3058 - accuracy: 0.9033 - val_loss: 1.4637 - val_accuracy: 0.6461\n",
      "Epoch 3587/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3072 - accuracy: 0.9023 - val_loss: 1.4585 - val_accuracy: 0.6494\n",
      "Epoch 3588/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2970 - accuracy: 0.9008 - val_loss: 1.4619 - val_accuracy: 0.6494\n",
      "Epoch 3589/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2934 - accuracy: 0.8906 - val_loss: 1.4797 - val_accuracy: 0.6429\n",
      "Epoch 3590/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3528 - accuracy: 0.8813 - val_loss: 1.4945 - val_accuracy: 0.6331\n",
      "Epoch 3591/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2866 - accuracy: 0.9131 - val_loss: 1.5109 - val_accuracy: 0.6266\n",
      "Epoch 3592/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2975 - accuracy: 0.8939 - val_loss: 1.5334 - val_accuracy: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3593/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3327 - accuracy: 0.8936 - val_loss: 1.5527 - val_accuracy: 0.6331\n",
      "Epoch 3594/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3352 - accuracy: 0.9008 - val_loss: 1.5803 - val_accuracy: 0.6169\n",
      "Epoch 3595/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3346 - accuracy: 0.8980 - val_loss: 1.5980 - val_accuracy: 0.6201\n",
      "Epoch 3596/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3490 - accuracy: 0.8789 - val_loss: 1.6323 - val_accuracy: 0.5909\n",
      "Epoch 3597/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3341 - accuracy: 0.8883 - val_loss: 1.6789 - val_accuracy: 0.5877\n",
      "Epoch 3598/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2777 - accuracy: 0.8966 - val_loss: 1.7347 - val_accuracy: 0.5812\n",
      "Epoch 3599/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3171 - accuracy: 0.8984 - val_loss: 1.7703 - val_accuracy: 0.5812\n",
      "Epoch 3600/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3579 - accuracy: 0.8848 - val_loss: 1.7885 - val_accuracy: 0.5617\n",
      "Epoch 3601/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2980 - accuracy: 0.9008 - val_loss: 1.8007 - val_accuracy: 0.5714\n",
      "Epoch 3602/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3540 - accuracy: 0.8841 - val_loss: 1.7841 - val_accuracy: 0.5649\n",
      "Epoch 3603/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2991 - accuracy: 0.9064 - val_loss: 1.7466 - val_accuracy: 0.5682\n",
      "Epoch 3604/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3292 - accuracy: 0.8841 - val_loss: 1.7058 - val_accuracy: 0.5747\n",
      "Epoch 3605/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3295 - accuracy: 0.8911 - val_loss: 1.6886 - val_accuracy: 0.5779\n",
      "Epoch 3606/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3870 - accuracy: 0.8855 - val_loss: 1.6743 - val_accuracy: 0.5877\n",
      "Epoch 3607/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3510 - accuracy: 0.8729 - val_loss: 1.6634 - val_accuracy: 0.5844\n",
      "Epoch 3608/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2933 - accuracy: 0.9120 - val_loss: 1.6531 - val_accuracy: 0.5942\n",
      "Epoch 3609/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3324 - accuracy: 0.8883 - val_loss: 1.6451 - val_accuracy: 0.5909\n",
      "Epoch 3610/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3465 - accuracy: 0.8887 - val_loss: 1.6317 - val_accuracy: 0.6006\n",
      "Epoch 3611/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3160 - accuracy: 0.8799 - val_loss: 1.5967 - val_accuracy: 0.6039\n",
      "Epoch 3612/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2948 - accuracy: 0.9014 - val_loss: 1.5664 - val_accuracy: 0.6104\n",
      "Epoch 3613/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3125 - accuracy: 0.8813 - val_loss: 1.5455 - val_accuracy: 0.6169\n",
      "Epoch 3614/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2675 - accuracy: 0.9078 - val_loss: 1.5519 - val_accuracy: 0.6104\n",
      "Epoch 3615/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3570 - accuracy: 0.8818 - val_loss: 1.5698 - val_accuracy: 0.6039\n",
      "Epoch 3616/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2726 - accuracy: 0.9120 - val_loss: 1.5699 - val_accuracy: 0.6006\n",
      "Epoch 3617/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2939 - accuracy: 0.9022 - val_loss: 1.5993 - val_accuracy: 0.5877\n",
      "Epoch 3618/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3463 - accuracy: 0.8953 - val_loss: 1.6139 - val_accuracy: 0.5942\n",
      "Epoch 3619/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2966 - accuracy: 0.9064 - val_loss: 1.6130 - val_accuracy: 0.5942\n",
      "Epoch 3620/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3444 - accuracy: 0.8896 - val_loss: 1.6059 - val_accuracy: 0.6006\n",
      "Epoch 3621/4000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3278 - accuracy: 0.8897 - val_loss: 1.5910 - val_accuracy: 0.6039\n",
      "Epoch 3622/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2605 - accuracy: 0.9246 - val_loss: 1.5772 - val_accuracy: 0.6039\n",
      "Epoch 3623/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3564 - accuracy: 0.8828 - val_loss: 1.5615 - val_accuracy: 0.6136\n",
      "Epoch 3624/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2851 - accuracy: 0.9078 - val_loss: 1.5528 - val_accuracy: 0.6169\n",
      "Epoch 3625/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3185 - accuracy: 0.8953 - val_loss: 1.5501 - val_accuracy: 0.6234\n",
      "Epoch 3626/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3373 - accuracy: 0.8926 - val_loss: 1.5508 - val_accuracy: 0.6299\n",
      "Epoch 3627/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3023 - accuracy: 0.8966 - val_loss: 1.5778 - val_accuracy: 0.6266\n",
      "Epoch 3628/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3399 - accuracy: 0.8906 - val_loss: 1.6094 - val_accuracy: 0.6006\n",
      "Epoch 3629/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3421 - accuracy: 0.8838 - val_loss: 1.6339 - val_accuracy: 0.5909\n",
      "Epoch 3630/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3546 - accuracy: 0.8848 - val_loss: 1.6779 - val_accuracy: 0.5844\n",
      "Epoch 3631/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3222 - accuracy: 0.9072 - val_loss: 1.7333 - val_accuracy: 0.5812\n",
      "Epoch 3632/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3092 - accuracy: 0.9050 - val_loss: 1.7952 - val_accuracy: 0.5649\n",
      "Epoch 3633/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3122 - accuracy: 0.8906 - val_loss: 1.8412 - val_accuracy: 0.5649\n",
      "Epoch 3634/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2993 - accuracy: 0.9082 - val_loss: 1.8702 - val_accuracy: 0.5519\n",
      "Epoch 3635/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3036 - accuracy: 0.9014 - val_loss: 1.8943 - val_accuracy: 0.5390\n",
      "Epoch 3636/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2568 - accuracy: 0.9120 - val_loss: 1.8924 - val_accuracy: 0.5455\n",
      "Epoch 3637/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3558 - accuracy: 0.8867 - val_loss: 1.8576 - val_accuracy: 0.5584\n",
      "Epoch 3638/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3620 - accuracy: 0.8827 - val_loss: 1.8183 - val_accuracy: 0.5617\n",
      "Epoch 3639/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3093 - accuracy: 0.9008 - val_loss: 1.7651 - val_accuracy: 0.5747\n",
      "Epoch 3640/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3411 - accuracy: 0.8887 - val_loss: 1.7066 - val_accuracy: 0.5877\n",
      "Epoch 3641/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3465 - accuracy: 0.8955 - val_loss: 1.6954 - val_accuracy: 0.5974\n",
      "Epoch 3642/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3248 - accuracy: 0.8911 - val_loss: 1.6845 - val_accuracy: 0.5974\n",
      "Epoch 3643/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2751 - accuracy: 0.9106 - val_loss: 1.6589 - val_accuracy: 0.6071\n",
      "Epoch 3644/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3024 - accuracy: 0.9120 - val_loss: 1.6477 - val_accuracy: 0.5942\n",
      "Epoch 3645/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3241 - accuracy: 0.8965 - val_loss: 1.6289 - val_accuracy: 0.6006\n",
      "Epoch 3646/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3312 - accuracy: 0.9004 - val_loss: 1.6106 - val_accuracy: 0.6071\n",
      "Epoch 3647/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3405 - accuracy: 0.8975 - val_loss: 1.6130 - val_accuracy: 0.6104\n",
      "Epoch 3648/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2971 - accuracy: 0.8925 - val_loss: 1.6107 - val_accuracy: 0.6071\n",
      "Epoch 3649/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3208 - accuracy: 0.9036 - val_loss: 1.6396 - val_accuracy: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3650/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3662 - accuracy: 0.8740 - val_loss: 1.6838 - val_accuracy: 0.5844\n",
      "Epoch 3651/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3461 - accuracy: 0.8906 - val_loss: 1.7256 - val_accuracy: 0.5747\n",
      "Epoch 3652/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3273 - accuracy: 0.8965 - val_loss: 1.7552 - val_accuracy: 0.5617\n",
      "Epoch 3653/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3054 - accuracy: 0.8925 - val_loss: 1.7665 - val_accuracy: 0.5682\n",
      "Epoch 3654/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3082 - accuracy: 0.9008 - val_loss: 1.7403 - val_accuracy: 0.5747\n",
      "Epoch 3655/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2814 - accuracy: 0.9082 - val_loss: 1.6927 - val_accuracy: 0.5974\n",
      "Epoch 3656/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3135 - accuracy: 0.8955 - val_loss: 1.6219 - val_accuracy: 0.6071\n",
      "Epoch 3657/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3634 - accuracy: 0.8855 - val_loss: 1.5823 - val_accuracy: 0.6136\n",
      "Epoch 3658/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2737 - accuracy: 0.9111 - val_loss: 1.5522 - val_accuracy: 0.6234\n",
      "Epoch 3659/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3542 - accuracy: 0.8897 - val_loss: 1.5449 - val_accuracy: 0.6396\n",
      "Epoch 3660/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3467 - accuracy: 0.8848 - val_loss: 1.5516 - val_accuracy: 0.6331\n",
      "Epoch 3661/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3009 - accuracy: 0.9036 - val_loss: 1.5684 - val_accuracy: 0.6331\n",
      "Epoch 3662/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3112 - accuracy: 0.9008 - val_loss: 1.5737 - val_accuracy: 0.6396\n",
      "Epoch 3663/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3117 - accuracy: 0.9062 - val_loss: 1.5841 - val_accuracy: 0.6396\n",
      "Epoch 3664/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3087 - accuracy: 0.8994 - val_loss: 1.5786 - val_accuracy: 0.6364\n",
      "Epoch 3665/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3061 - accuracy: 0.8994 - val_loss: 1.5503 - val_accuracy: 0.6429\n",
      "Epoch 3666/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3366 - accuracy: 0.8869 - val_loss: 1.5357 - val_accuracy: 0.6266\n",
      "Epoch 3667/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2998 - accuracy: 0.9023 - val_loss: 1.5205 - val_accuracy: 0.6266\n",
      "Epoch 3668/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3431 - accuracy: 0.8770 - val_loss: 1.4996 - val_accuracy: 0.6201\n",
      "Epoch 3669/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3217 - accuracy: 0.8897 - val_loss: 1.4749 - val_accuracy: 0.6266\n",
      "Epoch 3670/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3278 - accuracy: 0.9014 - val_loss: 1.4575 - val_accuracy: 0.6461\n",
      "Epoch 3671/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2847 - accuracy: 0.9141 - val_loss: 1.4602 - val_accuracy: 0.6558\n",
      "Epoch 3672/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3372 - accuracy: 0.8955 - val_loss: 1.4675 - val_accuracy: 0.6494\n",
      "Epoch 3673/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3009 - accuracy: 0.9004 - val_loss: 1.4898 - val_accuracy: 0.6429\n",
      "Epoch 3674/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3148 - accuracy: 0.8906 - val_loss: 1.5105 - val_accuracy: 0.6266\n",
      "Epoch 3675/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3810 - accuracy: 0.8827 - val_loss: 1.5722 - val_accuracy: 0.6234\n",
      "Epoch 3676/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3034 - accuracy: 0.9062 - val_loss: 1.6004 - val_accuracy: 0.6169\n",
      "Epoch 3677/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2916 - accuracy: 0.8984 - val_loss: 1.6376 - val_accuracy: 0.5974\n",
      "Epoch 3678/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2912 - accuracy: 0.9023 - val_loss: 1.6557 - val_accuracy: 0.5909\n",
      "Epoch 3679/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3167 - accuracy: 0.9036 - val_loss: 1.6638 - val_accuracy: 0.5779\n",
      "Epoch 3680/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3265 - accuracy: 0.8994 - val_loss: 1.6844 - val_accuracy: 0.5747\n",
      "Epoch 3681/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3581 - accuracy: 0.8789 - val_loss: 1.6740 - val_accuracy: 0.5812\n",
      "Epoch 3682/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2894 - accuracy: 0.9131 - val_loss: 1.6314 - val_accuracy: 0.5942\n",
      "Epoch 3683/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3444 - accuracy: 0.8818 - val_loss: 1.5567 - val_accuracy: 0.6169\n",
      "Epoch 3684/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3158 - accuracy: 0.8984 - val_loss: 1.5144 - val_accuracy: 0.6136\n",
      "Epoch 3685/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2889 - accuracy: 0.8994 - val_loss: 1.4903 - val_accuracy: 0.6234\n",
      "Epoch 3686/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3211 - accuracy: 0.9078 - val_loss: 1.4782 - val_accuracy: 0.6331\n",
      "Epoch 3687/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3146 - accuracy: 0.8955 - val_loss: 1.4777 - val_accuracy: 0.6331\n",
      "Epoch 3688/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2995 - accuracy: 0.9050 - val_loss: 1.4666 - val_accuracy: 0.6494\n",
      "Epoch 3689/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3050 - accuracy: 0.9022 - val_loss: 1.4656 - val_accuracy: 0.6558\n",
      "Epoch 3690/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3317 - accuracy: 0.8945 - val_loss: 1.4739 - val_accuracy: 0.6526\n",
      "Epoch 3691/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3522 - accuracy: 0.8857 - val_loss: 1.4994 - val_accuracy: 0.6429\n",
      "Epoch 3692/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2644 - accuracy: 0.9092 - val_loss: 1.5301 - val_accuracy: 0.6331\n",
      "Epoch 3693/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2991 - accuracy: 0.9121 - val_loss: 1.5388 - val_accuracy: 0.6201\n",
      "Epoch 3694/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2660 - accuracy: 0.9092 - val_loss: 1.5431 - val_accuracy: 0.6201\n",
      "Epoch 3695/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2777 - accuracy: 0.9150 - val_loss: 1.5499 - val_accuracy: 0.6201\n",
      "Epoch 3696/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3340 - accuracy: 0.8867 - val_loss: 1.5494 - val_accuracy: 0.6071\n",
      "Epoch 3697/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3482 - accuracy: 0.9022 - val_loss: 1.5260 - val_accuracy: 0.6169\n",
      "Epoch 3698/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2652 - accuracy: 0.9148 - val_loss: 1.5031 - val_accuracy: 0.6136\n",
      "Epoch 3699/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3127 - accuracy: 0.8965 - val_loss: 1.5130 - val_accuracy: 0.6266\n",
      "Epoch 3700/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3812 - accuracy: 0.8757 - val_loss: 1.5180 - val_accuracy: 0.6169\n",
      "Epoch 3701/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2901 - accuracy: 0.9078 - val_loss: 1.5139 - val_accuracy: 0.6234\n",
      "Epoch 3702/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3721 - accuracy: 0.8855 - val_loss: 1.5279 - val_accuracy: 0.6299\n",
      "Epoch 3703/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3153 - accuracy: 0.8994 - val_loss: 1.5518 - val_accuracy: 0.6169\n",
      "Epoch 3704/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3102 - accuracy: 0.9064 - val_loss: 1.5910 - val_accuracy: 0.5974\n",
      "Epoch 3705/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3243 - accuracy: 0.8906 - val_loss: 1.6392 - val_accuracy: 0.5877\n",
      "Epoch 3706/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3158 - accuracy: 0.8911 - val_loss: 1.6995 - val_accuracy: 0.5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3707/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3371 - accuracy: 0.8936 - val_loss: 1.7426 - val_accuracy: 0.5682\n",
      "Epoch 3708/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3534 - accuracy: 0.8799 - val_loss: 1.7594 - val_accuracy: 0.5747\n",
      "Epoch 3709/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2720 - accuracy: 0.9180 - val_loss: 1.7353 - val_accuracy: 0.5779\n",
      "Epoch 3710/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3410 - accuracy: 0.9036 - val_loss: 1.6834 - val_accuracy: 0.5877\n",
      "Epoch 3711/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2432 - accuracy: 0.9189 - val_loss: 1.6406 - val_accuracy: 0.5877\n",
      "Epoch 3712/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3269 - accuracy: 0.9053 - val_loss: 1.6154 - val_accuracy: 0.5877\n",
      "Epoch 3713/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3531 - accuracy: 0.8827 - val_loss: 1.6001 - val_accuracy: 0.5942\n",
      "Epoch 3714/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3074 - accuracy: 0.9014 - val_loss: 1.5794 - val_accuracy: 0.5974\n",
      "Epoch 3715/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3740 - accuracy: 0.8740 - val_loss: 1.5717 - val_accuracy: 0.6039\n",
      "Epoch 3716/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3172 - accuracy: 0.8945 - val_loss: 1.5636 - val_accuracy: 0.6104\n",
      "Epoch 3717/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3402 - accuracy: 0.8848 - val_loss: 1.5769 - val_accuracy: 0.6234\n",
      "Epoch 3718/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2559 - accuracy: 0.9162 - val_loss: 1.5994 - val_accuracy: 0.6039\n",
      "Epoch 3719/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3179 - accuracy: 0.8925 - val_loss: 1.6022 - val_accuracy: 0.6006\n",
      "Epoch 3720/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3787 - accuracy: 0.8799 - val_loss: 1.5821 - val_accuracy: 0.6104\n",
      "Epoch 3721/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3079 - accuracy: 0.9050 - val_loss: 1.5611 - val_accuracy: 0.6234\n",
      "Epoch 3722/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3253 - accuracy: 0.8953 - val_loss: 1.5481 - val_accuracy: 0.6299\n",
      "Epoch 3723/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3394 - accuracy: 0.8936 - val_loss: 1.5289 - val_accuracy: 0.6331\n",
      "Epoch 3724/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2854 - accuracy: 0.9008 - val_loss: 1.5148 - val_accuracy: 0.6266\n",
      "Epoch 3725/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3009 - accuracy: 0.9050 - val_loss: 1.4979 - val_accuracy: 0.6234\n",
      "Epoch 3726/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3340 - accuracy: 0.8945 - val_loss: 1.4964 - val_accuracy: 0.6169\n",
      "Epoch 3727/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3244 - accuracy: 0.8994 - val_loss: 1.5088 - val_accuracy: 0.6201\n",
      "Epoch 3728/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3132 - accuracy: 0.9022 - val_loss: 1.5173 - val_accuracy: 0.6039\n",
      "Epoch 3729/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2591 - accuracy: 0.9160 - val_loss: 1.5412 - val_accuracy: 0.6071\n",
      "Epoch 3730/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3322 - accuracy: 0.8838 - val_loss: 1.5675 - val_accuracy: 0.6071\n",
      "Epoch 3731/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3140 - accuracy: 0.8994 - val_loss: 1.5822 - val_accuracy: 0.6006\n",
      "Epoch 3732/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3310 - accuracy: 0.8965 - val_loss: 1.5844 - val_accuracy: 0.6006\n",
      "Epoch 3733/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3212 - accuracy: 0.8994 - val_loss: 1.5843 - val_accuracy: 0.6136\n",
      "Epoch 3734/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2940 - accuracy: 0.9078 - val_loss: 1.5793 - val_accuracy: 0.6201\n",
      "Epoch 3735/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2915 - accuracy: 0.9043 - val_loss: 1.5628 - val_accuracy: 0.6169\n",
      "Epoch 3736/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3168 - accuracy: 0.8936 - val_loss: 1.5453 - val_accuracy: 0.6006\n",
      "Epoch 3737/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3549 - accuracy: 0.8841 - val_loss: 1.5306 - val_accuracy: 0.6071\n",
      "Epoch 3738/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3134 - accuracy: 0.8966 - val_loss: 1.5180 - val_accuracy: 0.6136\n",
      "Epoch 3739/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2505 - accuracy: 0.9092 - val_loss: 1.5029 - val_accuracy: 0.6136\n",
      "Epoch 3740/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3697 - accuracy: 0.8828 - val_loss: 1.4859 - val_accuracy: 0.6234\n",
      "Epoch 3741/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3074 - accuracy: 0.9008 - val_loss: 1.4809 - val_accuracy: 0.6234\n",
      "Epoch 3742/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3028 - accuracy: 0.9036 - val_loss: 1.4733 - val_accuracy: 0.6364\n",
      "Epoch 3743/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3931 - accuracy: 0.8743 - val_loss: 1.4724 - val_accuracy: 0.6396\n",
      "Epoch 3744/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3414 - accuracy: 0.9022 - val_loss: 1.4720 - val_accuracy: 0.6396\n",
      "Epoch 3745/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2873 - accuracy: 0.9092 - val_loss: 1.4640 - val_accuracy: 0.6494\n",
      "Epoch 3746/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2568 - accuracy: 0.9121 - val_loss: 1.4587 - val_accuracy: 0.6429\n",
      "Epoch 3747/4000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2757 - accuracy: 0.9033 - val_loss: 1.4519 - val_accuracy: 0.6494\n",
      "Epoch 3748/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2974 - accuracy: 0.9121 - val_loss: 1.4498 - val_accuracy: 0.6429\n",
      "Epoch 3749/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3011 - accuracy: 0.9064 - val_loss: 1.4425 - val_accuracy: 0.6429\n",
      "Epoch 3750/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2963 - accuracy: 0.9199 - val_loss: 1.4387 - val_accuracy: 0.6396\n",
      "Epoch 3751/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3065 - accuracy: 0.8965 - val_loss: 1.4526 - val_accuracy: 0.6396\n",
      "Epoch 3752/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3357 - accuracy: 0.8911 - val_loss: 1.4448 - val_accuracy: 0.6364\n",
      "Epoch 3753/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2775 - accuracy: 0.8966 - val_loss: 1.4365 - val_accuracy: 0.6364\n",
      "Epoch 3754/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2988 - accuracy: 0.8926 - val_loss: 1.4365 - val_accuracy: 0.6494\n",
      "Epoch 3755/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2482 - accuracy: 0.9078 - val_loss: 1.4374 - val_accuracy: 0.6526\n",
      "Epoch 3756/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3208 - accuracy: 0.8926 - val_loss: 1.4475 - val_accuracy: 0.6526\n",
      "Epoch 3757/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2715 - accuracy: 0.9062 - val_loss: 1.4659 - val_accuracy: 0.6591\n",
      "Epoch 3758/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3374 - accuracy: 0.8848 - val_loss: 1.4906 - val_accuracy: 0.6558\n",
      "Epoch 3759/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2498 - accuracy: 0.9160 - val_loss: 1.5187 - val_accuracy: 0.6494\n",
      "Epoch 3760/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2970 - accuracy: 0.8911 - val_loss: 1.5464 - val_accuracy: 0.6494\n",
      "Epoch 3761/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2608 - accuracy: 0.9078 - val_loss: 1.5805 - val_accuracy: 0.6429\n",
      "Epoch 3762/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3416 - accuracy: 0.8897 - val_loss: 1.6101 - val_accuracy: 0.6331\n",
      "Epoch 3763/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3147 - accuracy: 0.9036 - val_loss: 1.6325 - val_accuracy: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3764/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3244 - accuracy: 0.8925 - val_loss: 1.6518 - val_accuracy: 0.6169\n",
      "Epoch 3765/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3124 - accuracy: 0.9043 - val_loss: 1.6625 - val_accuracy: 0.6136\n",
      "Epoch 3766/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3054 - accuracy: 0.9023 - val_loss: 1.6709 - val_accuracy: 0.6136\n",
      "Epoch 3767/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2934 - accuracy: 0.9050 - val_loss: 1.6593 - val_accuracy: 0.6006\n",
      "Epoch 3768/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3210 - accuracy: 0.8869 - val_loss: 1.6452 - val_accuracy: 0.5974\n",
      "Epoch 3769/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3077 - accuracy: 0.9023 - val_loss: 1.6347 - val_accuracy: 0.5974\n",
      "Epoch 3770/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3429 - accuracy: 0.8897 - val_loss: 1.6462 - val_accuracy: 0.5877\n",
      "Epoch 3771/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2751 - accuracy: 0.9232 - val_loss: 1.6347 - val_accuracy: 0.5942\n",
      "Epoch 3772/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2870 - accuracy: 0.9141 - val_loss: 1.6267 - val_accuracy: 0.5812\n",
      "Epoch 3773/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2979 - accuracy: 0.8980 - val_loss: 1.6188 - val_accuracy: 0.5812\n",
      "Epoch 3774/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3207 - accuracy: 0.9050 - val_loss: 1.6219 - val_accuracy: 0.5877\n",
      "Epoch 3775/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3219 - accuracy: 0.8926 - val_loss: 1.6408 - val_accuracy: 0.5942\n",
      "Epoch 3776/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3049 - accuracy: 0.9072 - val_loss: 1.6580 - val_accuracy: 0.5909\n",
      "Epoch 3777/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3145 - accuracy: 0.8945 - val_loss: 1.6785 - val_accuracy: 0.5877\n",
      "Epoch 3778/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3179 - accuracy: 0.9008 - val_loss: 1.6756 - val_accuracy: 0.6006\n",
      "Epoch 3779/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2736 - accuracy: 0.9036 - val_loss: 1.6794 - val_accuracy: 0.6071\n",
      "Epoch 3780/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2985 - accuracy: 0.9106 - val_loss: 1.6768 - val_accuracy: 0.6039\n",
      "Epoch 3781/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2768 - accuracy: 0.9102 - val_loss: 1.6476 - val_accuracy: 0.6071\n",
      "Epoch 3782/4000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3190 - accuracy: 0.9148 - val_loss: 1.6024 - val_accuracy: 0.6169\n",
      "Epoch 3783/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2879 - accuracy: 0.9043 - val_loss: 1.5601 - val_accuracy: 0.6234\n",
      "Epoch 3784/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2952 - accuracy: 0.9043 - val_loss: 1.5298 - val_accuracy: 0.6266\n",
      "Epoch 3785/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2842 - accuracy: 0.9082 - val_loss: 1.5076 - val_accuracy: 0.6299\n",
      "Epoch 3786/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3322 - accuracy: 0.8953 - val_loss: 1.4950 - val_accuracy: 0.6299\n",
      "Epoch 3787/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3589 - accuracy: 0.8877 - val_loss: 1.4851 - val_accuracy: 0.6201\n",
      "Epoch 3788/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3079 - accuracy: 0.8953 - val_loss: 1.4703 - val_accuracy: 0.6234\n",
      "Epoch 3789/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2596 - accuracy: 0.9162 - val_loss: 1.4671 - val_accuracy: 0.6266\n",
      "Epoch 3790/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3144 - accuracy: 0.9004 - val_loss: 1.4690 - val_accuracy: 0.6201\n",
      "Epoch 3791/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2702 - accuracy: 0.9092 - val_loss: 1.4691 - val_accuracy: 0.6299\n",
      "Epoch 3792/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3056 - accuracy: 0.8994 - val_loss: 1.4661 - val_accuracy: 0.6266\n",
      "Epoch 3793/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2918 - accuracy: 0.8939 - val_loss: 1.4720 - val_accuracy: 0.6331\n",
      "Epoch 3794/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2848 - accuracy: 0.8994 - val_loss: 1.4727 - val_accuracy: 0.6331\n",
      "Epoch 3795/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3076 - accuracy: 0.8975 - val_loss: 1.4708 - val_accuracy: 0.6169\n",
      "Epoch 3796/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3170 - accuracy: 0.8980 - val_loss: 1.4676 - val_accuracy: 0.6169\n",
      "Epoch 3797/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3323 - accuracy: 0.8906 - val_loss: 1.4572 - val_accuracy: 0.6331\n",
      "Epoch 3798/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2596 - accuracy: 0.9274 - val_loss: 1.4474 - val_accuracy: 0.6396\n",
      "Epoch 3799/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3038 - accuracy: 0.8994 - val_loss: 1.4358 - val_accuracy: 0.6494\n",
      "Epoch 3800/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2975 - accuracy: 0.9033 - val_loss: 1.4319 - val_accuracy: 0.6494\n",
      "Epoch 3801/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3162 - accuracy: 0.9008 - val_loss: 1.4295 - val_accuracy: 0.6494\n",
      "Epoch 3802/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2813 - accuracy: 0.9106 - val_loss: 1.4306 - val_accuracy: 0.6494\n",
      "Epoch 3803/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2899 - accuracy: 0.9176 - val_loss: 1.4310 - val_accuracy: 0.6429\n",
      "Epoch 3804/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3425 - accuracy: 0.8855 - val_loss: 1.4380 - val_accuracy: 0.6429\n",
      "Epoch 3805/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2804 - accuracy: 0.9150 - val_loss: 1.4551 - val_accuracy: 0.6494\n",
      "Epoch 3806/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2874 - accuracy: 0.9180 - val_loss: 1.4920 - val_accuracy: 0.6461\n",
      "Epoch 3807/4000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2901 - accuracy: 0.9111 - val_loss: 1.5490 - val_accuracy: 0.6461\n",
      "Epoch 3808/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3066 - accuracy: 0.9053 - val_loss: 1.5936 - val_accuracy: 0.6364\n",
      "Epoch 3809/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2948 - accuracy: 0.9106 - val_loss: 1.6084 - val_accuracy: 0.6299\n",
      "Epoch 3810/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3366 - accuracy: 0.8869 - val_loss: 1.5957 - val_accuracy: 0.6299\n",
      "Epoch 3811/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3239 - accuracy: 0.8966 - val_loss: 1.5633 - val_accuracy: 0.6364\n",
      "Epoch 3812/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2539 - accuracy: 0.9180 - val_loss: 1.5540 - val_accuracy: 0.6299\n",
      "Epoch 3813/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2797 - accuracy: 0.9106 - val_loss: 1.5593 - val_accuracy: 0.6429\n",
      "Epoch 3814/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2969 - accuracy: 0.8994 - val_loss: 1.5642 - val_accuracy: 0.6396\n",
      "Epoch 3815/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2733 - accuracy: 0.9218 - val_loss: 1.5592 - val_accuracy: 0.6364\n",
      "Epoch 3816/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2548 - accuracy: 0.9111 - val_loss: 1.5583 - val_accuracy: 0.6299\n",
      "Epoch 3817/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3025 - accuracy: 0.8980 - val_loss: 1.5661 - val_accuracy: 0.6364\n",
      "Epoch 3818/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3000 - accuracy: 0.8975 - val_loss: 1.5811 - val_accuracy: 0.6331\n",
      "Epoch 3819/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3198 - accuracy: 0.8926 - val_loss: 1.5763 - val_accuracy: 0.6396\n",
      "Epoch 3820/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2544 - accuracy: 0.9176 - val_loss: 1.5656 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3821/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3121 - accuracy: 0.8965 - val_loss: 1.5521 - val_accuracy: 0.6364\n",
      "Epoch 3822/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2791 - accuracy: 0.9092 - val_loss: 1.5406 - val_accuracy: 0.6396\n",
      "Epoch 3823/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2903 - accuracy: 0.9092 - val_loss: 1.5432 - val_accuracy: 0.6429\n",
      "Epoch 3824/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2944 - accuracy: 0.9148 - val_loss: 1.5430 - val_accuracy: 0.6396\n",
      "Epoch 3825/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2721 - accuracy: 0.9150 - val_loss: 1.5455 - val_accuracy: 0.6364\n",
      "Epoch 3826/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3202 - accuracy: 0.8953 - val_loss: 1.5407 - val_accuracy: 0.6396\n",
      "Epoch 3827/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3433 - accuracy: 0.9064 - val_loss: 1.5405 - val_accuracy: 0.6234\n",
      "Epoch 3828/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2645 - accuracy: 0.9134 - val_loss: 1.5314 - val_accuracy: 0.6234\n",
      "Epoch 3829/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2461 - accuracy: 0.9148 - val_loss: 1.5225 - val_accuracy: 0.6266\n",
      "Epoch 3830/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3101 - accuracy: 0.9004 - val_loss: 1.5217 - val_accuracy: 0.6266\n",
      "Epoch 3831/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3200 - accuracy: 0.8925 - val_loss: 1.5393 - val_accuracy: 0.6331\n",
      "Epoch 3832/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2962 - accuracy: 0.8994 - val_loss: 1.5802 - val_accuracy: 0.6396\n",
      "Epoch 3833/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3197 - accuracy: 0.8906 - val_loss: 1.6357 - val_accuracy: 0.6201\n",
      "Epoch 3834/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3342 - accuracy: 0.8916 - val_loss: 1.6836 - val_accuracy: 0.6136\n",
      "Epoch 3835/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2638 - accuracy: 0.9190 - val_loss: 1.7355 - val_accuracy: 0.6039\n",
      "Epoch 3836/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2741 - accuracy: 0.9082 - val_loss: 1.7705 - val_accuracy: 0.6006\n",
      "Epoch 3837/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2938 - accuracy: 0.9082 - val_loss: 1.8018 - val_accuracy: 0.5942\n",
      "Epoch 3838/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3267 - accuracy: 0.8994 - val_loss: 1.8125 - val_accuracy: 0.6006\n",
      "Epoch 3839/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3659 - accuracy: 0.8789 - val_loss: 1.7826 - val_accuracy: 0.6039\n",
      "Epoch 3840/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2874 - accuracy: 0.9078 - val_loss: 1.7416 - val_accuracy: 0.5942\n",
      "Epoch 3841/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2779 - accuracy: 0.9014 - val_loss: 1.7040 - val_accuracy: 0.5942\n",
      "Epoch 3842/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2932 - accuracy: 0.9053 - val_loss: 1.6691 - val_accuracy: 0.6169\n",
      "Epoch 3843/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2642 - accuracy: 0.9148 - val_loss: 1.6466 - val_accuracy: 0.6201\n",
      "Epoch 3844/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2799 - accuracy: 0.9053 - val_loss: 1.6318 - val_accuracy: 0.6331\n",
      "Epoch 3845/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3106 - accuracy: 0.8965 - val_loss: 1.6253 - val_accuracy: 0.6299\n",
      "Epoch 3846/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3209 - accuracy: 0.9033 - val_loss: 1.6188 - val_accuracy: 0.6266\n",
      "Epoch 3847/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2629 - accuracy: 0.9078 - val_loss: 1.6256 - val_accuracy: 0.6201\n",
      "Epoch 3848/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2652 - accuracy: 0.9082 - val_loss: 1.6381 - val_accuracy: 0.6201\n",
      "Epoch 3849/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2909 - accuracy: 0.9022 - val_loss: 1.6623 - val_accuracy: 0.6201\n",
      "Epoch 3850/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2837 - accuracy: 0.9062 - val_loss: 1.6901 - val_accuracy: 0.6006\n",
      "Epoch 3851/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3021 - accuracy: 0.8939 - val_loss: 1.7222 - val_accuracy: 0.5942\n",
      "Epoch 3852/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2634 - accuracy: 0.9120 - val_loss: 1.7647 - val_accuracy: 0.5974\n",
      "Epoch 3853/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2419 - accuracy: 0.9162 - val_loss: 1.7998 - val_accuracy: 0.5812\n",
      "Epoch 3854/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3017 - accuracy: 0.9082 - val_loss: 1.8101 - val_accuracy: 0.5779\n",
      "Epoch 3855/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2806 - accuracy: 0.9106 - val_loss: 1.7931 - val_accuracy: 0.5812\n",
      "Epoch 3856/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3024 - accuracy: 0.8980 - val_loss: 1.7537 - val_accuracy: 0.5877\n",
      "Epoch 3857/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2594 - accuracy: 0.9131 - val_loss: 1.6828 - val_accuracy: 0.6006\n",
      "Epoch 3858/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2856 - accuracy: 0.9004 - val_loss: 1.6037 - val_accuracy: 0.6136\n",
      "Epoch 3859/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2763 - accuracy: 0.9160 - val_loss: 1.5406 - val_accuracy: 0.6201\n",
      "Epoch 3860/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2684 - accuracy: 0.9170 - val_loss: 1.5055 - val_accuracy: 0.6331\n",
      "Epoch 3861/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2684 - accuracy: 0.9260 - val_loss: 1.4807 - val_accuracy: 0.6429\n",
      "Epoch 3862/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2683 - accuracy: 0.9131 - val_loss: 1.4602 - val_accuracy: 0.6494\n",
      "Epoch 3863/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3457 - accuracy: 0.8936 - val_loss: 1.4540 - val_accuracy: 0.6688\n",
      "Epoch 3864/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2921 - accuracy: 0.8966 - val_loss: 1.4521 - val_accuracy: 0.6721\n",
      "Epoch 3865/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2981 - accuracy: 0.9008 - val_loss: 1.4569 - val_accuracy: 0.6688\n",
      "Epoch 3866/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2945 - accuracy: 0.9036 - val_loss: 1.4651 - val_accuracy: 0.6591\n",
      "Epoch 3867/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2957 - accuracy: 0.8994 - val_loss: 1.4566 - val_accuracy: 0.6429\n",
      "Epoch 3868/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3285 - accuracy: 0.8896 - val_loss: 1.4457 - val_accuracy: 0.6558\n",
      "Epoch 3869/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2937 - accuracy: 0.9082 - val_loss: 1.4427 - val_accuracy: 0.6526\n",
      "Epoch 3870/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3364 - accuracy: 0.8953 - val_loss: 1.4577 - val_accuracy: 0.6526\n",
      "Epoch 3871/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2757 - accuracy: 0.9050 - val_loss: 1.4583 - val_accuracy: 0.6558\n",
      "Epoch 3872/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2838 - accuracy: 0.9062 - val_loss: 1.4612 - val_accuracy: 0.6526\n",
      "Epoch 3873/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2689 - accuracy: 0.9134 - val_loss: 1.4778 - val_accuracy: 0.6526\n",
      "Epoch 3874/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2613 - accuracy: 0.9170 - val_loss: 1.4895 - val_accuracy: 0.6461\n",
      "Epoch 3875/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3080 - accuracy: 0.8939 - val_loss: 1.5141 - val_accuracy: 0.6299\n",
      "Epoch 3876/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2799 - accuracy: 0.9004 - val_loss: 1.5442 - val_accuracy: 0.6396\n",
      "Epoch 3877/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2910 - accuracy: 0.9043 - val_loss: 1.5724 - val_accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3878/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2757 - accuracy: 0.9232 - val_loss: 1.6070 - val_accuracy: 0.6136\n",
      "Epoch 3879/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2391 - accuracy: 0.9344 - val_loss: 1.6208 - val_accuracy: 0.6104\n",
      "Epoch 3880/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3007 - accuracy: 0.9072 - val_loss: 1.6203 - val_accuracy: 0.6136\n",
      "Epoch 3881/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2739 - accuracy: 0.9111 - val_loss: 1.5833 - val_accuracy: 0.6201\n",
      "Epoch 3882/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2572 - accuracy: 0.9176 - val_loss: 1.5560 - val_accuracy: 0.6201\n",
      "Epoch 3883/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3092 - accuracy: 0.8877 - val_loss: 1.5217 - val_accuracy: 0.6169\n",
      "Epoch 3884/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2977 - accuracy: 0.9022 - val_loss: 1.4708 - val_accuracy: 0.6396\n",
      "Epoch 3885/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3180 - accuracy: 0.8906 - val_loss: 1.4248 - val_accuracy: 0.6558\n",
      "Epoch 3886/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2604 - accuracy: 0.9141 - val_loss: 1.4213 - val_accuracy: 0.6591\n",
      "Epoch 3887/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2611 - accuracy: 0.9148 - val_loss: 1.4151 - val_accuracy: 0.6656\n",
      "Epoch 3888/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4002 - accuracy: 0.8955 - val_loss: 1.4089 - val_accuracy: 0.6721\n",
      "Epoch 3889/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2882 - accuracy: 0.9111 - val_loss: 1.4315 - val_accuracy: 0.6656\n",
      "Epoch 3890/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3144 - accuracy: 0.9008 - val_loss: 1.4573 - val_accuracy: 0.6656\n",
      "Epoch 3891/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3051 - accuracy: 0.8994 - val_loss: 1.4977 - val_accuracy: 0.6526\n",
      "Epoch 3892/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2819 - accuracy: 0.9053 - val_loss: 1.5334 - val_accuracy: 0.6494\n",
      "Epoch 3893/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2658 - accuracy: 0.9219 - val_loss: 1.5625 - val_accuracy: 0.6429\n",
      "Epoch 3894/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3059 - accuracy: 0.8975 - val_loss: 1.5885 - val_accuracy: 0.6396\n",
      "Epoch 3895/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3106 - accuracy: 0.9004 - val_loss: 1.6057 - val_accuracy: 0.6364\n",
      "Epoch 3896/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3077 - accuracy: 0.9120 - val_loss: 1.6258 - val_accuracy: 0.6201\n",
      "Epoch 3897/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2856 - accuracy: 0.9078 - val_loss: 1.6275 - val_accuracy: 0.6039\n",
      "Epoch 3898/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2932 - accuracy: 0.9014 - val_loss: 1.6321 - val_accuracy: 0.6039\n",
      "Epoch 3899/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3000 - accuracy: 0.9092 - val_loss: 1.6375 - val_accuracy: 0.6169\n",
      "Epoch 3900/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2819 - accuracy: 0.9023 - val_loss: 1.6719 - val_accuracy: 0.6136\n",
      "Epoch 3901/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2801 - accuracy: 0.9064 - val_loss: 1.7098 - val_accuracy: 0.6039\n",
      "Epoch 3902/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2904 - accuracy: 0.9053 - val_loss: 1.7266 - val_accuracy: 0.6006\n",
      "Epoch 3903/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2900 - accuracy: 0.9078 - val_loss: 1.7443 - val_accuracy: 0.6104\n",
      "Epoch 3904/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3012 - accuracy: 0.9141 - val_loss: 1.7554 - val_accuracy: 0.6071\n",
      "Epoch 3905/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2728 - accuracy: 0.9148 - val_loss: 1.7630 - val_accuracy: 0.6039\n",
      "Epoch 3906/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2571 - accuracy: 0.9131 - val_loss: 1.7497 - val_accuracy: 0.6071\n",
      "Epoch 3907/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3237 - accuracy: 0.9064 - val_loss: 1.7002 - val_accuracy: 0.5974\n",
      "Epoch 3908/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3115 - accuracy: 0.8965 - val_loss: 1.6803 - val_accuracy: 0.6006\n",
      "Epoch 3909/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2999 - accuracy: 0.9004 - val_loss: 1.6459 - val_accuracy: 0.6136\n",
      "Epoch 3910/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2997 - accuracy: 0.9111 - val_loss: 1.6092 - val_accuracy: 0.6234\n",
      "Epoch 3911/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2765 - accuracy: 0.9092 - val_loss: 1.5959 - val_accuracy: 0.6266\n",
      "Epoch 3912/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2873 - accuracy: 0.9033 - val_loss: 1.5847 - val_accuracy: 0.6234\n",
      "Epoch 3913/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3136 - accuracy: 0.9008 - val_loss: 1.5696 - val_accuracy: 0.6234\n",
      "Epoch 3914/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3084 - accuracy: 0.9064 - val_loss: 1.5569 - val_accuracy: 0.6331\n",
      "Epoch 3915/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2884 - accuracy: 0.9148 - val_loss: 1.5393 - val_accuracy: 0.6299\n",
      "Epoch 3916/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3004 - accuracy: 0.9053 - val_loss: 1.5300 - val_accuracy: 0.6396\n",
      "Epoch 3917/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2716 - accuracy: 0.9092 - val_loss: 1.5286 - val_accuracy: 0.6364\n",
      "Epoch 3918/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3149 - accuracy: 0.9043 - val_loss: 1.5331 - val_accuracy: 0.6396\n",
      "Epoch 3919/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2837 - accuracy: 0.9150 - val_loss: 1.5368 - val_accuracy: 0.6396\n",
      "Epoch 3920/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2799 - accuracy: 0.9053 - val_loss: 1.5447 - val_accuracy: 0.6429\n",
      "Epoch 3921/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2710 - accuracy: 0.8994 - val_loss: 1.5453 - val_accuracy: 0.6429\n",
      "Epoch 3922/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3024 - accuracy: 0.9008 - val_loss: 1.5340 - val_accuracy: 0.6494\n",
      "Epoch 3923/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2930 - accuracy: 0.9062 - val_loss: 1.5247 - val_accuracy: 0.6364\n",
      "Epoch 3924/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2636 - accuracy: 0.9134 - val_loss: 1.5253 - val_accuracy: 0.6364\n",
      "Epoch 3925/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2687 - accuracy: 0.9131 - val_loss: 1.5359 - val_accuracy: 0.6364\n",
      "Epoch 3926/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2534 - accuracy: 0.9232 - val_loss: 1.5401 - val_accuracy: 0.6396\n",
      "Epoch 3927/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2812 - accuracy: 0.9064 - val_loss: 1.5336 - val_accuracy: 0.6461\n",
      "Epoch 3928/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2881 - accuracy: 0.9141 - val_loss: 1.5277 - val_accuracy: 0.6429\n",
      "Epoch 3929/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3074 - accuracy: 0.9062 - val_loss: 1.5248 - val_accuracy: 0.6429\n",
      "Epoch 3930/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3051 - accuracy: 0.8939 - val_loss: 1.5210 - val_accuracy: 0.6526\n",
      "Epoch 3931/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2716 - accuracy: 0.9176 - val_loss: 1.5231 - val_accuracy: 0.6526\n",
      "Epoch 3932/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2360 - accuracy: 0.9232 - val_loss: 1.5284 - val_accuracy: 0.6494\n",
      "Epoch 3933/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3783 - accuracy: 0.8841 - val_loss: 1.5213 - val_accuracy: 0.6429\n",
      "Epoch 3934/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3144 - accuracy: 0.8848 - val_loss: 1.5188 - val_accuracy: 0.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3935/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2906 - accuracy: 0.9064 - val_loss: 1.5131 - val_accuracy: 0.6234\n",
      "Epoch 3936/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2726 - accuracy: 0.9102 - val_loss: 1.5055 - val_accuracy: 0.6169\n",
      "Epoch 3937/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2850 - accuracy: 0.9053 - val_loss: 1.4993 - val_accuracy: 0.6234\n",
      "Epoch 3938/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3350 - accuracy: 0.8896 - val_loss: 1.4905 - val_accuracy: 0.6234\n",
      "Epoch 3939/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2695 - accuracy: 0.9043 - val_loss: 1.4837 - val_accuracy: 0.6299\n",
      "Epoch 3940/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2886 - accuracy: 0.9033 - val_loss: 1.4828 - val_accuracy: 0.6364\n",
      "Epoch 3941/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3073 - accuracy: 0.9036 - val_loss: 1.4953 - val_accuracy: 0.6266\n",
      "Epoch 3942/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2696 - accuracy: 0.9134 - val_loss: 1.5196 - val_accuracy: 0.6299\n",
      "Epoch 3943/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2760 - accuracy: 0.9082 - val_loss: 1.5473 - val_accuracy: 0.6234\n",
      "Epoch 3944/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3256 - accuracy: 0.8897 - val_loss: 1.5748 - val_accuracy: 0.6299\n",
      "Epoch 3945/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2804 - accuracy: 0.9120 - val_loss: 1.5920 - val_accuracy: 0.6201\n",
      "Epoch 3946/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2425 - accuracy: 0.9268 - val_loss: 1.6058 - val_accuracy: 0.6136\n",
      "Epoch 3947/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2954 - accuracy: 0.9120 - val_loss: 1.5904 - val_accuracy: 0.6136\n",
      "Epoch 3948/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2937 - accuracy: 0.9092 - val_loss: 1.5540 - val_accuracy: 0.6071\n",
      "Epoch 3949/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2899 - accuracy: 0.9064 - val_loss: 1.4987 - val_accuracy: 0.6071\n",
      "Epoch 3950/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2622 - accuracy: 0.9141 - val_loss: 1.4554 - val_accuracy: 0.6136\n",
      "Epoch 3951/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2960 - accuracy: 0.8984 - val_loss: 1.4143 - val_accuracy: 0.6494\n",
      "Epoch 3952/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2897 - accuracy: 0.9148 - val_loss: 1.3876 - val_accuracy: 0.6591\n",
      "Epoch 3953/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3229 - accuracy: 0.8994 - val_loss: 1.3769 - val_accuracy: 0.6591\n",
      "Epoch 3954/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3430 - accuracy: 0.9023 - val_loss: 1.3792 - val_accuracy: 0.6623\n",
      "Epoch 3955/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2830 - accuracy: 0.9082 - val_loss: 1.3935 - val_accuracy: 0.6721\n",
      "Epoch 3956/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3180 - accuracy: 0.9053 - val_loss: 1.4060 - val_accuracy: 0.6558\n",
      "Epoch 3957/4000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3088 - accuracy: 0.9082 - val_loss: 1.4277 - val_accuracy: 0.6526\n",
      "Epoch 3958/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2469 - accuracy: 0.9176 - val_loss: 1.4578 - val_accuracy: 0.6461\n",
      "Epoch 3959/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2936 - accuracy: 0.9092 - val_loss: 1.4967 - val_accuracy: 0.6364\n",
      "Epoch 3960/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3159 - accuracy: 0.8966 - val_loss: 1.5348 - val_accuracy: 0.6364\n",
      "Epoch 3961/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2682 - accuracy: 0.9043 - val_loss: 1.5408 - val_accuracy: 0.6299\n",
      "Epoch 3962/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3000 - accuracy: 0.9022 - val_loss: 1.5088 - val_accuracy: 0.6429\n",
      "Epoch 3963/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2833 - accuracy: 0.9190 - val_loss: 1.4796 - val_accuracy: 0.6396\n",
      "Epoch 3964/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3324 - accuracy: 0.9023 - val_loss: 1.4538 - val_accuracy: 0.6461\n",
      "Epoch 3965/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2823 - accuracy: 0.8984 - val_loss: 1.4254 - val_accuracy: 0.6591\n",
      "Epoch 3966/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3568 - accuracy: 0.8939 - val_loss: 1.4041 - val_accuracy: 0.6688\n",
      "Epoch 3967/4000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3118 - accuracy: 0.9022 - val_loss: 1.3909 - val_accuracy: 0.6558\n",
      "Epoch 3968/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2297 - accuracy: 0.9204 - val_loss: 1.3966 - val_accuracy: 0.6494\n",
      "Epoch 3969/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2923 - accuracy: 0.9120 - val_loss: 1.3985 - val_accuracy: 0.6494\n",
      "Epoch 3970/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2442 - accuracy: 0.9072 - val_loss: 1.4040 - val_accuracy: 0.6623\n",
      "Epoch 3971/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2876 - accuracy: 0.9092 - val_loss: 1.4157 - val_accuracy: 0.6526\n",
      "Epoch 3972/4000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2761 - accuracy: 0.9199 - val_loss: 1.4322 - val_accuracy: 0.6526\n",
      "Epoch 3973/4000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3295 - accuracy: 0.8939 - val_loss: 1.4724 - val_accuracy: 0.6364\n",
      "Epoch 3974/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2679 - accuracy: 0.9199 - val_loss: 1.5312 - val_accuracy: 0.6266\n",
      "Epoch 3975/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2817 - accuracy: 0.9078 - val_loss: 1.5949 - val_accuracy: 0.6201\n",
      "Epoch 3976/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2829 - accuracy: 0.8984 - val_loss: 1.6126 - val_accuracy: 0.6136\n",
      "Epoch 3977/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3392 - accuracy: 0.8925 - val_loss: 1.5675 - val_accuracy: 0.6169\n",
      "Epoch 3978/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3241 - accuracy: 0.8939 - val_loss: 1.5063 - val_accuracy: 0.6299\n",
      "Epoch 3979/4000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3297 - accuracy: 0.8953 - val_loss: 1.4734 - val_accuracy: 0.6331\n",
      "Epoch 3980/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3077 - accuracy: 0.8945 - val_loss: 1.4473 - val_accuracy: 0.6299\n",
      "Epoch 3981/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3349 - accuracy: 0.8867 - val_loss: 1.4392 - val_accuracy: 0.6234\n",
      "Epoch 3982/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3042 - accuracy: 0.8955 - val_loss: 1.4419 - val_accuracy: 0.6364\n",
      "Epoch 3983/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2948 - accuracy: 0.9014 - val_loss: 1.4430 - val_accuracy: 0.6396\n",
      "Epoch 3984/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2881 - accuracy: 0.9064 - val_loss: 1.4547 - val_accuracy: 0.6299\n",
      "Epoch 3985/4000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3025 - accuracy: 0.9092 - val_loss: 1.4668 - val_accuracy: 0.6234\n",
      "Epoch 3986/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2792 - accuracy: 0.9092 - val_loss: 1.4728 - val_accuracy: 0.6396\n",
      "Epoch 3987/4000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2669 - accuracy: 0.9120 - val_loss: 1.4875 - val_accuracy: 0.6364\n",
      "Epoch 3988/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2657 - accuracy: 0.9111 - val_loss: 1.5096 - val_accuracy: 0.6299\n",
      "Epoch 3989/4000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3062 - accuracy: 0.9004 - val_loss: 1.5220 - val_accuracy: 0.6234\n",
      "Epoch 3990/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2755 - accuracy: 0.9106 - val_loss: 1.5149 - val_accuracy: 0.6331\n",
      "Epoch 3991/4000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2722 - accuracy: 0.9199 - val_loss: 1.5279 - val_accuracy: 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3992/4000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2761 - accuracy: 0.9033 - val_loss: 1.5566 - val_accuracy: 0.6169\n",
      "Epoch 3993/4000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2464 - accuracy: 0.9036 - val_loss: 1.5567 - val_accuracy: 0.6169\n",
      "Epoch 3994/4000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3355 - accuracy: 0.8925 - val_loss: 1.5282 - val_accuracy: 0.6136\n",
      "Epoch 3995/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2507 - accuracy: 0.9218 - val_loss: 1.5136 - val_accuracy: 0.6201\n",
      "Epoch 3996/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3037 - accuracy: 0.8994 - val_loss: 1.5035 - val_accuracy: 0.6234\n",
      "Epoch 3997/4000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2772 - accuracy: 0.9218 - val_loss: 1.4810 - val_accuracy: 0.6299\n",
      "Epoch 3998/4000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2962 - accuracy: 0.9014 - val_loss: 1.4771 - val_accuracy: 0.6461\n",
      "Epoch 3999/4000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2784 - accuracy: 0.8994 - val_loss: 1.4792 - val_accuracy: 0.6429\n",
      "Epoch 4000/4000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2982 - accuracy: 0.9078 - val_loss: 1.4857 - val_accuracy: 0.6461\n",
      "CNN: Epochs=4000, Train accuracy=0.93436, Validation accuracy=0.69805\n"
     ]
    }
   ],
   "source": [
    "epochs = 4000\n",
    "batch_size = 1024\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AN-n4oXHFhB"
   },
   "outputs": [],
   "source": [
    "#k-fold로 훈련시키기\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "epochs = 800\n",
    "batch_size = 512\n",
    "\n",
    "for k_train_index, k_valid_index in kf.split(X, y):\n",
    "    history = model.fit(\n",
    "    datagen.flow(X[k_train_index,:], y[k_train_index,:], batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=X[k_train_index,:].shape[0]//batch_size,\n",
    "    validation_data=(X[k_valid_index,:], y[k_valid_index,:]),\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2603262,
     "status": "ok",
     "timestamp": 1598167802346,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "MaRzqpQmHFhD",
    "outputId": "9bcd067b-5514-491c-8c63-db77d4fc6967"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####relu 대신 leaky relu 써보자########\n",
    "#####Adam보단 RMSprop이 좋은 듯도?######\n",
    "#####후에 early stopping도 추가 ########\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHMuMFz1DsU2"
   },
   "outputs": [],
   "source": [
    "#model_10\n",
    "from tensorflow import keras\n",
    "#model_9-3 9-2에서 optimizer만 RMSprop로 바꿔봄\n",
    "from tensorflow import keras\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, Add,\n",
    "    Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             shear_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), Adagrad(lr=0.01, epsilon=None, decay=0.0), adadelta, adam\n",
    "    model.compile(optimizer=RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1598177058258,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "Lmn384nZDysV",
    "outputId": "5f3b3b04-604f-410c-ac92-0ec2f300de42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1201 (Conv2D)         (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1157 (Ba (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_630 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1202 (Conv2D)         (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1158 (Ba (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_631 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1203 (Conv2D)         (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1159 (Ba (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_632 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1204 (Conv2D)         (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1160 (Ba (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_633 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1205 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1161 (Ba (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_634 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1206 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1162 (Ba (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_635 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1207 (Conv2D)         (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1163 (Ba (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_636 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1208 (Conv2D)         (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1164 (Ba (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_637 (LeakyReLU)  (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1209 (Conv2D)         (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1165 (Ba (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_638 (LeakyReLU)  (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_31  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_639 (LeakyReLU)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 125,392\n",
      "Trainable params: 124,720\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVsg_v1KD4rr"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.975 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 858204,
     "status": "ok",
     "timestamp": 1598177916752,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "_YuoyhnFD7qj",
    "outputId": "9a9e2baa-b85e-405f-d605-310f7ff15302",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 2.4998 - accuracy: 0.1276 - val_loss: 2.2552 - val_accuracy: 0.1526\n",
      "Epoch 2/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 2.2659 - accuracy: 0.1593 - val_loss: 2.1527 - val_accuracy: 0.2273\n",
      "Epoch 3/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 2.1901 - accuracy: 0.1868 - val_loss: 2.3029 - val_accuracy: 0.1201\n",
      "Epoch 4/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 2.1149 - accuracy: 0.2330 - val_loss: 2.4806 - val_accuracy: 0.1039\n",
      "Epoch 5/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 2.0548 - accuracy: 0.2617 - val_loss: 3.0083 - val_accuracy: 0.1039\n",
      "Epoch 6/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.9871 - accuracy: 0.2875 - val_loss: 2.8759 - val_accuracy: 0.1039\n",
      "Epoch 7/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.8967 - accuracy: 0.3226 - val_loss: 2.7598 - val_accuracy: 0.1169\n",
      "Epoch 8/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.8031 - accuracy: 0.3618 - val_loss: 3.1314 - val_accuracy: 0.1234\n",
      "Epoch 9/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.7294 - accuracy: 0.3852 - val_loss: 1.8271 - val_accuracy: 0.4221\n",
      "Epoch 10/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.6769 - accuracy: 0.3958 - val_loss: 1.2876 - val_accuracy: 0.6234\n",
      "Epoch 11/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.5821 - accuracy: 0.4420 - val_loss: 1.3205 - val_accuracy: 0.5877\n",
      "Epoch 12/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.5320 - accuracy: 0.4693 - val_loss: 1.2170 - val_accuracy: 0.6136\n",
      "Epoch 13/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.4765 - accuracy: 0.4795 - val_loss: 1.2912 - val_accuracy: 0.5877\n",
      "Epoch 14/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.4124 - accuracy: 0.5146 - val_loss: 1.2951 - val_accuracy: 0.5714\n",
      "Epoch 15/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 1.3823 - accuracy: 0.5187 - val_loss: 1.4694 - val_accuracy: 0.5065\n",
      "Epoch 16/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.3816 - accuracy: 0.5018 - val_loss: 1.0159 - val_accuracy: 0.6688\n",
      "Epoch 17/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.2975 - accuracy: 0.5527 - val_loss: 1.0708 - val_accuracy: 0.6429\n",
      "Epoch 18/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.2263 - accuracy: 0.5796 - val_loss: 0.9438 - val_accuracy: 0.6818\n",
      "Epoch 19/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.2246 - accuracy: 0.5790 - val_loss: 1.0757 - val_accuracy: 0.6429\n",
      "Epoch 20/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.1714 - accuracy: 0.6060 - val_loss: 0.8871 - val_accuracy: 0.6883\n",
      "Epoch 21/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.1384 - accuracy: 0.6189 - val_loss: 0.8942 - val_accuracy: 0.6883\n",
      "Epoch 22/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.1424 - accuracy: 0.6171 - val_loss: 0.8996 - val_accuracy: 0.6851\n",
      "Epoch 23/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.0789 - accuracy: 0.6352 - val_loss: 0.7822 - val_accuracy: 0.7338\n",
      "Epoch 24/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.0897 - accuracy: 0.6306 - val_loss: 0.7925 - val_accuracy: 0.7338\n",
      "Epoch 25/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 1.0528 - accuracy: 0.6411 - val_loss: 0.7672 - val_accuracy: 0.7338\n",
      "Epoch 26/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9782 - accuracy: 0.6569 - val_loss: 0.6813 - val_accuracy: 0.7727\n",
      "Epoch 27/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9853 - accuracy: 0.6692 - val_loss: 0.7866 - val_accuracy: 0.7370\n",
      "Epoch 28/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9913 - accuracy: 0.6628 - val_loss: 0.8792 - val_accuracy: 0.7013\n",
      "Epoch 29/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9389 - accuracy: 0.6797 - val_loss: 0.7541 - val_accuracy: 0.7403\n",
      "Epoch 30/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9157 - accuracy: 0.6821 - val_loss: 0.9700 - val_accuracy: 0.6688\n",
      "Epoch 31/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9173 - accuracy: 0.6944 - val_loss: 0.6722 - val_accuracy: 0.7695\n",
      "Epoch 32/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.9178 - accuracy: 0.6909 - val_loss: 0.7221 - val_accuracy: 0.7500\n",
      "Epoch 33/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8738 - accuracy: 0.6915 - val_loss: 0.7235 - val_accuracy: 0.7597\n",
      "Epoch 34/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8717 - accuracy: 0.7067 - val_loss: 0.6963 - val_accuracy: 0.7727\n",
      "Epoch 35/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8594 - accuracy: 0.7190 - val_loss: 0.8186 - val_accuracy: 0.7175\n",
      "Epoch 36/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8376 - accuracy: 0.7149 - val_loss: 0.6815 - val_accuracy: 0.7825\n",
      "Epoch 37/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8354 - accuracy: 0.7084 - val_loss: 0.8090 - val_accuracy: 0.7305\n",
      "Epoch 38/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8459 - accuracy: 0.7137 - val_loss: 0.6092 - val_accuracy: 0.7955\n",
      "Epoch 39/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8059 - accuracy: 0.7336 - val_loss: 0.7836 - val_accuracy: 0.7338\n",
      "Epoch 40/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8178 - accuracy: 0.7219 - val_loss: 0.6750 - val_accuracy: 0.7597\n",
      "Epoch 41/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7811 - accuracy: 0.7254 - val_loss: 0.6522 - val_accuracy: 0.7630\n",
      "Epoch 42/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7963 - accuracy: 0.7365 - val_loss: 0.7445 - val_accuracy: 0.7435\n",
      "Epoch 43/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8059 - accuracy: 0.7196 - val_loss: 0.6496 - val_accuracy: 0.7857\n",
      "Epoch 44/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.8114 - accuracy: 0.7149 - val_loss: 0.7443 - val_accuracy: 0.7305\n",
      "Epoch 45/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7662 - accuracy: 0.7500 - val_loss: 0.6249 - val_accuracy: 0.7792\n",
      "Epoch 46/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7500 - accuracy: 0.7383 - val_loss: 0.6272 - val_accuracy: 0.7890\n",
      "Epoch 47/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7456 - accuracy: 0.7447 - val_loss: 0.7600 - val_accuracy: 0.7468\n",
      "Epoch 48/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7262 - accuracy: 0.7488 - val_loss: 0.5925 - val_accuracy: 0.7987\n",
      "Epoch 49/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7260 - accuracy: 0.7412 - val_loss: 0.6221 - val_accuracy: 0.8019\n",
      "Epoch 50/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7505 - accuracy: 0.7559 - val_loss: 0.7101 - val_accuracy: 0.7435\n",
      "Epoch 51/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7277 - accuracy: 0.7535 - val_loss: 0.6287 - val_accuracy: 0.7857\n",
      "Epoch 52/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7258 - accuracy: 0.7465 - val_loss: 0.5989 - val_accuracy: 0.8052\n",
      "Epoch 53/150\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.6988 - accuracy: 0.7652 - val_loss: 0.6240 - val_accuracy: 0.7825\n",
      "Epoch 54/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7062 - accuracy: 0.7541 - val_loss: 0.6481 - val_accuracy: 0.8019\n",
      "Epoch 55/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6977 - accuracy: 0.7711 - val_loss: 0.6163 - val_accuracy: 0.7987\n",
      "Epoch 56/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6929 - accuracy: 0.7570 - val_loss: 0.6802 - val_accuracy: 0.7792\n",
      "Epoch 57/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.7194 - accuracy: 0.7529 - val_loss: 0.6242 - val_accuracy: 0.7890\n",
      "Epoch 58/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6953 - accuracy: 0.7605 - val_loss: 0.6067 - val_accuracy: 0.8019\n",
      "Epoch 59/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6613 - accuracy: 0.7758 - val_loss: 0.6572 - val_accuracy: 0.7760\n",
      "Epoch 60/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6696 - accuracy: 0.7681 - val_loss: 0.8487 - val_accuracy: 0.7273\n",
      "Epoch 61/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6757 - accuracy: 0.7646 - val_loss: 0.7014 - val_accuracy: 0.7662\n",
      "Epoch 62/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6608 - accuracy: 0.7670 - val_loss: 0.6366 - val_accuracy: 0.7857\n",
      "Epoch 63/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6320 - accuracy: 0.7881 - val_loss: 0.7097 - val_accuracy: 0.7695\n",
      "Epoch 64/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6651 - accuracy: 0.7693 - val_loss: 0.6045 - val_accuracy: 0.7987\n",
      "Epoch 65/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6655 - accuracy: 0.7641 - val_loss: 0.6040 - val_accuracy: 0.7987\n",
      "Epoch 66/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6524 - accuracy: 0.7664 - val_loss: 0.6303 - val_accuracy: 0.8052\n",
      "Epoch 67/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6729 - accuracy: 0.7810 - val_loss: 0.6506 - val_accuracy: 0.7890\n",
      "Epoch 68/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6340 - accuracy: 0.7951 - val_loss: 0.6675 - val_accuracy: 0.7565\n",
      "Epoch 69/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6644 - accuracy: 0.7752 - val_loss: 0.5886 - val_accuracy: 0.8182\n",
      "Epoch 70/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6400 - accuracy: 0.7804 - val_loss: 0.5669 - val_accuracy: 0.7955\n",
      "Epoch 71/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6376 - accuracy: 0.7822 - val_loss: 0.5742 - val_accuracy: 0.8117\n",
      "Epoch 72/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6627 - accuracy: 0.7863 - val_loss: 0.6134 - val_accuracy: 0.8182\n",
      "Epoch 73/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6027 - accuracy: 0.7986 - val_loss: 0.6403 - val_accuracy: 0.7987\n",
      "Epoch 74/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6432 - accuracy: 0.7910 - val_loss: 0.6138 - val_accuracy: 0.7922\n",
      "Epoch 75/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6152 - accuracy: 0.7957 - val_loss: 0.5987 - val_accuracy: 0.8084\n",
      "Epoch 76/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6078 - accuracy: 0.7904 - val_loss: 0.6446 - val_accuracy: 0.7857\n",
      "Epoch 77/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6203 - accuracy: 0.7810 - val_loss: 0.6096 - val_accuracy: 0.7922\n",
      "Epoch 78/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6308 - accuracy: 0.7828 - val_loss: 0.6246 - val_accuracy: 0.7987\n",
      "Epoch 79/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6240 - accuracy: 0.7822 - val_loss: 0.6460 - val_accuracy: 0.7857\n",
      "Epoch 80/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6232 - accuracy: 0.7851 - val_loss: 0.6637 - val_accuracy: 0.7825\n",
      "Epoch 81/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6052 - accuracy: 0.7945 - val_loss: 0.7033 - val_accuracy: 0.7630\n",
      "Epoch 82/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6134 - accuracy: 0.7881 - val_loss: 0.5900 - val_accuracy: 0.7792\n",
      "Epoch 83/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6014 - accuracy: 0.7910 - val_loss: 0.6836 - val_accuracy: 0.7760\n",
      "Epoch 84/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5992 - accuracy: 0.7986 - val_loss: 0.6291 - val_accuracy: 0.7955\n",
      "Epoch 85/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5965 - accuracy: 0.8015 - val_loss: 0.6344 - val_accuracy: 0.7922\n",
      "Epoch 86/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6191 - accuracy: 0.7886 - val_loss: 0.6093 - val_accuracy: 0.7890\n",
      "Epoch 87/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6049 - accuracy: 0.7933 - val_loss: 0.6477 - val_accuracy: 0.7825\n",
      "Epoch 88/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5972 - accuracy: 0.7951 - val_loss: 0.5780 - val_accuracy: 0.8182\n",
      "Epoch 89/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6217 - accuracy: 0.7904 - val_loss: 0.5783 - val_accuracy: 0.8117\n",
      "Epoch 90/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.6283 - accuracy: 0.7886 - val_loss: 0.6178 - val_accuracy: 0.8019\n",
      "Epoch 91/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5556 - accuracy: 0.8085 - val_loss: 0.5905 - val_accuracy: 0.8084\n",
      "Epoch 92/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5792 - accuracy: 0.8085 - val_loss: 0.6331 - val_accuracy: 0.8019\n",
      "Epoch 93/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5812 - accuracy: 0.7904 - val_loss: 0.6477 - val_accuracy: 0.7825\n",
      "Epoch 94/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5970 - accuracy: 0.7945 - val_loss: 0.6084 - val_accuracy: 0.8019\n",
      "Epoch 95/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5827 - accuracy: 0.7998 - val_loss: 0.6296 - val_accuracy: 0.7760\n",
      "Epoch 96/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5704 - accuracy: 0.8103 - val_loss: 0.5769 - val_accuracy: 0.8052\n",
      "Epoch 97/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5970 - accuracy: 0.7904 - val_loss: 0.6054 - val_accuracy: 0.7955\n",
      "Epoch 98/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5966 - accuracy: 0.7998 - val_loss: 0.5896 - val_accuracy: 0.8019\n",
      "Epoch 99/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5565 - accuracy: 0.8050 - val_loss: 0.6189 - val_accuracy: 0.7922\n",
      "Epoch 100/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5815 - accuracy: 0.8142 - val_loss: 0.6427 - val_accuracy: 0.7890\n",
      "Epoch 101/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5558 - accuracy: 0.8091 - val_loss: 0.6597 - val_accuracy: 0.7695\n",
      "Epoch 102/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5649 - accuracy: 0.8208 - val_loss: 0.6526 - val_accuracy: 0.7760\n",
      "Epoch 103/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5582 - accuracy: 0.8080 - val_loss: 0.6199 - val_accuracy: 0.7792\n",
      "Epoch 104/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5485 - accuracy: 0.8004 - val_loss: 0.6102 - val_accuracy: 0.8052\n",
      "Epoch 105/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5762 - accuracy: 0.7992 - val_loss: 0.6148 - val_accuracy: 0.8019\n",
      "Epoch 106/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5449 - accuracy: 0.8109 - val_loss: 0.5861 - val_accuracy: 0.7987\n",
      "Epoch 107/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5728 - accuracy: 0.8068 - val_loss: 0.5534 - val_accuracy: 0.8084\n",
      "Epoch 108/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5739 - accuracy: 0.8039 - val_loss: 0.6319 - val_accuracy: 0.7825\n",
      "Epoch 109/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5926 - accuracy: 0.7968 - val_loss: 0.6230 - val_accuracy: 0.7825\n",
      "Epoch 110/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5398 - accuracy: 0.8156 - val_loss: 0.6177 - val_accuracy: 0.7922\n",
      "Epoch 111/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5507 - accuracy: 0.8121 - val_loss: 0.5872 - val_accuracy: 0.7987\n",
      "Epoch 112/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5678 - accuracy: 0.8074 - val_loss: 0.5963 - val_accuracy: 0.7955\n",
      "Epoch 113/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5877 - accuracy: 0.7869 - val_loss: 0.5938 - val_accuracy: 0.7987\n",
      "Epoch 114/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5390 - accuracy: 0.8144 - val_loss: 0.6080 - val_accuracy: 0.7922\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5488 - accuracy: 0.8021 - val_loss: 0.5931 - val_accuracy: 0.7890\n",
      "Epoch 116/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5346 - accuracy: 0.8197 - val_loss: 0.6279 - val_accuracy: 0.7955\n",
      "Epoch 117/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5494 - accuracy: 0.8138 - val_loss: 0.6057 - val_accuracy: 0.7922\n",
      "Epoch 118/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5358 - accuracy: 0.8208 - val_loss: 0.6448 - val_accuracy: 0.7857\n",
      "Epoch 119/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5234 - accuracy: 0.8185 - val_loss: 0.6424 - val_accuracy: 0.7792\n",
      "Epoch 120/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5441 - accuracy: 0.8197 - val_loss: 0.6297 - val_accuracy: 0.7825\n",
      "Epoch 121/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5419 - accuracy: 0.8050 - val_loss: 0.6185 - val_accuracy: 0.7890\n",
      "Epoch 122/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5313 - accuracy: 0.8132 - val_loss: 0.6142 - val_accuracy: 0.7922\n",
      "Epoch 123/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5291 - accuracy: 0.8074 - val_loss: 0.6259 - val_accuracy: 0.7857\n",
      "Epoch 124/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5717 - accuracy: 0.8109 - val_loss: 0.6011 - val_accuracy: 0.7955\n",
      "Epoch 125/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5470 - accuracy: 0.8068 - val_loss: 0.5836 - val_accuracy: 0.8084\n",
      "Epoch 126/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5441 - accuracy: 0.8056 - val_loss: 0.6075 - val_accuracy: 0.8019\n",
      "Epoch 127/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5169 - accuracy: 0.8279 - val_loss: 0.6067 - val_accuracy: 0.7890\n",
      "Epoch 128/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5248 - accuracy: 0.8290 - val_loss: 0.6456 - val_accuracy: 0.7825\n",
      "Epoch 129/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5331 - accuracy: 0.8214 - val_loss: 0.5859 - val_accuracy: 0.8019\n",
      "Epoch 130/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5741 - accuracy: 0.8091 - val_loss: 0.5804 - val_accuracy: 0.8214\n",
      "Epoch 131/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5071 - accuracy: 0.8349 - val_loss: 0.6237 - val_accuracy: 0.7857\n",
      "Epoch 132/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5542 - accuracy: 0.8050 - val_loss: 0.5969 - val_accuracy: 0.8052\n",
      "Epoch 133/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5382 - accuracy: 0.8138 - val_loss: 0.6054 - val_accuracy: 0.8019\n",
      "Epoch 134/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5441 - accuracy: 0.8121 - val_loss: 0.6200 - val_accuracy: 0.7922\n",
      "Epoch 135/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5653 - accuracy: 0.8103 - val_loss: 0.5936 - val_accuracy: 0.7987\n",
      "Epoch 136/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5032 - accuracy: 0.8167 - val_loss: 0.6319 - val_accuracy: 0.7857\n",
      "Epoch 137/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5513 - accuracy: 0.8115 - val_loss: 0.6192 - val_accuracy: 0.7922\n",
      "Epoch 138/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5346 - accuracy: 0.8097 - val_loss: 0.6215 - val_accuracy: 0.7955\n",
      "Epoch 139/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5496 - accuracy: 0.8144 - val_loss: 0.6068 - val_accuracy: 0.7922\n",
      "Epoch 140/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.4722 - accuracy: 0.8367 - val_loss: 0.6097 - val_accuracy: 0.7890\n",
      "Epoch 141/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5471 - accuracy: 0.8138 - val_loss: 0.6330 - val_accuracy: 0.7760\n",
      "Epoch 142/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5260 - accuracy: 0.8273 - val_loss: 0.6185 - val_accuracy: 0.7825\n",
      "Epoch 143/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5475 - accuracy: 0.8068 - val_loss: 0.6198 - val_accuracy: 0.7987\n",
      "Epoch 144/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.4780 - accuracy: 0.8367 - val_loss: 0.6033 - val_accuracy: 0.7987\n",
      "Epoch 145/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5080 - accuracy: 0.8208 - val_loss: 0.6197 - val_accuracy: 0.7890\n",
      "Epoch 146/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5259 - accuracy: 0.8132 - val_loss: 0.6196 - val_accuracy: 0.7922\n",
      "Epoch 147/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5112 - accuracy: 0.8343 - val_loss: 0.5957 - val_accuracy: 0.7987\n",
      "Epoch 148/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5390 - accuracy: 0.8285 - val_loss: 0.6055 - val_accuracy: 0.7987\n",
      "Epoch 149/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5543 - accuracy: 0.8156 - val_loss: 0.6071 - val_accuracy: 0.7922\n",
      "Epoch 150/150\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.5114 - accuracy: 0.8249 - val_loss: 0.6284 - val_accuracy: 0.7825\n",
      "CNN: Epochs=150, Train accuracy=0.83665, Validation accuracy=0.82143\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[annealer]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztjVtlqWD_Nb"
   },
   "outputs": [],
   "source": [
    "#9까지는 콘볼루션 층 9개에 맥스풀링과 FC 첨가해서 필터 수나 러닝 레이트만 조절해봄\n",
    "#이번 model_10은 LeNet 참고하여 비슷한 구조로 얕게 층 쌓음 - 훈련 모델에 대한 오버피팅 심함\n",
    "\n",
    "#conv 층과 maxpooling 층에 strides=2 추가해서 해봄\n",
    "\n",
    "#randomsearch로 learning rate 등 하이퍼 파라미터 조절해보기\n",
    "#차라리 learning rate 줄이고 많이 시도? 진동하는 경향이 좀 보임 epoch 75부터 계~~~~속 0.7대 유지 - epoch 161 아직도 탈출 못함\n",
    "#0.003으로 했을 때 진동하는 듯 해서 0.001로 다시 돌아옴 - 근데 학습률이 낮아도 마찬가지로 local 최적에 빠지지 않을까? - global 최적 찾을 방법은??\n",
    "#이것도 진동에 빠진다면 0.002로 시도해보기\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PD-P4Jt4MoI7"
   },
   "outputs": [],
   "source": [
    "#model_11 - 모델 10에서 Leaky ReLU나 PReLU나 Maxout 시도해보기\n",
    "#아무튼 적당한 활성화 함수 찾은 후 RandomSearch로 하이퍼파라미터 최적화\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from keras import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Dropout,\n",
    "    Flatten, Dense, Input, Concatenate, LeakyReLU, PReLU, \n",
    "    Add, Activation, BatchNormalization, MaxPooling2D\n",
    ")\n",
    "\n",
    "#데이터 증강\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.1, #10퍼센트 확대\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1)\n",
    "\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal', input_shape = train_X.shape[1:]))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(16, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(32, (3,3), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1597999558839,
     "user": {
      "displayName": "피자치즈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgJ4P6BaNTGcVVotmUSqhc7XW3XdSFyT9bNAKBTCg=s64",
      "userId": "09924732654346687849"
     },
     "user_tz": -540
    },
    "id": "RkVofda0MsDp",
    "outputId": "303bc178-314a-4b69-a342-b6fb04ffc8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1174 (Conv2D)         (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_600 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1175 (Conv2D)         (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_601 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1176 (Conv2D)         (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_602 (LeakyReLU)  (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1177 (Conv2D)         (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_603 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1178 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_604 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1179 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_605 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1180 (Conv2D)         (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_606 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1181 (Conv2D)         (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_607 (LeakyReLU)  (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1182 (Conv2D)         (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_608 (LeakyReLU)  (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_28  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_609 (LeakyReLU)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 124,048\n",
      "Trainable params: 124,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UJzxDuhMvcE"
   },
   "outputs": [],
   "source": [
    "#각 epoch마다 learning rate 낮춰줌\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.975 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByQCa6DAMyRv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 230\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(train_X, train_y, batch_size=batch_size),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "    validation_data=(valid_X,valid_y),\n",
    "    verbose=1,\n",
    "    callbacks=[annealer]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"CNN: Epochs={epochs:d}, \" +\n",
    "    f\"Train accuracy={max(history.history['accuracy']):.5f}, \" +\n",
    "    f\"Validation accuracy={max(history.history['val_accuracy']):.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8AWxmI1RF-1"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(valid_X, valid_y, verbose=2)\n",
    "print('테스트 손실함수:', test_loss, '\\n테스트 정확도:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vr35rvIqPNUu"
   },
   "source": [
    "시도해볼 것: maxout은 dropout과 함께 쓰면 효과가 좋은 활성화함수라고 함\n",
    "    - maxout 적용 예제 / 코드 찾아서 직접 사용해보기\n",
    "    - maxout의 단점은 파라미터의 수가 두배가 된다는 점\n",
    "             maxpooling만 쓰지 말고 각 방법의 장단점 파악 후 새롭게 적용해보기\n",
    "\n",
    "Summary\n",
    "Stanford의 CS231n 강의에서는 다음과 같은 순서로 Activation function을 시도해볼 것을 권한다.\n",
    "\n",
    "(1) ReLU를 사용하자.\n",
    "\n",
    "(2) 성능이 만족스럽지 않다면, LeakyReLU, Maxout, ELU를 사용하라.\n",
    "\n",
    "(3) 그래도 만족스럽지 않다면, tanh를 사용하라. 하지만 많은 기대는 하지말자.\n",
    "\n",
    "(4) 그러나 Sigmoid는 사용하지 말라.\n",
    "\n",
    "실무적으로는 ReLU를 가장 많이 사용하며, LeakyReLU/Maxout/ELU도 좋은 선택지가 될 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XskrI2jTrcn"
   },
   "source": [
    "learning rate 점점 감소 시켰을 때: 0.7941\n",
    "\n",
    "그냥 없이 했을 때: 0.8284\n",
    "\n",
    "model_5: train - 0.87, valid - 0.82\n",
    "\n",
    "model_6: Train accuracy=0.89975, Validation accuracy=0.82195\n",
    "\n",
    "강아지 품종 분류 cnn, 데이터 부풀리기 비교 예제\n",
    "\n",
    "# https://lsjsj92.tistory.com/387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_GEq195_R9L"
   },
   "source": [
    "cnn\n",
    "# https://m.blog.naver.com/laonple/221212462034\n",
    "\n",
    "https://m.blog.naver.com/laonple/220808903260\n",
    "\n",
    "# https://buomsoo-kim.github.io/keras/2018/05/05/Easy-deep-learning-with-Keras-11.md/\n",
    "\n",
    "https://machine-geon.tistory.com/46\n",
    "\n",
    "https://excelsior-cjh.tistory.com/152\n",
    "\n",
    "cnn 정확도 높이기\n",
    "\n",
    "https://manofconcrete.blogspot.com/2019/12/mnist-hands-on-3.html\n",
    "\n",
    "randpm search vs grid search\n",
    "\n",
    "https://shwksl101.github.io/ml/dl/2019/01/30/Hyper_parameter_optimization.html\n",
    "\n",
    "케라스 튜너\n",
    "# https://github.com/keras-team/keras-tuner\n",
    "https://tykimos.github.io/2019/05/10/KerasTuner/\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Md1ua95yr-J"
   },
   "outputs": [],
   "source": [
    "'''def model_fn():\n",
    "    LR = Choice('learning_rate', [0.001, 0.0005, 0.0001], group='optimizer')\n",
    "    DROPOUT_RATE = Linear('dropout_rate', 0.0, 0.5, 5, group='dense')\n",
    "    NUM_DIMS = Range('num_dims', 8, 32, 8, group='dense')\n",
    "    NUM_LAYERS = Range('num_layers', 1, 3, group='dense')\n",
    "    L2_NUM_FILTERS = Range('l2_num_filters', 8, 64, 8, group='cnn')\n",
    "    L1_NUM_FILTERS = Range('l1_num_filters', 8, 64, 8, group='cnn')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(L1_NUM_FILTERS, kernel_size=(3, 3), activation='relu'), input_shape = train_X.shape[1:])\n",
    "    model.add(Conv2D(L2_NUM_FILTERS, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    for _ in range(NUM_LAYERS):\n",
    "        model.add(Dense(NUM_DIMS, activation='relu'))\n",
    "        model.add(Dropout(DROPOUT_RATE))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(LR), metrics=['accuracy'])\n",
    "\n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uEnvE_8RF-3"
   },
   "outputs": [],
   "source": [
    "'''def create_cnn_model(train_x):\n",
    "    inputs = tf.keras.layers.Input(train_x.shape[1:])\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu')(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2,2))(conv)\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(pool)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2,2))(conv)\n",
    "    \n",
    "    flatten = tf.keras.layers.Flatten()(pool)\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(flatten)\n",
    "    dense = tf.keras.layers.Dense(1000, activation='relu')(bn)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(dense)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(bn)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvsh_fosRF-5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''model = create_cnn_model(train_X)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=20)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEvNqqJeRF-8"
   },
   "outputs": [],
   "source": [
    "'''test_loss, test_acc = model.evaluate(valid_X, valid_y, verbose=2)\n",
    "print('테스트 손실함수:', test_loss, '\\n테스트 정확도:', test_acc)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcIjWCcoRF-_"
   },
   "source": [
    "https://excelsior-cjh.tistory.com/152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8lDYw1oRF-_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import cv2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train = pd.read_csv('./data/cvision/train.csv')\n",
    "train_x = train.iloc[:,3:].values.reshape(-1,28,28)\n",
    "data = train_x[0]\n",
    "data.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZiVdnh4RF_C"
   },
   "outputs": [],
   "source": [
    "'''#상하좌우 이동\n",
    "samples = np.expand_dims(data, 0)\n",
    "\n",
    "#Generator 생성\n",
    "#range를 설정해 얼마나 움직일지 정해줌\n",
    "gen = ImageDataGenerator(width_shift_range=[-10,10])\n",
    "\n",
    "#figure 생성\n",
    "fig = plt.figure(figsize=(28,28))\n",
    "\n",
    "#it\n",
    "it = gen.flow(samples, batch_size=1)\n",
    "\n",
    "#9개 이미지 생성\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    \n",
    "    batch = it.next()\n",
    "    img = batch[0].astype('uint8')\n",
    "    \n",
    "    #plot raw pixel data\n",
    "    plt.imshow(img)\n",
    "    \n",
    "#show the figure\n",
    "plt.title('moving')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P75WrD3Zkzfb"
   },
   "outputs": [],
   "source": [
    "#ResNet - Residual Block\n",
    "\n",
    "'''\n",
    "from keras import layers\n",
    "\n",
    "def residual_block(x, filters_in, filters_out, k_size):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters_in, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters_in, kernel_size=(k_size, k_size), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)    \n",
    "    \n",
    "    x = layers.Conv2D(filters_out, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    shortcut_channel = x.shape.as_list()[-1]\n",
    "    \n",
    "    if shortcut_channel != filters_out:\n",
    "        shortcut = layers.Conv2D(filters_out, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(shortcut)\n",
    "        \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return layers.LeakyReLU()(x)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Co_cvision_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
