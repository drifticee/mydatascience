{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.788\n",
      "RandomForestClassifier 0.808\n",
      "SVC 0.804\n",
      "VotingClassifier 0.816\n"
     ]
    }
   ],
   "source": [
    "#투표 기반 분류기 (page. 246)\n",
    "#말 그대로 여러 분류기의 예측을 투표로 받아 가장 많이 선택된 클래스로 예측함\n",
    "#사이킷런의 투표 기반 분류기\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#moons 데이터셋 가져오기\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_moons(n_samples=1000, noise=0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                             voting='hard')\n",
    "voting_clf = voting_clf.fit(X_train, y_train)\n",
    "\n",
    "#각 분류기의 테스트셋 정확도 확인\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.788\n",
      "RandomForestClassifier 0.816\n",
      "SVC 0.804\n",
      "VotingClassifier 0.82\n"
     ]
    }
   ],
   "source": [
    "#voting=soft - 분류기가 클래스의 확률 예측할 수 있으면(predict_proba)\n",
    "#개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True) #확률 예측 가능 (predict_proba 메서드 사용 가능)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                             voting='soft')\n",
    "voting_clf = voting_clf.fit(X_train, y_train)\n",
    "\n",
    "#각 분류기의 테스트셋 정확도 확인\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824\n"
     ]
    }
   ],
   "source": [
    "#배깅(중복 허용), 페이스팅(중복 x) - 무작위로 샘플링하여 여러 개의 예측기를 훈련\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500, max_samples=100,\n",
    "                            bootstrap=True, n_jobs=-1) #bootstrap=False 하면 배깅 대신 페이스팅\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8386666666666667"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단 한 번도 선택되지 않은 37%의 샘플들 - out of bag, oob\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                            n_estimators=500, bootstrap=True,\n",
    "                            n_jobs=-1, oob_score=True) #oob_score\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74011299, 0.25988701],\n",
       "       [0.735     , 0.265     ],\n",
       "       [0.15425532, 0.84574468],\n",
       "       ...,\n",
       "       [0.25294118, 0.74705882],\n",
       "       [0.03333333, 0.96666667],\n",
       "       [0.98979592, 0.01020408]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_\n",
    "#각 훈련 샘플의 클래스 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 패치(훈련 특성과 샘플 모두 샘플링),\n",
    "그리고 랜덤 서브스페이스 - 훈련 샘플을 모두 사용하고(bootstrap=False, max_samples=1.0)\n",
    "특성은 샘플링(bootstrap_features=True 그리고/또는 max_features= 1.0보다 작게 설정)\n",
    "BaggingClassifier는 특성 샘플링도 지원\n",
    "매개변수는 max_features, bootstrap_features\n",
    "\n",
    "특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "#랜덤 포레스트 - 배깅/페이스팅을 적용한 결정 트리의 앙상블\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "#n_estimators는 숲을 이루는 tree의 수\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "#위를 BaggingClassifier로 유사하게 만든 것\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features='auto', max_leaf_nodes=16),\n",
    "                           n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bag = rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09495566434345105\n",
      "sepal width (cm) 0.02610340581159169\n",
      "petal length (cm) 0.4401680297313323\n",
      "petal width (cm) 0.4387729001136249\n"
     ]
    }
   ],
   "source": [
    "#익스트림 랜덤 트리 앙상블 (엑스트라 트리) - 극단적으로 무작위한 트리의 랜덤 포레스트\n",
    "#랜덤 포레스트의 장점 - 특성 중요도 측정이 쉬움 feature_importances_\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부스팅 - 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
    "#이전 예측기를 보완하는 새로운 예측기를 만드는 방법 - 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것\n",
    "#에이다부스트:\n",
    "#1. 알고리즘의 기반이 되는 첫 분류기를 훈련 세트에서 훈련시키고 예측을 만듦\n",
    "#2. 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높임\n",
    "#3. 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만듦\n",
    "#4. 그다음 다시 가중치를 업데이트 - 계속 반복\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
