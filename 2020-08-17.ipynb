{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "target 클래스의 값과 분포도\n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "target 클래스의 이름들\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] \n",
      "\n",
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######### 20개 뉴스 그룹 데이터셋을 이용한 텍스트 분류 #########\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "\n",
    "print(news_data.keys())\n",
    "\n",
    "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들\\n', news_data.target_names, '\\n')\n",
    "\n",
    "#첫번째 기사 내용 확인\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#subset = 'train' 으로 학습용 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "\n",
    "train_news = fetch_20newsgroups(subset='train',\n",
    "                                remove=('headers', 'footers', 'quotes'),\n",
    "                                random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 11314, 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "#subset = 'test' 로 테스트용 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "\n",
    "test_news = fetch_20newsgroups(subset='test',\n",
    "                               remove=('headers', 'footers', 'quotes'),\n",
    "                               random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "print('학습 데이터 크기 {0}, 테스트 데이터 크기 {1}'.format(len(train_news.data), len(test_news.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "#카운트 기반으로 벡터화\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:', X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer LogisticRegression 예측 정확도 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression을 이용하여 학습데이터에 관한 예측 평가 수행\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "\n",
    "print('CountVectorizer LogisticRegression 예측 정확도 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer LogisticRegression 예측 정확도: 0.674\n"
     ]
    }
   ],
   "source": [
    "#Tf-Idf Vectorizer 기반 벡터화\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "\n",
    "print('TfidfVectorizer LogisticRegression 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 LogisticRegression 예측 정확도: 0.701\n"
     ]
    }
   ],
   "source": [
    "#Pipeline을 이용해 GridSearchCV 실행\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "                     ('lr_clf', LogisticRegression(C=10))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "\n",
    "print('Pipeline을 통한 LogisticRegression 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "########## 하이퍼 파라미터 찾기 ###########\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                     ('lr_clf', LogisticRegression())])\n",
    "\n",
    "params = {'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "          'tfidf_vect__max_df': [100, 300, 700],\n",
    "          'lr_clf__C': [1, 5, 10]}\n",
    "\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3,\n",
    "                            scoring='accuracy', verbose=1)\n",
    "grid_cv_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)\n",
    "\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "\n",
    "print('Pipeline을 통한 LogisticRegression 예측 정확도: {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "########텍스트 전처리 1 : 문장 토큰화, 단어 토큰화 #############\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text_sample)\n",
    "print(type(sentences), len(sentences ))\n",
    "print(sentences )\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words ))\n",
    "print(words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "########텍스트 전처리 2 : stopword 제거 (필터링)#############\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "for sentence in word_tokens :\n",
    "    filtered_words=[]\n",
    "    for word in sentence :\n",
    "        word = word.lower()\n",
    "        if word not in stopwords :\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n"
     ]
    }
   ],
   "source": [
    "########텍스트 전처리 3 : 어근 추출 #############\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working' , 'v'), lemma.lemmatize('works', 'v'), lemma.lemmatize('worked', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [2, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######텍스트 전처리 4 : 수치 데이터 벡터화 ##################\n",
    "#희소 행렬 COO 형식\n",
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])\n",
    "\n",
    "from scipy import sparse\n",
    "data = np.array([3, 1, 2])\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo=sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "sparse_coo.toarray()\n",
    "\n",
    "\n",
    "#희소 행렬 CSR 형식\n",
    "dense2  = np.array([[0,0,1,0,0,5],\n",
    "                    [1,4,0,3,2,5],\n",
    "                    [0,6,0,3,0,0],\n",
    "                    [2,0,0,0,0,0],\n",
    "                    [0,0,0,7,0,8],\n",
    "                    [1,0,0,0,0,0]])\n",
    "\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "row_pos = np.array([0, 0,1,1,1,1,1, 2, 2, 3, 4, 4, 1])\n",
    "col_pos = np.array([2, 5, 0, 1,3,4,5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "sparse_csr=sparse.csr_matrix((data2, (row_pos, col_pos)))\n",
    "sparse_csr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 5, 'epochs': 5, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "##############신경망 하이퍼파라미터 찾기 : GridSearchDV ####\n",
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 100\n",
    "\n",
    "features, target = make_classification(n_samples=10000, n_features = number_of_features, n_classes=2, n_informative=3, n_redundant=0, weights=[.5, .5], random_state = 0)\n",
    "\n",
    "def create_network(optimizer='rmsprop') :\n",
    "    network = Sequential();\n",
    "    network.add(Dense(units=16, activation='relu', input_shape=(number_of_features,)))\n",
    "    network.add(Dense(units=16, activation='relu'))\n",
    "    network.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    network.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)\n",
    "\n",
    "epochs=[5, 10]\n",
    "batchs =[5, 10, 100]\n",
    "optimizers =['rmsprop', 'adam']\n",
    "\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batchs)\n",
    "\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "\n",
    "grid_result = grid.fit(features, target)\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############CNN 실습##########################\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "numpy.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# 데이터 로드\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#정규화\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# 정답 label one-hot enconding\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\anaconda3\\envs\\mypy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#모델 정의\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,  activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05496, saving model to ./model/01-0.0550.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05496 to 0.04196, saving model to ./model/02-0.0420.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04196 to 0.03608, saving model to ./model/03-0.0361.hdf5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03608\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03608 to 0.03187, saving model to ./model/06-0.0319.hdf5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03187\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03187 to 0.03060, saving model to ./model/08-0.0306.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03060\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03060 to 0.03004, saving model to ./model/10-0.0300.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03004\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03004 to 0.02921, saving model to ./model/12-0.0292.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02921\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02921 to 0.02902, saving model to ./model/22-0.0290.hdf5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02902\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02902\n"
     ]
    }
   ],
   "source": [
    "#모델 구축 컴파일\n",
    "model.compile(loss='categorical_crossentropy',   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#모델 실행 결과 model폴더에 파일로 저장\n",
    "#학습 중단 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 529us/step\n",
      "\n",
      " Test Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "#정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#학습셋 오차와 테스트셋 오차를 시각화\n",
    "# 테스트 셋의 오차\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=3)\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis([0, 20, 0, 0.35])\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
